# SOME DESCRIPTIVE TITLE
# Copyright (C) YEAR Free Software Foundation, Inc.
# This file is distributed under the same license as the PACKAGE package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PACKAGE VERSION\n"
"POT-Creation-Date: 2022-05-12 15:52+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: \n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#. type: YAML Front Matter: author
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:1
#, fuzzy, no-wrap
msgid "ebernard"
msgstr "ǞǞǞ"

#. type: YAML Front Matter: date
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:1
#, fuzzy, no-wrap
msgid "2020-04-21"
msgstr "2020-04-21"

#. type: YAML Front Matter: layout
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:1
#, fuzzy, no-wrap
msgid "post"
msgstr "职位"

#. type: YAML Front Matter: synopsis
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:1
#, fuzzy, no-wrap
msgid "Understand when and how to use the Quarkus IO thread and its influence on microbenchmarks."
msgstr "了解何时和如何使用Quarkus IO线程以及它对微观测试的影响。"

#. type: YAML Front Matter: tags
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:1
#, fuzzy, no-wrap
msgid "performance"
msgstr "业绩"

#. type: YAML Front Matter: title
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:1
#, fuzzy, no-wrap
msgid "A IO thread and a worker thread walk into a bar: a microbenchmark story"
msgstr "一个IO线程和一个工人线程走进酒吧：一个微基准的故事"

#. type: Plain text
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:17
#, fuzzy
msgid "A competitor recently published a microbenchmark comparing the performance of their stack to Quarkus.  The Quarkus team feels this microbenchmark shouldn’t be taken at face value because it wasn’t making a like-to-like comparison leading to incorrect conclusions.  Both of the two frameworks under comparison support reactive processing.  Reactive processing enables running the business logic directly on the _IO thread_, which ultimately performs better in microbenchmark focusing on response time and concurrency.  The microbenchmark should have been written so that both frameworks (or neither framework) obtain this benefit.  Anyway, this turns out to be a very interesting topic and good information for Quarkus users, so read on."
msgstr "一个竞争对手最近发布了一个微观基准，将他们的堆栈的性能与Quarkus进行比较。Quarkus团队认为这个微测试不应该被当作表面价值，因为它没有进行类似的比较，导致了不正确的结论。被比较的两个框架都支持反应式处理。反应式处理能够直接在 _IO线程_ 上运行业务逻辑，最终在注重响应时间和并发性的微观基准测试中表现更好。编写微基准时，应该让两个框架（或两个框架都不支持）都获得这种好处。总之，这变成了一个非常有趣的话题，对Quarkus用户来说是很好的信息，所以请继续阅读。"

#. type: Title ==
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:18
#, fuzzy, no-wrap
msgid "tl; dr;"
msgstr "tl; dr;"

#. type: Plain text
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:22
#, fuzzy
msgid "Quarkus https://quarkus.io/blog/runtime-performance/[has great performance] for both imperative and reactive workloads.  It’s because Quarkus is itself based on Eclipse Vert.x, a mature top performing reactive framework, in such a way that allows you to layer, mix and match the IO paradigm that best fits your use-case."
msgstr "Quarkus对于命令式和反应式工作负载都 link:https://quarkus.io/blog/runtime-performance/[有很好的性能] 。这是因为Quarkus本身是基于Eclipse Vert.x的，这是一个成熟的顶级性能的反应式框架，这样的方式允许你分层、混合和匹配最适合你使用情况的IO范式。"

#. type: Plain text
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:24
#, fuzzy
msgid "If you have a REST scenario well suited to run _purely on the IO thread_, add a Vert.x Reactive Route using https://quarkus.io/guides/reactive-routes[Quarkus Reactive Routes] and your app will get better performance than using Quarkus RESTEasy."
msgstr "如果你有一个很适合 _纯粹在IO线程上_ 运行的REST场景，使用 link:https://quarkus.io/guides/reactive-routes[Quarkus Reactive Routes] 添加一个Vert.x Reactive Route，你的应用将获得比使用Quarkus RESTEasy更好的性能。"

#. type: Plain text
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:28
#, fuzzy
msgid "We ran this low-work REST + validation competitor-written microbenchmark which features no blocking operation, just returning static data.  When using Quarkus Reactive Routes to run Quarkus _purely on the IO thread_, **we observed 2.6x times the requests/sec and 30% less memory usage (RSS)** than running with Quarkus RESTEasy (which mixes IO thread and worker thread).  But that’s on a microbenchmark purpose built to this specific scenario (more on that later)."
msgstr "我们运行了这个低工作的REST+验证竞争者编写的微基准，其特点是没有阻塞操作，只是返回静态数据。当使用Quarkus Reactive Routes _纯粹在IO线程上_ 运行Quarkus时， *我们观察到* 比使用Quarkus RESTEasy（混合了IO线程和工作线程）运行的 *请求/秒多2.6倍，内存用量少30%（RSS）* 。但这是在一个专门为这个特定场景建立的微观基准上（后面会有更多介绍）。"

#. type: Target for macro image
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:29
#, no-wrap
msgid "throughput.png"
msgstr ""

#. type: Title ==
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:31
#, fuzzy, no-wrap
msgid "More interesting read"
msgstr "更多有趣的阅读"

#. type: Plain text
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:35
#, fuzzy
msgid "The microbenchmark itself is uninteresting, but it is a good demonstrator of a phenomenon that can happen in reactive stacks.  Let’s use it as a vehicle to learn more about Quarkus and its reactive engine."
msgstr "这个微测试本身并不有趣，但它很好地展示了一个在反应堆中可能发生的现象。让我们把它作为一个载体来学习更多关于Quarkus和它的反应式引擎。"

#. type: Title ===
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:36
#, fuzzy, no-wrap
msgid "Imperative and Reactive: the elevator pitch"
msgstr "强迫性和反应性：升降机演讲"

#. type: Plain text
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:40
#, fuzzy
msgid "This blog post does not explain the fundamental differences between the imperative execution model and the reactive execution model.  However, to understand why we see so much difference in the mentioned microbenchmark, we need some notions."
msgstr "这篇博文并没有解释命令式执行模型和反应式执行模型之间的根本区别。然而，为了理解为什么我们在提到的微基准中看到如此大的差异，我们需要一些概念。"

#. type: Plain text
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:49
#, fuzzy
msgid "In general, Java web applications use imperative programming combined with blocking IO operations.  This is incredibly popular because it is easier to reason about the code.  _Things_ get executed sequentially.  To make sure one request is not affected by another, they are run on different threads.  When your workload needs to interact with a database or another remote service, it relies on blocking IO.  The thread is blocked waiting for the answer.  Other requests running on different threads are not slowed down significantly.  But this means one thread for every concurrent request, which limits the overall concurrency."
msgstr "一般来说，Java网络应用程序使用命令式编程与阻塞式IO操作相结合。这是令人难以置信的流行，因为它更容易推理代码。 _事情_ 是按顺序执行的。为了确保一个请求不受另一个请求的影响，它们被运行在不同的线程上。当你的工作负载需要与数据库或其他远程服务交互时，它依赖于阻塞式IO。该线程被阻塞，等待答案。在不同线程上运行的其他请求不会被明显减慢。但这意味着每个并发请求都有一个线程，这限制了整体的并发性。"

#. type: Plain text
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:64
#, fuzzy
msgid "On the other side, the reactive execution model embraces asynchronous development models and non blocking IOs.  With this model, multiple requests can be handled by the same thread.  When the processing of a request cannot make progress anymore (because it requests a remote service, or interacts with a database), it uses non blocking IO.  This releases the thread immediately, which can then be used to serve another request.  When the result of the IO operation is available, the processing of the request is restored and continues its execution.  This model enables the usage of the _IO thread_ to handle multiple requests.  There are two significant benefits.  First, the response time is smaller because it does not have to jump to another thread.  Second, it reduces memory consumption as it decreases the usage of threads.  The reactive model uses the hardware resources more efficiently, but... there is a significant drawback.  If the processing of a request starts to block, this gets real bad.  No other request can be handled.  To avoid this, you need to learn how to write non blocking code, how to structure asynchronous processing, and how to use non blocking IOs.  It's a paradigm shift."
msgstr "另一方面，反应式执行模型包含了异步开发模型和非阻塞式IO。在这种模式下，多个请求可以由同一个线程处理。当一个请求的处理不能再取得进展时（因为它请求一个远程服务，或与数据库交互），它使用非阻塞的IO。这将立即释放该线程，然后它可以被用来处理另一个请求。当IO操作的结果可用时，请求的处理被恢复并继续其执行。这种模式可以使用 _IO线程_ 来处理多个请求。这有两个显著的好处。首先，响应时间更小，因为它不需要跳转到另一个线程。第二，它减少了内存的消耗，因为它减少了线程的使用。反应式模型更有效地利用了硬件资源，但也有一个明显的缺点。如果一个请求的处理开始阻塞，这将变得非常糟糕。没有其他请求可以被处理。为了避免这种情况，你需要学习如何编写非阻塞的代码，如何构造异步处理，以及如何使用非阻塞的IO。这是一个范式的转变。"

#. type: Plain text
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:68
#, fuzzy
msgid "In Quarkus, we want to make the shift as easy as possible.  However, we have observed that the majority of user applications are written using the imperative model.  That is why, when the user application uses JAX-RS, Quarkus defaults to execute the (imperative) workload to a _worker thread_."
msgstr "在Quarkus中，我们想让这种转变尽可能的简单。然而，我们观察到，大多数用户应用程序是使用命令式模型编写的。这就是为什么当用户应用程序使用JAX-RS时，Quarkus默认将（命令式）工作负载执行到一个 _工作线程_ 。"

#. type: Title ===
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:69
#, fuzzy, no-wrap
msgid "Hello world microbenchmark: IO thread or worker thread?"
msgstr "你好，世界》微测试。IO线程还是工作线程？"

#. type: Plain text
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:74
#, fuzzy
msgid "Back to the competitor’s microbenchmark, we have a REST endpoint doing some trivial processing and some equally trivial validation.  Pretty much no meaningful business work.  This is the Hello World of REST for all intents and purposes."
msgstr "回到竞争对手的微基准，我们有一个REST端点在做一些琐碎的处理和一些同样琐碎的验证。几乎没有任何有意义的业务工作。这就是REST的 \"你好，世界\"，就所有的目的而言。"

#. type: Plain text
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:79
#, fuzzy
msgid "When you run the microbenchmark with Quarkus RESTEasy, the request is handled by the reactive engine on the _IO thread_ but then the processing work is handed over to a second thread from the _worker thread pool_.  That’s called _dispatch_.  When your microbenchmark does as little as Hello World, then the dispatch overhead is proportionally big.  The dispatch overhead is not visible in most (real life) applications but is very visible in artificial constructs like microbenchmarks."
msgstr "当你用Quarkus RESTEasy运行微基准时，请求由 _IO线程_ 上的反应式引擎处理，但随后处理工作被移交给 _工作线程池_ 的第二个线程。这就是所谓的 _调度_ 。当你的微基准做得像Hello World那样少时，那么调度开销就会成比例地大。派遣开销在大多数（现实生活中）的应用中是不可见的，但在微基准这样的人工结构中却非常明显。"

#. type: Plain text
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:83
#, fuzzy
msgid "The competitor’s stack, however, runs all the request operations on the IO thread by default.  So what this microbenchmark was actually comparing is just the cost of dispatching to the worker thread pool.  And frankly (according to the competitor's numbers) and in spite of this extra dispatch work, Quarkus did very very well achieving ~95% of the competitor’s throughput today! I say today because we are always improving upon performance, and in fact we expect to see further gains in the soon to be released 1.4 release."
msgstr "而竞争对手的堆栈则默认在IO线程上运行所有的请求操作。因此，这个微基准测试实际上比较的只是调度到工作线程池的成本。坦率地说（根据竞争对手的数据），尽管有这些额外的调度工作，Quarkus做得非常好，达到了竞争对手今天的95%的吞吐量。我说今天，是因为我们一直在提高性能，事实上，我们希望在即将发布的1.4版本中看到进一步的提高。"

#. type: Plain text
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:85
#, fuzzy, no-wrap
msgid "*When compared at a disadvantage (dispatching to a worker thread), Quarkus is nevertheless almost as fast in throughput.*\n"
msgstr " *当在劣势下进行比较时（调度到工作线程），Quarkus的吞吐量还是几乎一样快。* "

#. type: Plain text
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:90
#, fuzzy
msgid "… But wait, Quarkus can also avoid the dispatch altogether and run operations on the IO Thread.  This is a more accurate comparison to how the competitor' stack was configured to do as in both case, it is the user's responsibility to ask for a dispatch if and when needed by the application.  To compare apples to apples, let’s use https://quarkus.io/guides/reactive-routes[Quarkus Reactive Routes] backed by Eclipse Vert.x.  In this model, operations are run on the IO thread by default."
msgstr "......但是等等，Quarkus也可以完全避免调度，在IO线程上运行操作。这与竞争对手的堆栈的配置方式相比更加准确，因为在这两种情况下，如果应用程序需要，用户有责任要求进行调度。为了比较苹果，让我们使用由Eclipse Vert.x支持的 link:https://quarkus.io/guides/reactive-routes[Quarkus Reactive Routes] 。在这个模型中，操作默认是在IO线程上运行。"

#. type: Plain text
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:95
#, fuzzy
msgid "@ApplicationScoped public class MyDeclarativeRoutes {"
msgstr "@ApplicationScoped public class MyDeclarativeRoutes {"

#. type: Plain text
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:98
#, fuzzy, no-wrap
msgid ""
"  @Inject\n"
"  Validator validator;\n"
msgstr ""
"@Inject\n"
"验证器验证器。"

#. type: Plain text
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:111
#, fuzzy, no-wrap
msgid ""
"  @Route(path = \"/hello/:name\", methods = HttpMethod.GET)\n"
"  void greetings(RoutingExchange ex) {\n"
"    RequestWrapper requestWrapper = new RequestWrapper(ex.getParam(\"name\").orElse(\"world\"));\n"
"    Set<ConstraintViolation<RequestWrapper>> violations = validator.validate(requestWrapper));\n"
"    if( violations.size() == 0) {\n"
"      ex.ok(\"hello \" + requestWrapper.name);\n"
"    } else {\n"
"      StringBuilder validationError = new StringBuilder();\n"
"      violations.stream().forEach(violation -> validationError.append(violation.getMessage()));\n"
"      ex.response().setStatusCode(400).end(validationError.toString());\n"
"    }\n"
"  }\n"
msgstr ""
"@Route(path = \"/hello/:name\", methods = HttpMethod.GET)\n"
"void greetings(RoutingExchange ex) {\n"
"  RequestWrapper requestWrapper = new RequestWrapper(ex.getParam(\"name\").orElse(\"world\")) 。\n"
"  Set<ConstraintViolation<RequestWrapper>> violations = validator.validate(requestWrapper)）。\n"
"  if( violations.size() == 0) {\n"
"    ex.ok(\"hello \" + requestWrapper.name)。\n"
"  } else {\n"
"    StringBuilder validationError = new StringBuilder();\n"
"    violations.stream().forEach(violence -> validationError.append(violence.getMessage()))。\n"
"    ex.response().setStatusCode（400）.end（validationError.toString（））。\n"
"  }\n"
"}"

#. type: Plain text
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:115
#, fuzzy, no-wrap
msgid ""
"  private class RequestWrapper {\n"
"    @NotBlank\n"
"    public String name;\n"
msgstr ""
"private class RequestWrapper {\n"
"  @NotBlank\n"
"  public String name;"

#. type: Plain text
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:120
#, fuzzy, no-wrap
msgid ""
"    public RequestWrapper(String name) {\n"
"      this.name = name;\n"
"    }\n"
"  }\n"
msgstr ""
"  public RequestWrapper(String name) {\n"
"    this.name = name。\n"
"  }\n"
"}"

#. type: Plain text
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:122
#, fuzzy
msgid "}"
msgstr "}"

#. type: Plain text
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:125
#, fuzzy
msgid "This is not very different from your JAX-RS equivalent."
msgstr "这与你的JAX-RS等价物没有很大区别。"

#. type: Title ===
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:126
#, fuzzy, no-wrap
msgid "Throughput Numbers"
msgstr "吞吐量数字"

#. type: Plain text
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:129
#, fuzzy
msgid "We ran the microbenchmark application in a docker container constrained to reflect a typical resource allocation to a container orchestrated by Kubernetes:"
msgstr "我们在一个docker容器中运行微测试应用程序，以反映由Kubernetes协调的容器的典型资源分配。"

#. type: Plain text
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:131
#, fuzzy
msgid "4 CPU"
msgstr "4 CPU"

#. type: Plain text
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:132
#, fuzzy
msgid "256 MB of RAM"
msgstr "256MB的内存"

#. type: Plain text
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:133
#, fuzzy
msgid "and `-Xmx128m` heap usage for the Java process"
msgstr "和 `-Xmx128m` Java进程的堆使用情况"

#. type: Plain text
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:139
#, fuzzy
msgid "We saw that Quarkus using Reactive Routes ran 2.6 times the requests/sec.  2.6 times! It makes sense! Remember the application code virtually does nothing, so the dispatch cost is comparatively high.  If you were to write a more real life workload (maybe even having a blocking operation like a JPA access and therefore forcing a dispatch), then the results would be very different.  Context matters!"
msgstr "我们看到，使用Reactive Routes的Quarkus运行了2.6倍的请求/秒。2.6倍!这是有道理的!请记住，应用程序代码几乎什么都没做，所以调度成本相对较高。如果你要写一个更真实的工作负载（也许甚至有一个像JPA访问那样的阻塞操作，因此强迫调度），那么结果会非常不同。语境很重要!"

#. type: Plain text
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:141
#, fuzzy
msgid "You can find code and how to reproduce the microbenchmark https://github.com/johnaohara/quarkus-iothread-workerpool/tree/1.3.1.Final[here on GitHub]."
msgstr "你可以 link:https://github.com/johnaohara/quarkus-iothread-workerpool/tree/1.3.1.Final[在GitHub上] 找到代码和如何重现该微观基准。"

#. type: Block title
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:142
#, fuzzy, no-wrap
msgid "Microbenchmark results comparing Quarkus dispatching to a worker thread vs running purely on the IO thread"
msgstr "比较Quarkus调度到工作线程与纯粹在IO线程上运行的微观基准测试结果"

#. type: Table
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:146
#, fuzzy, no-wrap
msgid "Quarkus - 1.3.1.Final - 4 CPU's"
msgstr "Quarkus - 1.3.1.Final - 4个CPU"

#. type: Table
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:147
#, fuzzy, no-wrap
msgid "Worker thread"
msgstr "工作者线程"

#. type: Table
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:148
#, fuzzy, no-wrap
msgid "IO thread"
msgstr "IO线程"

#. type: Table
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:150
#, fuzzy, no-wrap
msgid "Ratio"
msgstr "比率"

#. type: Table
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:151
#, fuzzy, no-wrap
msgid "Mean Start Time to First Request (ms) footnote:[‘Mean Start Time to First Request’ was measured using an application built as an UberJar]"
msgstr "到第一个请求的平均开始时间（毫秒）[ link:#_footnotedef_1[1, id=\"_footnoteref_1\", class=\"footnote\", title=\"View footnote.\"]] 。 "

#. type: Table
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:152
#, fuzzy, no-wrap
msgid "993.9"
msgstr "993.9"

#. type: Table
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:153
#, fuzzy, no-wrap
msgid "868.3"
msgstr "868.3"

#. type: Table
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:155
#, fuzzy, no-wrap
msgid "87.4%"
msgstr "87.4%"

#. type: Table
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:156
#, fuzzy, no-wrap
msgid "Max RSS (MB)"
msgstr "最大RSS (MB)"

#. type: Table
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:157
#, fuzzy, no-wrap
msgid "138.8"
msgstr "138.8"

#. type: Table
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:158
#, fuzzy, no-wrap
msgid "97.9"
msgstr "97.9"

#. type: Table
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:160
#, fuzzy, no-wrap
msgid "70.5%"
msgstr "70.5%"

#. type: Table
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:161
#, fuzzy, no-wrap
msgid "Max Throughput (req/sec)"
msgstr "最大吞吐量（req/秒）"

#. type: Table
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:162
#, fuzzy, no-wrap
msgid "46,172.2"
msgstr "46,172.2"

#. type: Table
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:163
#, fuzzy, no-wrap
msgid "123,520.4"
msgstr "123,520.4"

#. type: Table
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:165
#, fuzzy, no-wrap
msgid "267.5%"
msgstr "267.5%"

#. type: Table
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:166
#, fuzzy, no-wrap
msgid "Max Req/Sec/MB"
msgstr "最大需求量/秒/MB"

#. type: Table
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:167
#, fuzzy, no-wrap
msgid "332.7"
msgstr "332.7"

#. type: Table
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:168
#, fuzzy, no-wrap
msgid "1,262.1"
msgstr "1,262.1"

#. type: Table
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:169
#, fuzzy, no-wrap
msgid "379.4%"
msgstr "379.4%"

#. type: Target for macro image
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:171
#, no-wrap
msgid "throughput-percentile.png"
msgstr ""

#. type: Plain text
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:174
#, fuzzy, no-wrap
msgid "*In a fair comparison (purely remaining on the IO thread - no dispatch), Quarkus more than double its throughput.*\n"
msgstr " *在一个公平的比较中（纯粹停留在IO线程上--没有调度），Quarkus的吞吐量增加了一倍以上。* "

#. type: Plain text
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:179
#, fuzzy
msgid "As the generated load tends towards the maximum throughput of the system under test, the response time experienced by the client increases exponentially.  So the best system (for the workload) has a vertical line as far to the right as possible.  Equally important is to have as flat a line as possible for the longest time.  You do not want the response time to degrade before the system reaches maximum throughput."
msgstr "当产生的负载趋向于被测系统的最大吞吐量时，客户端所经历的响应时间会呈指数级增长。因此，最好的系统（对于工作负荷）有一条尽可能靠右的垂直线。同样重要的是，在最长的时间内，尽可能有一条平坦的线。你不希望在系统达到最大吞吐量之前，响应时间就退化了。"

#. type: Plain text
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:184
#, fuzzy
msgid "By the way, in the competitor microbenchmark Quarkus is shown as consuming more RSS (more RAM).This is also explained by the worker thread pool being operated whereas the competitor did not have a worker thread pool.  The Quarkus Reactive Routes solution (on a pure IO event run) shows a 30% RSS usage reduction."
msgstr "顺便说一下，在竞争对手的微观基准中，Quarkus显示消耗了更多的RSS（更多的RAM）。这也可以解释为工人线程池被操作，而竞争对手没有工人线程池。Quarkus的Reactive Routes解决方案（在纯IO事件运行中）显示RSS使用量减少了30%。"

#. type: Target for macro image
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:185
#, no-wrap
msgid "rss.png"
msgstr ""

#. type: Plain text
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:189
#, fuzzy
msgid "In this graph, the lower, the better.  We see that the pure IO thread solution manages to increase throughput with little to no change to the memory usage (RSS), that's very good!"
msgstr "在这个图表中，越低越好。我们看到，纯IO线程解决方案能够在内存使用量几乎没有变化的情况下增加吞吐量（RSS），这非常好"

#. type: Title ==
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:190
#, fuzzy, no-wrap
msgid "Conclusion"
msgstr "总结"

#. type: Plain text
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:196
#, fuzzy
msgid "Quarkus offers you the ability to safely run blocking operations, run non blocking operations on the IO thread or mix both models.  The Quarkus team takes performance very seriously and we see Quarkus as offering great numbers whether you use the imperative or reactive models.  In more realistic workloads, the dispatch cost would be much less significant, you would not see such drastic differences between the two approaches.  As usual, test as close to your real application as possible."
msgstr "Quarkus为你提供了安全运行阻塞操作、在IO线程上运行非阻塞操作或混合两种模式的能力。Quarkus团队非常重视性能，我们认为无论你使用命令式还是反应式模型，Quarkus都能提供很好的数据。在更现实的工作负载中，调度成本将不那么重要，你不会看到这两种方法之间有如此大的差异。像往常一样，尽可能地接近你的真实应用进行测试。"

#. type: Plain text
#: upstream/_posts/2020-04-21-io-thread-benchmark.adoc:203
msgid "Mystery solved.  Benchmarks are hard, challenge them.  But the moral of the story is that in all bad comes some good.  We’ve now learned how to run Quarkus applications entirely on the IO thread.  And how in some situations that can make a big difference.  Remember, don’t block! In fact, Quarkus https://quarkus.io/guides/all-config#quarkus-vertx-core_quarkus.vertx.warning-exception-time[can warn you if you do so].  Oh and we also learned that Quarkus is so fast, it can even beat itself ;p"
msgstr ""
