msgid ""
msgstr ""
"Language: zh_CN\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"X-Generator: jekyll-l10n\n"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "Crafting a Local RAG application with Quarkus"
msgstr "使用 Quarkus 创建本地 RAG 应用程序"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "How to build a RAG chatbot using the Granite LLM."
msgstr "如何使用 Granite LLM 构建 RAG 聊天机器人。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid ""
"This blog post demonstrate how to build an AI-infused chatbot application using Quarkus, LangChain4j, https://infinispan.org/[Infinispan], and the https://github.com/ibm-granite/granite-code-models[Granite LLM].\n"
"In this post, we will create an **entirely** local solution, eliminating the need for any cloud services, including the LLM."
msgstr "本博文将演示如何使用 Quarkus、LangChain4j、 link:https://infinispan.org/[Infinispan] 和 link:https://github.com/ibm-granite/granite-code-models[Granite LLM] 构建一个人工智能聊天机器人应用程序。在本博文中，我们将创建一个 *完全* 本地的解决方案，无需任何云服务，包括 LLM。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid ""
"Our chatbot leverages the Granite LLM, a language model that generates contextually relevant text based on user prompts.\n"
"To run Granite locally, we'll be utilizing https://instructlab.ai/[InstructLab], though https://github.com/containers/podman-desktop-extension-ai-lab[Podman AI Studio] is another viable option."
msgstr "我们的聊天机器人采用了 Granite LLM，这是一种语言模型，可根据用户提示生成与上下文相关的文本。要在本地运行 Granite，我们将使用 link:https://instructlab.ai/[InstructLab] ，不过 link:https://github.com/containers/podman-desktop-extension-ai-lab[Podman AI Studio] 也是另一个可行的选择。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid ""
"The core of our application is based on the RAG (Retrieval-Augmented Generation) pattern.\n"
"This approach enhances the chatbot's responses by retrieving pertinent information from a vector database — in this case, Infinispan — before generating a response."
msgstr "我们应用的核心基于 RAG（检索-增强生成）模式。这种方法通过从向量数据库（在本例中为 Infinispan）中检索相关信息来增强聊天机器人的响应，然后再生成响应。"

#: _posts/2024-06-14-granite-rag.adoc
msgid "Architecture"
msgstr "架构"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "Our chatbot application is composed of four main components:"
msgstr "我们的聊天机器人应用由四个主要部分组成："

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid ""
"**Web Socket Endpoint**: This component serves as the communication bridge between the chatbot's backend and the frontend interface.\n"
"This component uses the new Quarkus WebSocket-Next extension to handle WebSocket connections efficiently.\n"
"It relies on the AI Service to interact with the LLM."
msgstr "*Web Socket 端点* ：该组件是聊天机器人后端与前端界面之间的通信桥梁。该组件使用新的 Quarkus WebSocket-Next 扩展来高效处理 WebSocket 连接。它依靠人工智能服务与 LLM 进行交互。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "**Ingestor**: The ingestor is responsible for populating the database with relevant data. It processes a set of local documents, split them into text segments, compute their vector representation and store them into Infinispan."
msgstr "*摄取器* ：摄取器负责用相关数据填充数据库。它处理一组本地文档，将其分割成文本段，计算其向量表示并将其存储到 Infinispan 中。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "**Retriever**: The retriever allows finding relevant text segments into Infinispan. When a user inputs a query, the retriever searches the vector database to find the most relevant pieces of information."
msgstr "*检索* 器检索器允许在 Infinispan 中查找相关的文本片段。当用户输入查询时，检索器会搜索矢量数据库，找到最相关的信息片段。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "**AI Service**: This is the core component of the chatbot, combining the capabilities of the retriever and the Granite LLM. The AI service takes the relevant information fetched by the retriever and uses the Granite LLM to generate the appropriate responses."
msgstr "*人工智能服务* ：这是聊天机器人的核心组件，结合了寻回器和花岗岩 LLM 的功能。人工智能服务接收寻回器获取的相关信息，并使用 Granite LLM 生成适当的回复。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "The following picture illustrates the high-level architecture:"
msgstr "下图展示了高层架构："

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "A simple explanation of the RAG pattern"
msgstr "RAG 模式的简单解释"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "The RAG (Retrieval-Augmented Generation) pattern is one of the most popular AI patterns, combining a retrieval mechanism with a generation mechanism to provide more accurate and relevant responses."
msgstr "RAG（检索-增强生成）模式是最流行的人工智能模式之一，它将检索机制与生成机制相结合，以提供更准确、更相关的响应。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "The RAG pattern operates in two main steps:"
msgstr "RAG 模式主要分为两个步骤："

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "**Ingestion**: The application ingests a set of documents, processes them, and stores them in a vector database."
msgstr "*输入* ：应用程序接收一组文件，对其进行处理，并将其存储到矢量数据库中。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "**Retrieval**: When a user inputs a query, the application retrieves the most relevant information from the vector database."
msgstr "*检索* ：当用户输入查询时，应用程序会从矢量数据库中检索最相关的信息。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "The following image gives a high-level overview of the _traditional_ RAG pattern:"
msgstr "下图是 _传统_ RAG 模式的高级概览："

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "There are more advanced version of the RAG pattern, but let's stick to the basics for this application."
msgstr "RAG 模式还有更高级的版本，但在本应用中，我们还是以基本模式为主。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "Ingestion"
msgstr "摄入"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid ""
"Let's first look at the ingestion step.\n"
"The ingestion process involves reading a set of documents, splitting them into text segments, computing their vector representations, and storing them in Infinispan."
msgstr "让我们先看看摄取步骤。摄取过程包括读取一组文档，将其分割成文本段，计算其向量表示，并将其存储在 Infinispan 中。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid ""
"The secret of an effective RAG implementation lies in how the text segments are computed.\n"
"In our application, we will follow a straightforward approach, but more advanced techniques are available and often required.\n"
"Depending on the document, you can use a variety of techniques to split the text into segments, such as paragraph splitting, sentence splitting, or more advanced techniques like the `recursive` splitter.\n"
"Also, if the document has a specific structure, you can use this structure to split the text into segments (like sections, chapters, etc.)."
msgstr "有效实施 RAG 的秘诀在于如何计算文本片段。在我们的应用中，我们将采用一种简单直接的方法，但更高级的技术是可用的，而且经常需要。根据文档的不同，可以使用多种技术将文本分割成不同的段落，如段落分割、句子分割或更高级的技术，如 `recursive` 分割器。此外，如果文档有特定的结构，也可以使用这种结构将文本分割成段（如节、章等）。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid ""
"The embedding model is responsible for converting text into a vector representation.\n"
"For simplicity, we use an in-process embedding model (`BGE-small`).\n"
"Other options, like the Universal Angle Embedding, are available, but we'll stick with BGE-small for simplicity."
msgstr "嵌入模型负责将文本转换为矢量表示。为简单起见，我们使用进程内嵌入模型 ( `BGE-small` )。还有其他选择，如通用角度嵌入，但为了简单起见，我们还是使用 BGE-small。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "Retrieval"
msgstr "检索"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid ""
"The second step is the retrieval process.\n"
"When a user inputs a query, the application searches the vector database to find the most relevant text segments."
msgstr "第二步是检索过程。当用户输入查询时，应用程序会搜索矢量数据库，找出最相关的文本片段。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid ""
"To achieve this, the application computes the vector representation of the user query using the **same** embedding model and compares it with the vector representations of the stored text segments in Infinispan.\n"
"It selects the most relevant text segments based on the similarity between the query vector and the text segment vectors."
msgstr "为此，应用程序使用 *相同的* 嵌入模型计算用户查询的向量表示，并将其与 Infinispan 中存储的文本段向量表示进行比较。它根据查询向量和文本段向量之间的相似性选择最相关的文本段。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid ""
"Then, it augments the user query with the retrieved text segments and sends it to the LLM.\n"
"Note that until this step, the LLM is not used."
msgstr "然后，它用检索到的文本片段增强用户查询，并将其发送给 LLM。请注意，在这一步之前，LLM 尚未使用。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "Implementing the chatbot"
msgstr "实施聊天机器人"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid ""
"Enough theory—let's dive into the implementation.\n"
"You can find the final version on https://github.com/cescoffier/quarkus-granite-rag-demo[GitHub]."
msgstr "理论讲得够多了，让我们来看看具体实现。您可以在 link:https://github.com/cescoffier/quarkus-granite-rag-demo[GitHub] 上找到最终版本。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "Used extensions and dependencies"
msgstr "使用的扩展和依赖项"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "To implement our chatbot, we rely on the following Quarkus extensions:"
msgstr "为了实现我们的聊天机器人，我们使用了以下 Quarkus 扩展："

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "`quarkus-langchain4j-openai`: Integrates LLM providers using the OpenAI API, suitable for both InstructLab and Podman AI Studio."
msgstr "`quarkus-langchain4j-openai` :使用 OpenAI API 集成 LLM 提供商，适用于 InstructLab 和 Podman AI Studio。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "`quarkus-websockets-next`: Provides support for WebSocket communication."
msgstr "`quarkus-websockets-next` :为 WebSocket 通信提供支持。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "`quarkus-langchain4j-infinispan`: Integrates Infinispan with LangChain4j, allowing us to store and retrieve vector representations of text segments."
msgstr "`quarkus-langchain4j-infinispan` :将 Infinispan 与 LangChain4j 集成，允许我们存储和检索文本段的向量表示。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "`quarkus-web-bundler`: Bundles frontend resources with the Quarkus application."
msgstr "`quarkus-web-bundler` :将前端资源与 Quarkus 应用程序捆绑在一起。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "We also need a specific dependency to use the BGE-small embedding model:"
msgstr "我们还需要一个特定的依赖关系来使用 BGE-small 嵌入模型："

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "Configuration"
msgstr "配置"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "We need a bit of configuration to ensure our application uses Granite and sets up the Infinispan database correctly:"
msgstr "我们需要进行一些配置，以确保应用程序使用 Granite 并正确设置 Infinispan 数据库："

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "Configure the dimension of the vectors stored in Infinispan, which depends on the embedding model (BGE-small in our case)."
msgstr "配置存储在 Infinispan 中的向量的维度，这取决于嵌入模型（在我们的例子中为 BGE-small）。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid ""
"Configure the OpenAI service to use InstructLab.\n"
"You can replace the base URL with the one for Podman AI Studio if you prefer.\n"
"Indeed, InstructLab and Podman AI Studio expose an OpenAI-compatible API."
msgstr "配置 OpenAI 服务以使用 InstructLab。如果您愿意，可以用 Podman AI Studio 的 URL 替换基本 URL。事实上，InstructLab 和 Podman AI Studio 提供了与 OpenAI 兼容的应用程序接口。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "Set the default embedding model to BGE-small."
msgstr "将默认嵌入模型设为 BGE-small。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "The Ingestor"
msgstr "摄取者"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid ""
"With the configuration in place, let's implement the https://github.com/cescoffier/quarkus-granite-rag-demo/blob/main/src/main/java/me/escoffier/granite/rag/Ingestion.java[ingestion part (Ingestion.java)].\n"
"The ingestor reads documents from the `documents` directory, splits them into text segments, computes their vector representations, and stores them in Infinispan:"
msgstr "配置就绪后，让我们来实现 link:https://github.com/cescoffier/quarkus-granite-rag-demo/blob/main/src/main/java/me/escoffier/granite/rag/Ingestion.java[摄取部分（Ingestion.java）] 。摄取器从 `documents` 目录中读取文档，将它们分割成文本段，计算它们的向量表示并将它们存储到 Infinispan 中："

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "The `@Startup` annotation ensures that the ingestion process starts when the application launches."
msgstr "`@Startup` 注释可确保在应用程序启动时开始摄取进程。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "The `Ingestion` class uses an (automatically injected) `EmbeddingStore<TextSegment>` (Infinispan) and an `EmbeddingModel` (BGE-small)."
msgstr "`Ingestion` 类使用（自动注入的） `EmbeddingStore<TextSegment>` (Infinispan) 和 `EmbeddingModel` (BGE-small)。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid ""
"We use a simple document splitter (`recursive(1024, 0)`) to split the documents into text segments.\n"
"More advanced techniques may be used to improve the accuracy of the RAG model."
msgstr "我们使用简单的文档分割器 ( `recursive(1024, 0)` ) 将文档分割成文本段。还可以使用更先进的技术来提高 RAG 模型的准确性。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "The retriever"
msgstr "寻回犬"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid ""
"Next, let's implement the https://github.com/cescoffier/quarkus-granite-rag-demo/blob/main/src/main/java/me/escoffier/granite/rag/Retriever.java[retriever (Retriever.java)].\n"
"The retriever finds the most relevant text segments in Infinispan based on the user query:"
msgstr "接下来，让我们来实现 link:https://github.com/cescoffier/quarkus-granite-rag-demo/blob/main/src/main/java/me/escoffier/granite/rag/Retriever.java[检索器（Retriever.java）] 。检索器会根据用户查询在 Infinispan 中找到最相关的文本片段："

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid ""
"To implement a retriever, expose a bean that implements the `Supplier<RetrievalAugmentor>` interface.\n"
"The `Retriever` class uses `EmbeddingStore<TextSegment>` (Infinispan) and `EmbeddingModel` (BGE-small) to build the retriever."
msgstr "要实现检索器，需要公开一个实现 `Supplier<RetrievalAugmentor>` 接口的 Bean。 `Retriever` 类使用 `EmbeddingStore<TextSegment>` (Infinispan) 和 `EmbeddingModel` (BGE-small) 来构建检索器。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid ""
"The `maxResults` method in the EmbeddingStoreContentRetriever builder specifies the number of text segments to retrieve.\n"
"Since our segments are large, we retrieve only two segments."
msgstr "EmbeddingStoreContentRetriever 生成器中的 `maxResults` 方法指定了要检索的文本片段数量。由于我们的片段较多，因此只检索两个片段。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "The AI Service"
msgstr "人工智能服务"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "The https://github.com/cescoffier/quarkus-granite-rag-demo/blob/main/src/main/java/me/escoffier/granite/rag/ChatBot.java[AI Service (ChatBot.java)] is the core component of our chatbot, combining the capabilities of the retriever and the Granite LLM to generate appropriate responses."
msgstr "link:https://github.com/cescoffier/quarkus-granite-rag-demo/blob/main/src/main/java/me/escoffier/granite/rag/ChatBot.java[人工智能服务（ChatBot.java）] 是我们聊天机器人的核心组件，它结合了寻回器和花岗岩 LLM 的功能，以生成适当的回复。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "With Quarkus, implementing an AI service is straightforward:"
msgstr "有了 Quarkus，实施人工智能服务就变得简单易行："

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "The `@RegisterAiService` annotation specifies the retrieval augmentor to use, which in our case is the `Retriever` bean defined earlier."
msgstr "`@RegisterAiService` 注解指定了要使用的检索增强器，在我们的例子中就是前面定义的 `Retriever` Bean。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "The `@SystemMessage` annotation provides the main instructions for the AI model."
msgstr "`@SystemMessage` 注释为人工智能模型提供了主要指令。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "The `@SessionScoped` annotation ensures that the AI service is stateful, maintaining context between user interactions for more engaging conversations."
msgstr "`@SessionScoped` 注释可确保人工智能服务是有状态的，从而保持用户交互之间的上下文，使对话更具吸引力。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "The `ChatBot` interface defines a single method, `chat`, which takes a user question as input and returns the chatbot's response."
msgstr "`ChatBot` 接口定义了一个方法 `chat` ，该方法将用户问题作为输入，并返回聊天机器人的回复。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "The WebSocket endpoint"
msgstr "WebSocket 端点"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "The final piece is the https://github.com/cescoffier/quarkus-granite-rag-demo/blob/main/src/main/java/me/escoffier/granite/rag/ChatWebSocket.java[WebSocket endpoint (ChatWebSocket.java)], which serves as the communication bridge between the chatbot's backend and the frontend interface:"
msgstr "最后一部分是 link:https://github.com/cescoffier/quarkus-granite-rag-demo/blob/main/src/main/java/me/escoffier/granite/rag/ChatWebSocket.java[WebSocket 端点（ChatWebSocket.java）] ，它是聊天机器人后端和前端界面之间的通信桥梁："

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "The `@WebSocket` annotation specifies the WebSocket path."
msgstr "`@WebSocket` 注解指定了 WebSocket 路径。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "The `@OnOpen` method sends a welcome message when a user connects to the _WebSocket_."
msgstr "当用户连接到 _WebSocket_ 时， `@OnOpen` 方法会发送一条欢迎信息。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "The `@OnTextMessage` method processes the user's messages and returns the chatbot's responses, using the injected AI service."
msgstr "`@OnTextMessage` 方法使用注入的人工智能服务处理用户消息并返回聊天机器人的回复。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "That's it! Our chatbot is now ready to chat with users, providing contextually relevant responses based on the RAG pattern."
msgstr "就是这样！我们的聊天机器人现在可以与用户聊天，根据 RAG 模式提供与上下文相关的回复。"

#: _posts/2024-06-14-granite-rag.adoc
msgid "Running the application"
msgstr "运行应用程序"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid ""
"Let's run the application and see our chatbot in action.\n"
"First, clone the https://github.com/cescoffier/quarkus-granite-rag-demo/tree/main[repository] and run the following command:"
msgstr "让我们运行应用程序，看看聊天机器人的运行情况。首先，克隆版本 link:https://github.com/cescoffier/quarkus-granite-rag-demo/tree/main[库] 并运行以下命令："

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid ""
"This command starts the Quarkus application in development mode.\n"
"Ensure you have InstructLab or Podman AI Studio running to use the Granite LLM.\n"
"You will also need Docker or Podman to automatically start Infinispan."
msgstr "该命令以开发模式启动 Quarkus 应用程序。确保已运行 InstructLab 或 Podman AI Studio，以便使用 Granite LLM。您还需要使用 Docker 或 Podman 自动启动 Infinispan。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "Podman AI Studio or InstructLab?"
msgstr "Podman AI Studio 还是 InstructLab？"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid ""
"You can use either Podman AI Studio or InstructLab to run the Granite LLM locally.\n"
"Depending on the OS, Podman may not have GPU support. Thus, response time can be high.\n"
"In this case, InstructLab is the preferred option for better response times.\n"
"Typically, on a Mac, you would use InstructLab, while on Linux, Podman AI Studio shows great performances."
msgstr "您可以使用 Podman AI Studio 或 InstructLab 在本地运行 Granite LLM。根据操作系统的不同，Podman 可能不支持 GPU。因此，响应时间可能较长。在这种情况下，InstructLab 是响应时间更短的首选。通常情况下，在 Mac 上，您会使用 InstructLab，而在 Linux 上，Podman AI Studio 则表现出色。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid ""
"Once the application is up and running, open your browser and navigate to http://localhost:8080.\n"
"You should see the chatbot interface, where you can start chatting with Mona:"
msgstr "应用程序启动并运行后，打开浏览器并导航至 http://localhost:8080 。您应该会看到聊天机器人界面，可以开始与 Mona 聊天："

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "Summary"
msgstr "摘要"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid ""
"That's it!\n"
"With just a few lines of code, we have implemented a chatbot using the RAG pattern, combining the capabilities of the Granite LLM, Infinispan, and Quarkus.\n"
"This application runs entirely locally, eliminating the need for any cloud services and addressing privacy concerns."
msgstr "就是这样！只需几行代码，我们就利用 RAG 模式，结合 Granite LLM、Infinispan 和 Quarkus 的功能，实现了一个聊天机器人。该应用程序完全在本地运行，无需任何云服务，并解决了隐私问题。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid ""
"This is just an example of what you can achieve with the Quarkus LangChain4j extension.\n"
"You can easily extend this application by adding more advanced features, such as sophisticated document splitters, embedding models, or retrieval mechanisms.\n"
"Quarkus LangChain4J also provides support for https://docs.langchain4j.dev/tutorials/rag/#advanced-rag[_advanced_ RAG], many other LLM and embedding models and vector stores.\n"
"Find out more on https://docs.quarkiverse.io/quarkus-langchain4j/dev/index.html[Quarkus LangChain4J]."
msgstr "这只是使用 Quarkus LangChain4j 扩展实现功能的一个示例。您可以通过添加更多高级功能来轻松扩展此应用程序，例如复杂的文档分割器、嵌入模型或检索机制。Quarkus LangChain4J 还支持 link:https://docs.langchain4j.dev/tutorials/rag/#advanced-rag[高级RAG] 、许多其他 LLM 和嵌入模型以及向量存储。了解有关 link:https://docs.quarkiverse.io/quarkus-langchain4j/dev/index.html[Quarkus LangChain] 4J 的更多信息。"
