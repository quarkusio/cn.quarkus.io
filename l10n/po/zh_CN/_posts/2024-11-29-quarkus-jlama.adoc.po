msgid ""
msgstr ""
"Language: zh_CN\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"X-Generator: jekyll-l10n\n"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid ""
"Creating pure Java LLM infused application with Quarkus, Langchain4j and "
"Jlama"
msgstr "使用 Quarkus、Langchain4j 和 Jlama 创建注入纯 Java LLM 的应用程序"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid ""
"Creating pure Java LLM infused application with Quarkus, LangChain4j and "
"Jlama"
msgstr "使用 Quarkus、LangChain4j 和 Jlama 创建注入 LLM 的纯 Java 应用程序"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid ""
"Currently the vast majority of LLM-based applications rely on external "
"services provided by specialized companies. These services typically offer "
"the access to huge, general purpose models, implying energy consumption and "
"then costs that are proportional to the size of these models."
msgstr ""
"目前，绝大多数基于 LLM 的应用都依赖于专业公司提供的外部服务。这些服务通常提供对大型通用模型的访问，这意味着能源消耗和成本与这些模型的大小成正比。"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid ""
"Even worse, this usage pattern also comes with both privacy and security "
"concerns, since it is virtually impossible to be sure how those service "
"providers will eventually re-use the prompts of their customers, which in "
"some cases could also contain sensitive information."
msgstr ""
"更糟糕的是，这种使用模式还会带来隐私和安全问题，因为几乎不可能确定这些服务提供商最终会如何重新使用客户的提示信息，在某些情况下，这些提示信息还可能包含敏感信息。"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid ""
"For these reasons many companies are exploring the option of training or "
"fine-tuning smaller models that do not claim to be usable in a general "
"context, but that are tailored towards specific business needs and "
"subsequently running (serving in LLM terms) these models on premise or on "
"private clouds."
msgstr ""
"由于这些原因，许多公司正在探索培训或微调较小的模型，这些模型并不声称可以在一般情况下使用，而是根据特定业务需求量身定制的，随后在企业内部或私有云上运行（用 "
"LLM 术语来说就是服务）这些模型。"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid ""
"The features provided by these specialized models need to be integrated into "
"the existing software infrastructure, that in the enterprise world are very "
"often written in Java. This could be accomplished following a traditional "
"client-server architecture, for instance serving the model through an "
"external server like https://ollama.com/[Ollama] and querying it through "
"REST calls. While this should not present any particular problem for Java "
"developers, they could work more efficiently, if they could consume the "
"model directly in Java and without any need to install additional tools. "
"Finally the possibility of embedding the LLM interaction directly in the "
"same Java process running the application will make it easier to move from "
"local dev to deployment, relieving IT from the burden of managing an "
"external server, thus bypassing the need for a more mature platform "
"engineering strategy. This is where Jlama comes into play."
msgstr ""
"这些专业模型提供的功能需要集成到现有的软件基础设施中，而在企业界，这些基础设施通常是用 Java 编写的。这可以通过传统的客户端-"
"服务器架构来实现，例如通过 link:https://ollama.com/[Ollama] 等外部服务器提供模型，并通过 REST "
"调用进行查询。虽然这不会给 Java 开发人员带来任何特别的问题，但如果他们能直接使用 Java "
"中的模型，而无需安装额外的工具，他们的工作效率会更高。最后，在运行应用程序的同一个 Java 进程中直接嵌入 LLM "
"交互的可能性将使从本地开发到部署变得更加容易，从而减轻 IT 部门管理外部服务器的负担，从而绕过对更成熟的平台工程战略的需求。这就是 Jlama "
"发挥作用的地方。"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "How and why executing LLM inference in pure Java with Jlama"
msgstr "如何以及为何使用 Jlama 在纯 Java 中执行 LLM 推理"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid ""
"https://github.com/tjake/Jlama[Jlama] is a library allowing to execute LLM "
"inference in pure Java. It supports many LLM model families like Llama, "
"Mistral, Qwen2 and Granite. It also implements out-of-the-box many useful "
"LLM related features like functions calling, models quantization, mixture of "
"experts and even distributed inference."
msgstr ""
"link:https://github.com/tjake/Jlama[Jlama] 是一个允许用纯 Java 执行 LLM 推理的库。它支持许多 "
"LLM 模型系列，如 Llama、Mistral、Qwen2 和 Granite。它还实现了许多有用的 LLM "
"相关功能，如函数调用、模型量化、专家混合，甚至分布式推理。"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid ""
"Jlama is well integrated with Quarkus through the https://quarkus.io/"
"extensions/io.quarkiverse.langchain4j/quarkus-langchain4j-jlama/[dedicated "
"lanchain4j based extension]. Note that for performance reasons Jlama uses "
"the https://openjdk.org/jeps/469[Vector API] which is still in preview in "
"Java 23, and very likely will be released as a supported feature in Java 25."
msgstr ""
"通过 link:https://quarkus.io/extensions/io.quarkiverse.langchain4j/quarkus-"
"langchain4j-jlama/[基于 lanchain4j 的专用扩展] ，Jlama 已与 Quarkus "
"完美集成。请注意，出于性能方面的考虑，Jlama 使用了 link:https://openjdk.org/jeps/469[向量 API] ，该 "
"link:https://openjdk.org/jeps/469[API] 在 Java 23 中仍处于预览阶段，很有可能在 Java 25 "
"中作为支持功能发布。"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid ""
"In essence Jlama makes it possible to serve an LLM in Java, directly "
"embedded in the same JVM running your Java application, but why could this "
"be useful? Actually this is desirable in many use cases and presents a "
"number of relevant advantages like the following:"
msgstr ""
"从本质上讲，Jlama 使直接嵌入到运行 Java 应用程序的同一 JVM 中的 Java LLM "
"服务成为可能，但为什么这会有用呢？事实上，这在许多用例中都是可取的，并具有以下一些相关优势："

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid ""
"*Similar lifecycle between model and app*: There can be use cases where the "
"model and the application using it have the same lifecycle, so that the "
"development of a new feature in the application also requires a change in "
"the model. Similarly, since prompts are very dependent on the model, when "
"the model gets updated even through fine-tuning, your prompt may need to be "
"replaced. In these situations having the model embedded in the application "
"will contribute to simplify the versioning and traceability of the "
"development cycle."
msgstr ""
"*模型和应用程序的生命周期相似* "
"：在某些用例中，模型和使用模型的应用程序具有相同的生命周期，因此开发应用程序中的新功能也需要更改模型。同样，由于提示非常依赖于模型，当模型更新时，即使是通过微调，也可能需要更换提示。在这种情况下，将模型嵌入到应用程序中将有助于简化开发周期的版本控制和可追溯性。"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid ""
"*Fast development/prototyping*: Not having to install, configure and "
"interact with an external server can make the development of a LLM-based "
"Java application much easier."
msgstr "*快速开发/原型设计* ：无需安装、配置和与外部服务器交互，可使基于 LLM 的 Java 应用程序的开发变得更加容易。"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid ""
"*Easy models testing*: Running the LLM inference embedded in the JVM also "
"makes it easier to test different models and their integration during the "
"development phase."
msgstr "*方便模型测试* ：在 JVM 中运行 LLM 推理，还能在开发阶段更轻松地测试不同模型及其集成。"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid ""
"*Security*: Performing the model inference in the same JVM instance that is "
"run the application using it, eliminates the need of interacting with the "
"LLM only through REST calls, thus preventing the leak of private data and "
"allowing to enforce user authorization at a much finer grain."
msgstr ""
"*安全性* ：在运行应用程序的同一 JVM 实例中执行模型推理，无需仅通过 REST 调用与 LLM "
"进行交互，从而防止了私人数据的泄漏，并能以更精细的方式执行用户授权。"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid ""
"*Monolithic applications support*: The former point will be also beneficial "
"for users still running monolithic applications, who in this way will be "
"also able to include LLM-based capabilities in those applications without "
"changing their architecture or platform."
msgstr ""
"*支持单体应用程序* ：前一点对仍在运行单体应用程序的用户也有好处，因为这样他们就能在不改变架构或平台的情况下，在这些应用程序中加入基于 LLM "
"的功能。"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid ""
"*Monitoring and Observability*: Running the LLM inference in pure Java will "
"also allow simplify monitoring and observability, gathering statistics on "
"the reliability and speed of the LLM response."
msgstr "*监控和可观察性* ：用纯 Java 运行 LLM 推理还可以简化监控和可观察性，收集有关 LLM 响应的可靠性和速度的统计数据。"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid ""
"*Developer Experience*: Debuggability will be simplified in the same way, "
"allowing the Java developer to also navigate and debug the Jlama code if "
"necessary."
msgstr "*开发人员体验* ：将以同样的方式简化调试功能，使 Java 开发人员在必要时也能浏览和调试 Jlama 代码。"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid ""
"*Distribution*: Having the possibility to run LLM inference embedded in the "
"same Java process will also make it possible to include the model itself "
"into the same application package of the application using it (even though "
"this could probably be advisable only in very specific circumstances)."
msgstr ""
"*分布* ：如果可以在同一个 Java 进程中运行 LLM "
"推理，还可以将模型本身包含在使用该模型的应用程序的同一个应用程序包中（尽管这可能只在非常特殊的情况下才是明智之举）。"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid ""
"*Edge friendliness*: The possibility of implementing and deploying a self-"
"contained LLM-capable Java application will also make it a better fit than a "
"client/server architecture for edge environments."
msgstr "*边缘友好性* ：在边缘环境中，实施和部署自足的 LLM Java 应用程序比客户端/服务器架构更合适。"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid ""
"*Embedding of auxiliary LLMs*: Many applications, especially the ones "
"relying on agentic AI patterns, uses many different LLMs at once. For "
"instance a smaller LLM could be used to validate and approve the responses "
"of the main bigger one. In this case an hybrid approach could be convenient, "
"embedding the smaller auxiliary LLMs while keeping serving the main one "
"through a dedicated server."
msgstr ""
"*嵌入辅助 LLM* ：许多应用，尤其是依赖于代理人工智能模式的应用，会同时使用许多不同的 LLM。例如，一个较小的 LLM 可用于验证和批准较大的主要 "
"LLM 的响应。在这种情况下，可以采用混合方法，嵌入较小的辅助 LLM，同时通过专用服务器为主要 LLM 提供服务。"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "The site summarizer: a pure Java LLM-based application"
msgstr "网站摘要器：基于 LLM 的纯 Java 应用程序"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid ""
"To demonstrate how Quarkus, Langchain4j and Jlama make straightforward to "
"create a pure Java LLM infused application, where the LLM inference is "
"directly embedded in the same JVM running the application I created a https:/"
"/github.com/mariofusco/site-summarizer[simple project] that uses a LLM to "
"automatically generate the summarization of a Wikipedia page or more in "
"general of a blog post taken from any website."
msgstr ""
"为了演示 Quarkus、Langchain4j 和 Jlama 如何直接创建纯 Java LLM 注入应用程序，其中 LLM "
"推理直接嵌入到运行应用程序的同一个 JVM 中，我创建了一个 link:https://github.com/mariofusco/site-"
"summarizer[简单的项目] ，该 link:https://github.com/mariofusco/site-summarizer[项目] "
"使用 LLM 自动生成维基百科页面的摘要，或者更笼统地说，自动生成从任何网站获取的博文的摘要。"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "This video demonstrates how the application works."
msgstr "这段视频演示了应用程序的工作原理。"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid ""
"Out-of-the-box this project uses a https://huggingface.co/tjake/Llama-3.2-1B-"
"Instruct-JQ4[small Llama-3.2 model with 4-bit quantization]. When the "
"application is compiled for the first time the model is automatically "
"downloaded locally by Jlama from the Huggingface repository. However it is "
"possible to replace this model and experiment with any other one by simply "
"editing the https://github.com/mariofusco/site-summarizer/blob/main/src/main/"
"resources/application.properties#L4[quarkus.langchain4j.jlama.chat-model."
"model-name property] in the application.properties file."
msgstr ""
"在开箱即用的情况下，该项目使用了一个 link:https://huggingface.co/tjake/Llama-3.2-1B-Instruct-"
"JQ4[4 位量化的小型 Llama-3.2 模型] 。首次编译应用程序时，Jlama 会自动从 Huggingface "
"资源库下载该模型到本地。不过，只需编辑 application.properties 文件中的 link:https://github.com/"
"mariofusco/site-summarizer/blob/main/src/main/resources/application."
"properties#L4[quarkus.langchain4j.jlama.chat-model.model-name 属性] "
"，就可以替换该模型并尝试使用其他模型。"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid ""
"After having compiled and packaged the project with `mvn clean package`, the "
"simplest way to use it is launching the jar passing as argument the URL of "
"the web page containing the article that you want to summarize, something "
"like:"
msgstr ""
"使用 `mvn clean package` 对项目进行编译和打包后，最简单的使用方法是启动 jar，并将包含文章摘要的网页 URL "
"作为参数传递，类似于这样："

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "that will generate an output like the following:"
msgstr "将产生类似下面的输出："

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid ""
"Note that it is necessary to launch the JVM with a few additional arguments "
"that enable the access to the Vector API which is still a Java preview "
"feature, but it is internally used by Jlama to speed up the computation."
msgstr ""
"需要注意的是，启动 JVM 时需要附加一些参数，以便访问矢量 API，这仍然是 Java 的预览功能，但 Jlama 内部使用它来加快计算速度。"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid ""
"As better clarified by the readme of the project, there's a dedicated "
"operating mode to process Wikipedia pages and also the possibility to expose "
"this service through a REST endpoint."
msgstr "正如项目的自述文件所进一步说明的，维基百科页面有专门的运行模式来处理，而且还可以通过 REST 端点公开这项服务。"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid ""
"The internal implementation of this project is relatively straightforward: "
"after having programmatically extracted the text to be summarized from the "
"HTML page containing it, that text is sent to Jlama to be processed via a "
"usual Langchain4j AiService."
msgstr ""
"这个项目的内部实现相对简单：从包含摘要的 HTML 页面中以编程方式提取文本后，将文本发送到 Jlama，通过常见的 Langchain4j "
"AiService 进行处理。"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid ""
"As anticipated, despite this implementation looks identical to any other LLM "
"inference engine integrations, in this case there isn't any remote call to "
"an external service, but the LLM inference is performed directly inside the "
"same JVM running the application."
msgstr ""
"正如预期的那样，尽管这一实现看起来与其他 LLM 推理引擎集成完全相同，但在这种情况下，并没有对外部服务进行任何远程调用，而是直接在运行应用程序的同一 "
"JVM 中执行 LLM 推理。"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid ""
"The combination of the 2 trends of the increasing spread of small and "
"tailored models and the adoption of these models in the enterprise software "
"development world will very likely promote the use of similar solutions in "
"the near future."
msgstr "小型和定制模式的日益普及以及企业软件开发领域对这些模式的采用，这两种趋势的结合将很有可能在不久的将来促进类似解决方案的使用。"
