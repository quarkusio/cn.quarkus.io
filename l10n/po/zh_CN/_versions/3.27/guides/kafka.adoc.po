msgid ""
msgstr ""
"Language: zh_CN\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"X-Generator: jekyll-l10n\n"

#: _versions/3.27/guides/kafka.adoc
msgid "Apache Kafka Reference Guide"
msgstr "Apache Kafka参考指南"

#: _versions/3.27/guides/kafka.adoc
msgid "This reference guide demonstrates how your Quarkus application can utilize Quarkus Messaging to interact with Apache Kafka."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Introduction"
msgstr "简介"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"https://kafka.apache.org[Apache Kafka] is a popular open-source distributed event streaming platform.\n"
"It is used commonly for high-performance data pipelines, streaming analytics, data integration, and mission-critical applications.\n"
"Similar to a message queue, or an enterprise messaging platform, it lets you:"
msgstr "link:https://kafka.apache.org[Apache Kafka] 是一个流行的开源分布式事件流平台。它通常用于高性能数据管道、流式分析、数据集成以及任务关键型应用。类似于消息队列或企业消息平台，它可以允许您："

#: _versions/3.27/guides/kafka.adoc
msgid "*publish* (write) and *subscribe* to (read) streams of events, called _records_."
msgstr "*发布* (写)以及 *订阅* (读)事件流，称为 _记录_ 。\n"

#: _versions/3.27/guides/kafka.adoc
msgid "*store* streams of records durably and reliably inside _topics_."
msgstr "在 _topic_ 内持久而可靠地 *存储* 流式记录。\n"

#: _versions/3.27/guides/kafka.adoc
msgid "*process* streams of records as they occur or retrospectively."
msgstr "对流式记录进行起始或回溯*处理*。\n"

#: _versions/3.27/guides/kafka.adoc
msgid "And all this functionality is provided in a distributed, highly scalable, elastic, fault-tolerant, and secure manner."
msgstr "而所有这些功能都是以分布式、高可扩展性、弹性、容错以及安全的方式提供的。"

#: _versions/3.27/guides/kafka.adoc
msgid "Quarkus Extension for Apache Kafka"
msgstr "Apache Kafka Quarkus扩展"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Quarkus provides support for Apache Kafka through https://smallrye.io/smallrye-reactive-messaging/[SmallRye Reactive Messaging] framework.\n"
"Based on Eclipse MicroProfile Reactive Messaging specification 2.0, it proposes a flexible programming model bridging CDI and event-driven."
msgstr "Quarkus通过 link:https://smallrye.io/smallrye-reactive-messaging/[SmallRye Reactive Messaging] 框架为Apache Kafka提供支持。基于Eclipse MicroProfile Reactive Messaging 2.0 规范，框架提供了一种灵活的基于CDI和事件驱动的编程模型。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"This guide provides an in-depth look on Apache Kafka and SmallRye Reactive Messaging framework.\n"
"For a quick start take a look at xref:kafka-getting-started.adoc[Getting Started to Quarkus Messaging with Apache Kafka]."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "You can add the `messaging-kafka` extension to your project by running the following command in your project base directory:"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "This will add the following to your build file:"
msgstr "这会将以下内容添加到你的构建文件中:"

#: _versions/3.27/guides/kafka.adoc
msgid "pom.xml"
msgstr "pom.xml"

#: _versions/3.27/guides/kafka.adoc
msgid "build.gradle"
msgstr "build.gradle"

#: _versions/3.27/guides/kafka.adoc
msgid "The extension includes `kafka-clients` version 3.2.1 as a transitive dependency and is compatible with Kafka brokers version 2.x."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Configuring SmallRye Kafka Connector"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Because SmallRye Reactive Messaging framework supports different messaging backends like Apache Kafka, AMQP, Apache Camel, JMS, MQTT, etc., it employs a generic vocabulary:"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Applications send and receive *messages*. A message wraps a _payload_ and can be extended with some _metadata_. With the Kafka connector, a _message_ corresponds to a Kafka _record_."
msgstr "应用程序发送和接收 *messages* 。一条消息包含一个 _payload_ ，并可以用一些 _metadata_ 进行扩展。通过Kafka connector，一条 _message_ 对应于一条Kafka _record_。"

#: _versions/3.27/guides/kafka.adoc
msgid "Messages transit on *channels*. Application components connect to channels to publish and consume messages. The Kafka connector maps _channels_ to Kafka _topics_."
msgstr "信息在 *channels* 上传输。应用程序组件通过连接 channels 来发布和消费消息。Kafka connector将 _channels_ 映射到Kafka的 _topics_上 。"

#: _versions/3.27/guides/kafka.adoc
msgid "Channels are connected to message backends using *connectors*. Connectors are configured to map incoming messages to a specific channel (consumed by the application) and collect outgoing messages sent to a specific channel. Each connector is dedicated to a specific messaging technology. For example, the connector dealing with Kafka is named `smallrye-kafka`."
msgstr "Channels 通过 *connectors* 连接到消息后端。Connectors通过配置将传入的消息映射到一个指定channel上(该channel由应用程序来消费)，并对发送到指定channel的消息进行收集。每个connector都专用于某种特定的消息传递技术。例如，与Kafka交互的的connector被命名为 `smallrye-kafka` 。"

#: _versions/3.27/guides/kafka.adoc
msgid "A minimal configuration for the Kafka connector with an incoming channel looks like the following:"
msgstr "一个配有消息接收channel的Kafka connector的最小配置如下所示："

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Configure the broker location for the production profile. You can configure it globally or per channel using `mp.messaging.incoming.$channel.bootstrap.servers` property.\n"
"In dev mode and when running tests, <<kafka-dev-services>> automatically starts a Kafka broker.\n"
"When not provided this property defaults to `localhost:9092`."
msgstr "请务必为生产环境配置broker地址。您可以在全局环境配置或使用 `mp.messaging.incoming.$channel.bootstrap.servers` 属性来针对特定channel配置。在开发模式和运行测试时， <<kafka-dev-services>>会自动启动一个Kafka broker。如果没有提供这个属性，则默认为 `localhost:9092` 。"

#: _versions/3.27/guides/kafka.adoc
msgid "Configure the connector to manage the prices channel. By default, the topic name is same as the channel name. You can configure the topic attribute to override it."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "The `%prod` prefix indicates that the property is only used when the application runs in prod mode (so not in dev or test). Refer to the xref:config-reference.adoc#profiles[Profile documentation] for further details."
msgstr "`%prod` 前缀表示该属性只在应用程序运行在生产模式下时生效(而不是在开发或测试模式)。更多细节请参考 xref:config-reference.adoc#profiles[Profile documentation]。"

#: _versions/3.27/guides/kafka.adoc
msgid "Connector auto-attachment"
msgstr "Connector auto-attachment"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"If you have a single connector on your classpath, you can omit the `connector` attribute configuration.\n"
"Quarkus automatically associates _orphan_ channels to the (unique) connector found on the classpath.\n"
"_Orphans_ channels are outgoing channels without a downstream consumer or incoming channels without an upstream producer."
msgstr "如果在你的classpath上有一个连接器，你可以省略 `connector` 属性配置。Quarkus会自动将 _orphan_ 通道与classpath上找到的（唯一的）连接器联系起来。 _Orphans_ 通道是没有下游消费者的传出通道或没有上游生产者的传入通道。"

#: _versions/3.27/guides/kafka.adoc
msgid "This auto-attachment can be disabled using:"
msgstr "可以用以下方法禁用这种auto-attachment功能："

#: _versions/3.27/guides/kafka.adoc
msgid "Receiving messages from Kafka"
msgstr "接收来自Kafka的消息"

#: _versions/3.27/guides/kafka.adoc
msgid "Continuing from the previous minimal configuration, your Quarkus application can receive message payload directly:"
msgstr "让我们继续刚才的最小配置。您的Quarkus应用程序可以直接接收消息payload："

#: _versions/3.27/guides/kafka.adoc
msgid "There are several other ways your application can consume incoming messages:"
msgstr "您的应用程序还可以通过另外集中方式来消费接收到的消息："

#: _versions/3.27/guides/kafka.adoc
msgid "Message"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"The `Message` type lets the consuming method access the incoming message metadata and handle the acknowledgment manually.\n"
"We'll explore different acknowledgment strategies in <<commit-strategies>>."
msgstr "`Message` 类型允许consuming methond访问接收到消息的metadata并手动进行确认。我们将在 <<commit-strategies>>中探讨不同的确认策略。"

#: _versions/3.27/guides/kafka.adoc
msgid "If you want to access the Kafka record objects directly, use:"
msgstr "如果您想直接访问Kafka record对象，请使用："

#: _versions/3.27/guides/kafka.adoc
msgid "ConsumerRecord"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"`ConsumerRecord` is provided by the underlying Kafka client and can be injected directly to the consumer method.\n"
"Another simpler approach consists in using `Record`:"
msgstr "`ConsumerRecord` 由底层Kafka client提供，并且可以直接注入到consumer method中。另一种更简单的方法是使用  `Record`："

#: _versions/3.27/guides/kafka.adoc
msgid "Record"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "`Record` is a simple wrapper around key and payload of the incoming Kafka record."
msgstr "`Record` 提供了对接收到的Kafka record中key和payload的简单的包装。"

#: _versions/3.27/guides/kafka.adoc
msgid "@Channel"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Alternatively, your application can inject a `Multi` in your bean and subscribe to its events as the following example:"
msgstr "另外，您的应用程序可以在您的Bean中注入一个 `Multi` ，然后像下面的例子那样订阅它的事件："

#: _versions/3.27/guides/kafka.adoc
msgid ""
"This is a good example of how to integrate a Kafka consumer with another downstream,\n"
"in this example exposing it as a Server-Sent Events endpoint."
msgstr "这个例子很好的展示了如何将Kafka consumer与另一个downstream进行集成。在这个例子中，我们将这个downstream暴露为一个服务端事件节点(Server-Sent Events endpoint)。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"When consuming messages with `@Channel`, the application code is responsible for the subscription.\n"
"In the example above, the Quarkus REST (formerly RESTEasy Reactive) endpoint handles that for you."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Following types can be injected as channels:"
msgstr "以下类型可以作为 channels 被注入："

#: _versions/3.27/guides/kafka.adoc
msgid ""
"As with the previous `Message` example, if your injected channel receives payloads (`Multi<T>`), it acknowledges the message automatically, and support multiple subscribers.\n"
"If you injected channel receives Message (`Multi<Message<T>>`), you will be responsible for the acknowledgment and broadcasting.\n"
"We will explore sending broadcast messages in <<broadcasting-messages-on-multiple-consumers>>."
msgstr "如前面 `Message` 例子所示，如果您的注入channel接收到了playloads( `Multi<T>` )，它可以支持多订阅者自动确认消息。如果您的注入channel收到Message( `Multi<Message<T>>` )，那么您需要自行负责消息确认和广播。我们将在<<broadcasting-messages-on-multiple-consumers>> 中探讨消息的发送和广播。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Injecting `@Channel(\"prices\")` or having `@Incoming(\"prices\")` does not automatically configure the application to consume messages from Kafka.\n"
"You need to configure an inbound connector with `mp.messaging.incoming.prices\\...` or have an `@Outgoing(\"prices\")` method somewhere in your application (in which case, `prices` will be an in-memory channel)."
msgstr "注入 `@Channel(\"prices\")` 或使用 `@Incoming(\"prices\")` 无法通过配置使应用程序自动从Kafka消费消息。您需要用 `mp.messaging.incoming.prices...` 配置一个接收connector，或者在您的应用程序中使用 `@Outgoing(\"prices\")` 方法(在这种情况下， `prices` 将是一个内存型channel)。"

#: _versions/3.27/guides/kafka.adoc
msgid "Blocking processing"
msgstr "阻塞处理"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Reactive Messaging invokes your method on an I/O thread.\n"
"See the xref:quarkus-reactive-architecture.adoc[Quarkus Reactive Architecture documentation] for further details on this topic.\n"
"But, you often need to combine Reactive Messaging with blocking processing such as database interactions.\n"
"For this, you need to use the `@Blocking` annotation indicating that the processing is _blocking_ and should not be run on the caller thread."
msgstr "Reactive Messaging会在一个I/O线程中调用您的方法。关于这个话题的更多细节，请看  xref:quarkus-reactive-architecture.adoc[Quarkus Reactive Architecture documentation]  。但是您可能需要经常将Reactive Messaging 与阻塞式处理相结合使用，比如与数据库通信。为此，您需要使用 `@Blocking` 注解来表该明处理是 _阻塞的_ ，并且不在调用者线程中运行。"

#: _versions/3.27/guides/kafka.adoc
msgid "For example, The following code illustrates how you can store incoming payloads to a database using Hibernate with Panache:"
msgstr "例如，下面的代码演示了如何使用Hibernate与Panache将接收到的payload存储到数据库："

#: _versions/3.27/guides/kafka.adoc
msgid "The complete example is available in the `kafka-panache-quickstart` link:{quickstarts-tree-url}/kafka-panache-quickstart[directory]."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "There are 2 `@Blocking` annotations:"
msgstr "有2种 `@Blocking` 注解："

#: _versions/3.27/guides/kafka.adoc
msgid "`io.smallrye.reactive.messaging.annotations.Blocking`"
msgstr "`io.smallrye.reactive.messaging.annotations.Blocking`"

#: _versions/3.27/guides/kafka.adoc
msgid "`io.smallrye.common.annotation.Blocking`"
msgstr "`io.smallrye.common.annotation.Blocking`"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"They have the same effect.\n"
"Thus, you can use both.\n"
"The first one provides more fine-grained tuning such as the worker pool to use and whether it preserves the order.\n"
"The second one, used also with other reactive features of Quarkus, uses the default worker pool and preserves the order."
msgstr "它们效果相同。因此，您可以随意使用。第一个提供了更精细的配置，比如worker pool以及是否保留顺序。第二种，同其他的Quarkus Reactive功能类似，使用默认的worker pool并且保留了顺序。"

#: _versions/3.27/guides/kafka.adoc
msgid "Detailed information on the usage of `@Blocking` annotation can be found in https://smallrye.io/smallrye-reactive-messaging/latest/concepts/blocking/[SmallRye Reactive Messaging – Handling blocking execution]."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "@RunOnVirtualThread"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "For running the blocking processing on Java _virtual threads_, see the xref:messaging-virtual-threads.adoc[Quarkus Virtual Thread support with Reactive Messaging documentation]."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "@Transactional"
msgstr "@Transactional"

#: _versions/3.27/guides/kafka.adoc
msgid "If your method is annotated with `@Transactional`, it will be considered _blocking_ automatically, even if the method is not annotated with `@Blocking`."
msgstr "如果您的方法加了 `@Transactional` 注解，那么它即使没有添加 `@Blocking` 注解也将被自动视为 _阻塞_ 方法 。"

#: _versions/3.27/guides/kafka.adoc
msgid "Acknowledgment Strategies"
msgstr "确认策略(Acknowledgment Strategies)"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"All messages received by a consumer must be acknowledged.\n"
"In the absence of acknowledgment, the processing is considered in error.\n"
"If the consumer method receives a `Record` or a payload, the message will be acked on method return, also known as `Strategy.POST_PROCESSING`.\n"
"If the consumer method returns another reactive stream or `CompletionStage`, the message will be acked when the downstream message is acked.\n"
"You can override the default behavior to ack the message on arrival (`Strategy.PRE_PROCESSING`),\n"
"or do not ack the message at all (`Strategy.NONE`) on the consumer method as in the following example:"
msgstr "消费者收到的所有消息都必须被确认(acknowleged)。在没有确认的情况下，消息处理会出错。如果消费者方法收到一个 `Record` 或一个payload，该消息将通过方法返回被确认，也被称为 `Strategy.POST_PROCESSING` 。如果消费者方法返回另一个reactive stream或 `CompletionStage` ，那么当下游消息(downstream message)被确认时，该消息也将被确认。您可以覆盖默认行为从而在消息到达时进行确认( `Strategy.PRE_PROCESSING` )，或者在消费者方法中不进行任何确认( `Strategy.NONE` )，如下例所示："

#: _versions/3.27/guides/kafka.adoc
msgid ""
"If the consumer method receives a `Message`, the acknowledgment strategy is `Strategy.MANUAL`\n"
"and the consumer method is in charge of ack/nack the message."
msgstr "如果消费者方法接收到一个 `Message` ，那么确认策略是 `Strategy.MANUAL` 并且消费者方法将负责对消息进行ack/nack。"

#: _versions/3.27/guides/kafka.adoc
msgid "As mentioned above, the method can also override the acknowledgment strategy to `PRE_PROCESSING` or `NONE`."
msgstr "如上所述，该方法还可以将确认策略设置为 `PRE_PROCESSING` 或 `NONE` 。"

#: _versions/3.27/guides/kafka.adoc
msgid "Commit Strategies"
msgstr "提交策略(Commit Strategies)"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"When a message produced from a Kafka record is acknowledged, the connector invokes a commit strategy.\n"
"These strategies decide when the consumer offset for a specific topic/partition is committed.\n"
"Committing an offset indicates that all previous records have been processed.\n"
"It is also the position where the application would restart the processing after a crash recovery or a restart."
msgstr "当一条由Kafka记录产生的消息被确认时，connector将会调用一个提交策略。这些策略决定了特定topic/分区(topic/partition)的消费者偏移将在何时被提交。提交一个偏移量(offset)表明所有之前的记录已经被处理了。它也是应用程序从崩溃中恢复后或重启后重新开始处理的位置。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Committing every offset has performance penalties as Kafka offset management can be slow.\n"
"However, not committing the offset often enough may lead to message duplication if the application crashes between two commits."
msgstr "由于Kafka的偏移量管理可能很慢，所以每次提交偏移量都会有性能上的损失。然而，如果程序在两次提交之间崩溃，不够频繁的偏移量提交可能会导致消息出现重复提交。"

#: _versions/3.27/guides/kafka.adoc
msgid "The Kafka connector supports three strategies:"
msgstr "Kafka connector支持三种策略："

#: _versions/3.27/guides/kafka.adoc
msgid ""
"`throttled` keeps track of received messages and commits an offset of the latest acked message in sequence (meaning, all previous messages were also acked).\n"
"This strategy guarantees at-least-once delivery even if the channel performs asynchronous processing.\n"
"The connector tracks the received records and periodically (period specified by `auto.commit.interval.ms`, default: 5000 ms) commits the highest consecutive offset.\n"
"The connector will be marked as unhealthy if a message associated with a record is not acknowledged in `throttled.unprocessed-record-max-age.ms` (default: 60000 ms).\n"
"Indeed, this strategy cannot commit the offset as soon as a single record processing fails.\n"
"If `throttled.unprocessed-record-max-age.ms` is set to less than or equal to `0`, it does not perform any health check verification.\n"
"Such a setting might lead to running out of memory if there are \"poison pill\" messages (that are never acked).\n"
"This strategy is the default if `enable.auto.commit` is not explicitly set to true."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"`checkpoint` allows persisting consumer offsets on a **state store**, instead of committing them back to the Kafka broker.\n"
"Using the `CheckpointMetadata` API, consumer code can persist a _processing state_ with the record offset to mark the progress of a consumer.\n"
"When the processing continues from a previously persisted offset, it seeks the Kafka consumer to that offset and also restores the persisted state, continuing the stateful processing from where it left off.\n"
"The checkpoint strategy holds locally the processing state associated with the latest offset, and persists it periodically to the state store (period specified by `auto.commit.interval.ms` (default: 5000)).\n"
"The connector will be marked as unhealthy if no processing state is persisted to the state store in `checkpoint.unsynced-state-max-age.ms` (default: 10000).\n"
"If `checkpoint.unsynced-state-max-age.ms` is set to less than or equal to 0, it does not perform any health check verification.\n"
"For more information, see <<stateful-processing-checkpointing>>"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"`latest` commits the record offset received by the Kafka consumer as soon as the associated message is acknowledged (if the offset is higher than the previously committed offset).\n"
"This strategy provides at-least-once delivery if the channel processes the message without performing any asynchronous processing. Specifically, the offset of the most recent acknowledged\n"
"message will always be committed, even if older messages have not finished being processed. In case of an incident such as a crash, processing would restart after the last commit, leading\n"
"to older messages never being successfully and fully processed, which would appear as message loss.\n"
"This strategy should not be used in high load environment, as offset commit is expensive. However, it reduces the risk of duplicates."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"`ignore` performs no commit. This strategy is the default strategy when the consumer is explicitly configured with `enable.auto.commit` to true.\n"
"It delegates the offset commit to the underlying Kafka client.\n"
"When `enable.auto.commit` is `true` this strategy **DOES NOT** guarantee at-least-once delivery.\n"
"SmallRye Reactive Messaging processes records asynchronously, so offsets may be committed for records that have been polled but not yet processed.\n"
"In case of a failure, only records that were not committed yet will be re-processed."
msgstr "`ignore` 不执行任何提交。当消费者的 `enable.auto.commit` 属性被明确配置为true时，该策略将是默认策略。它将偏移量的提交委托给底层Kafka client负责。当 `enable.auto.commit` 为true的时候 ，该策略 *不* 保证至少会有一次提交。SmallRye Reactive Messaging是异步处理记录的，所以那些已经被轮询但尚未处理的record的偏移量有可能会被提交。如果提交失败，只有那些尚未被提交的record才会被重新处理。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"The Kafka connector disables the Kafka auto commit when it is not explicitly enabled. This behavior differs from the traditional Kafka consumer.\n"
"If high throughput is important for you, and you are not limited by the downstream, we recommend to either:"
msgstr "当Kafka connector没有明确启用时，它将禁用Kafka自动提交。这与传统的Kafka消费者不同。如果高吞吐量对您来说很重要而且您不受下游的限制，我们建议要么："

#: _versions/3.27/guides/kafka.adoc
msgid "use the `throttled` policy,"
msgstr "使用 `throttled` 策略,"

#: _versions/3.27/guides/kafka.adoc
msgid "or set `enable.auto.commit` to true and annotate the consuming method with `@Acknowledgment(Acknowledgment.Strategy.NONE)`."
msgstr "要么将 `enable.auto.commit` 设置为true，并在consuming方法中使用 `@Acknowledgment(Acknowledgment.Strategy.NONE)` 注解。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"SmallRye Reactive Messaging enables implementing custom commit strategies.\n"
"See https://smallrye.io/smallrye-reactive-messaging/latest/kafka/receiving-kafka-records/#acknowledgement[SmallRye Reactive Messaging documentation] for more information."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Error Handling Strategies"
msgstr "错误处理策略(Error Handling Strategies)"

#: _versions/3.27/guides/kafka.adoc
msgid "If a message produced from a Kafka record is nacked, a failure strategy is applied. The Kafka connector supports three strategies:"
msgstr "如果从Kafka record中产生的消息未被确认，那么一个失败策略就会被启用。Kafka connector支持三种策略："

#: _versions/3.27/guides/kafka.adoc
msgid "`fail`: fail the application, no more records will be processed (default strategy). The offset of the record that has not been processed correctly is not committed."
msgstr "`fail` ：直接使程序失败，不再处理更多的记录(默认策略)。未被正确处理的记录的偏移量不会被提交。"

#: _versions/3.27/guides/kafka.adoc
msgid "`ignore`: the failure is logged, but the processing continue. The offset of the record that has not been processed correctly is committed."
msgstr "`ignore` ：记录失败的日志，但消息处理将继续进行。没有被正确处理的记录的偏移量会被提交。"

#: _versions/3.27/guides/kafka.adoc
msgid "`dead-letter-queue`: the offset of the record that has not been processed correctly is committed, but the record is written to a Kafka dead letter topic."
msgstr "`dead-letter-queue` ：未被正确处理的记录的偏移量会被提交，但该记录会被写入Kafka的dead letter topic。"

#: _versions/3.27/guides/kafka.adoc
msgid "The strategy is selected using the `failure-strategy` attribute."
msgstr "失败策略通过 `failure-strategy` 属性来设置。"

#: _versions/3.27/guides/kafka.adoc
msgid "In the case of `dead-letter-queue`, you can configure the following attributes:"
msgstr "在 `dead-letter-queue` 情况下 ，您可以配置以下属性："

#: _versions/3.27/guides/kafka.adoc
msgid "`dead-letter-queue.topic`: the topic to use to write the records not processed correctly, default is `dead-letter-topic-$channel`, with `$channel` being the name of the channel."
msgstr "`dead-letter-queue.topic` : 该topic用来保存未被正确处理的记录，默认为 `dead-letter-topic-$channel` ， `$channel` 是channel的名称。"

#: _versions/3.27/guides/kafka.adoc
msgid "`dead-letter-queue.key.serializer`: the serializer used to write the record key on the dead letter queue. By default, it deduces the serializer from the key deserializer."
msgstr "`dead-letter-queue.key.serializer` 该序列化器用来对记录到dead letter queue中的record key进行序列化。默认情况下，该序列化器会从key的反序列化器反推出。"

#: _versions/3.27/guides/kafka.adoc
msgid "`dead-letter-queue.value.serializer`: the serializer used to write the record value on the dead letter queue. By default, it deduces the serializer from the value deserializer."
msgstr "`dead-letter-queue.value.serializer` :该序列化器用来对记录到dead letter queue中的record value进行序列化。默认情况下，该序列化器会从value的反序列化器反推出。"

#: _versions/3.27/guides/kafka.adoc
msgid "The record written on the dead letter queue contains a set of additional headers about the original record:"
msgstr "所有写入dead letter queue中的记录将包含一组关于原始记录的附加headers："

#: _versions/3.27/guides/kafka.adoc
msgid "*dead-letter-reason*: the reason of the failure"
msgstr "*dead-letter-reason* ：失败原因\n"

#: _versions/3.27/guides/kafka.adoc
msgid "*dead-letter-cause*: the cause of the failure if any"
msgstr "*dead-letter-cause* ：失败的起因(如果有)。\n"

#: _versions/3.27/guides/kafka.adoc
msgid "*dead-letter-topic*: the original topic of the record"
msgstr "*dead-letter-topic* ：失败记录的原始topic\n"

#: _versions/3.27/guides/kafka.adoc
msgid "*dead-letter-partition*: the original partition of the record (integer mapped to String)"
msgstr "*dead-letter-partition* ：失败记录的原始分区(integer映射为String)\n"

#: _versions/3.27/guides/kafka.adoc
msgid "*dead-letter-offset*: the original offset of the record (long mapped to String)"
msgstr "*dead-letter-offset* ：失败记录的原始偏移量(long映射为String)\n"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"SmallRye Reactive Messaging enables implementing custom failure strategies.\n"
"See https://smallrye.io/smallrye-reactive-messaging/latest/kafka/receiving-kafka-records/#acknowledgement[SmallRye Reactive Messaging documentation] for more information."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Retrying processing"
msgstr "重试处理(Retrying processing)"

#: _versions/3.27/guides/kafka.adoc
msgid "You can combine Reactive Messaging with https://github.com/smallrye/smallrye-fault-tolerance[SmallRye Fault Tolerance], and retry processing if it failed:"
msgstr "您可以将Reactive Messaging与 link:https://github.com/smallrye/smallrye-fault-tolerance[SmallRye 容错]结合起来，如果失败的话可以进行重试："

#: _versions/3.27/guides/kafka.adoc
msgid "You can configure the delay, the number of retries, the jitter, etc."
msgstr "您可以对延迟，重试次数以及抖动(jitter)等处理方式进行设置。"

#: _versions/3.27/guides/kafka.adoc
msgid "If your method returns a `Uni` or `CompletionStage`, you need to add the `@NonBlocking` annotation:"
msgstr "如果您的方法返回一个 `Uni` 或 `CompletionStage` ，您需要添加 `@NonBlocking` 注解："

#: _versions/3.27/guides/kafka.adoc
msgid ""
"The `@NonBlocking` annotation is only required with SmallRye Fault Tolerance 5.1.0 and earlier.\n"
"Starting with SmallRye Fault Tolerance 5.2.0 (available since Quarkus 2.1.0.Final), it is not necessary.\n"
"See https://smallrye.io/docs/smallrye-fault-tolerance/5.2.0/usage/extra.html#_non_compatible_mode[SmallRye Fault Tolerance documentation] for more information."
msgstr "`@NonBlocking` 注解仅在SmallRye Fault Tolerance 5.1.0及之前版本中需要。从SmallRye Fault Tolerance 5.2.0开始(Quarkus 2.1.0.Final开始)，它就不再必须了。更多信息请参见link:https://smallrye.io/docs/smallrye-fault-tolerance/5.2.0/usage/extra.html#_non_compatible_mode[SmallRye 容错文档] 。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"The incoming messages are acknowledged only once the processing completes successfully.\n"
"So, it commits the offset after the successful processing.\n"
"If the processing still fails, even after all retries, the message is _nacked_ and the failure strategy is applied."
msgstr "传入的消息只有在处理成功完成后才会被确认。所以，它在处理成功后会提交偏移量。如果在所有的重试后处理仍然失败， 消息就会被标记为 _未确认(nacked)_ ，然后触发失败策略。"

#: _versions/3.27/guides/kafka.adoc
msgid "Handling Deserialization Failures"
msgstr "反序列化失败的处理"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"When a deserialization failure occurs, you can intercept it and provide a failure strategy.\n"
"To achieve this, you need to create a bean implementing `DeserializationFailureHandler<T>` interface:"
msgstr "当反序列化失败发生时，您可以对其进行拦截并提供一个失败处理策略。为了实现这一点，您需要创建一个实现 `DeserializationFailureHandler<T>` 接口的bean："

#: _versions/3.27/guides/kafka.adoc
msgid "To use this failure handler, the bean must be exposed with the `@Identifier` qualifier and the connector configuration must specify the attribute `mp.messaging.incoming.$channel.[key|value]-deserialization-failure-handler` (for key or value deserializers)."
msgstr "要使用这个故障处理的handler，Bean必须使用 `@Identifier` 限定符来暴露，并且connector配置必须指定属性 `mp.messaging.incoming.$channel.[key|value]-deserialization-failure-handler` (对于键或值的反序列化器)。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"The handler is called with details of the deserialization, including the action represented as `Uni<T>`.\n"
"On the deserialization `Uni` failure strategies like retry, providing a fallback value or applying timeout can be implemented."
msgstr "这个handler在被调用提供反序列化的细节，包括以 `Uni<T>` 所表示的操作(action)。在 `Uni` 提供的反序列化错误处理策略中，可以实现例如重试，提供回调(fallback)值或超时处理等等方式。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"If you don’t configure a deserialization failure handler and a deserialization failure happens, the application is marked unhealthy.\n"
"You can also ignore the failure, which will log the exception and produce a `null` value.\n"
"To enable this behavior, set the `mp.messaging.incoming.$channel.fail-on-deserialization-failure` attribute to `false`."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "If the `fail-on-deserialization-failure` attribute is set to `false` and the `failure-strategy` attribute is `dead-letter-queue` the failed record will be sent to the corresponding *dead letter queue* topic."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Consumer Groups"
msgstr "消费者组(Consumer Groups)"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"In Kafka, a consumer group is a set of consumers which cooperate to consume data from a topic.\n"
"A topic is divided into a set of partitions.\n"
"The partitions of a topic are assigned among the consumers in the group, effectively allowing to scale consumption throughput.\n"
"Note that each partition is assigned to a single consumer from a group.\n"
"However, a consumer can be assigned multiple partitions if the number of partitions is greater than the number of consumer in the group."
msgstr "在Kafka中，消费者组表示可以通过合作来消费来自于同一个topic的数据的一组消费者。 一个topic可以包含一组分区(partitions)。一个topic的分区会在组内的消费者之间分配，从而有效地提高消费的吞吐量。请注意，每个分区只会被分配给组内的一个消费者。但如果分区的数量大于组中消费者的数量， 那么一个消费者可以被分配多个分区。"

#: _versions/3.27/guides/kafka.adoc
msgid "Let's explore briefly different producer/consumer patterns and how to implement them using Quarkus:"
msgstr "让我们简单展示一下不同的生产者/消费者模式以及如何使用Quarkus来实现它们："

#: _versions/3.27/guides/kafka.adoc
msgid "*Single consumer thread inside a consumer group*"
msgstr "*消费者组内使用单一消费者线程*\n"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"This is the default behavior of an application subscribing to a Kafka topic: Each Kafka connector will create a single consumer thread and place it inside a single consumer group.\n"
"Consumer group id defaults to the application name as set by the `quarkus.application.name` configuration property.\n"
"It can also be set using the `kafka.group.id` property."
msgstr "这是一个应用程序订阅Kafka topic的默认方式：每个Kafka Connector将创建一个独立的消费者线程，并将其置于一个单独的消费者组内。消费者组id默认为 `quarkus.application.name` 所设定的应用程序名称。它也可以使用 `kafka.group.id` 来设置。"

#: _versions/3.27/guides/kafka.adoc
msgid "*Multiple consumer threads inside a consumer group*"
msgstr "*一个消费者组内使用多个消费者线程* \n"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"For a given application instance, the number of consumers inside the consumer group can be configured using `mp.messaging.incoming.$channel.concurrency` property.\n"
"The partitions of the subscribed topic will be divided among the consumer threads.\n"
"Note that if the `concurrency` value exceed the number of partitions of the topic, some consumer threads won't be assigned any partitions."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Deprecation"
msgstr "弃用"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"The https://smallrye.io/smallrye-reactive-messaging/latest/concepts/incoming-concurrency/[concurrency attribute]\n"
"provides a connector agnostic way for non-blocking concurrent channels and replaces the Kafka connector specific `partitions` attribute.\n"
"The `partitions` attribute is therefore deprecated and will be removed in future releases."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "*Multiple consumer applications inside a consumer group*"
msgstr "*一个消费者组内有多个消费者应用程序* \n"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Similar to the previous example, multiple instances of an application can subscribe to a single consumer group, configured via `mp.messaging.incoming.$channel.group.id` property, or left default to the application name.\n"
"This in turn will divide partitions of the topic among application instances."
msgstr "与前面的例子类似，一个应用程序的多个实例可以订阅同一个消费者组。这种方式可以通过 `mp.messaging.incoming.$channel.group.id` 进行配置，或默认为应用程序的名称。这种方式会在应用程序实例之间分配topic的分区。"

#: _versions/3.27/guides/kafka.adoc
msgid "*Pub/Sub: Multiple consumer groups subscribed to a topic*"
msgstr "*发布/订阅：多个消费者群体订阅同一个topic* \n"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Lastly different applications can subscribe independently to same topics using different *consumer group ids*.\n"
"For example, messages published to a topic called _orders_ can be consumed independently on two consumer applications, one with `mp.messaging.incoming.orders.group.id=invoicing` and second with `mp.messaging.incoming.orders.group.id=shipping`.\n"
"Different consumer groups can thus scale independently according to the message consumption requirements."
msgstr "最后，不同的应用程序可以使用不同的 *consumer group ids* 来的独立订阅同一topic。例如，发布在一个名为 _orders_ 的topic上的消息可以被两个消费者应用相互独立的消费，其中一个的group id是 `mp.messaging.incoming.orders.group.id=invoicing` ，另一个是 `mp.messaging.incoming.orders.group.id=shipping` 。因此，不同的消费者组可以根据消息消费的需求独立的进行扩容。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"A common business requirement is to consume and process Kafka records in order.\n"
"The Kafka broker preserves order of records inside a partition and not inside a topic.\n"
"Therefore, it is important to think about how records are partitioned inside a topic.\n"
"The default partitioner uses record key hash to compute the partition for a record, or when the key is not defined, chooses a partition randomly per batch or records."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"During normal operation, a Kafka consumer preserves the order of records inside each partition assigned to it.\n"
"SmallRye Reactive Messaging keeps this order for processing, unless `@Blocking(ordered = false)` is used (see <<blocking-processing>>)."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Note that due to consumer rebalances, Kafka consumers only guarantee at-least-once processing of single records, meaning that uncommitted records _can_ be processed again by consumers."
msgstr "请注意，由于消费者之间的再平衡(rebalances)，Kafka消费者只保证对单一records的至少一次(at-least-once)处理，这意味着未提交的records _可以_ 被消费者再次处理。"

#: _versions/3.27/guides/kafka.adoc
msgid "Consumer Rebalance Listener"
msgstr "消费者再平衡监听器(Consumer Rebalance Listener)"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Inside a consumer group, as new group members arrive and old members leave, the partitions are re-assigned so that each member receives a proportional share of the partitions.\n"
"This is known as rebalancing the group.\n"
"To handle offset commit and assigned partitions yourself, you can provide a consumer rebalance listener.\n"
"To achieve this, implement the `io.smallrye.reactive.messaging.kafka.KafkaConsumerRebalanceListener` interface and expose it as a CDI bean with the `@Idenfier` qualifier.\n"
"A common use case is to store offset in a separate data store to implement exactly-once semantic, or starting the processing at a specific offset."
msgstr "在一个消费者组内，随着新老组员的交替，分区将会被重新分配，从而使每个组员都能分配到分区。这就是组的再平衡。为了处理偏移提交以及分区的分配，您可以提供一个消费者再平衡监听器。为了实现这一点，请实现 `io.smallrye.reactive.messaging.kafka.KafkaConsumerRebalanceListener` 接口，并将其暴露为CDI bean并使用 `@Idenfier` 修饰符修饰。一个常见的用例是将偏移量存储在一个单独的数据存储中以使其保证语义上的精准一次(exactly-once semantic)，或者在某一个特定的偏移量开始时处理。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"The listener is invoked every time the consumer topic/partition assignment changes.\n"
"For example, when the application starts, it invokes the `partitionsAssigned` callback with the initial set of topics/partitions associated with the consumer.\n"
"If, later, this set changes, it calls the `partitionsRevoked` and `partitionsAssigned` callbacks again, so you can implement custom logic."
msgstr "监听器会在消费者的topic/分区分配发生变化时启动。例如，当应用程序启动时，它会调用 `partitionsAssigned`  回调并传入与消费者相关的初始topic/分区集合 。如果后来这个集合发生变化，它会再次调用 `partitionsRevoked` 和 `partitionsAssigned` 回调，所以您可以自行实现对应的逻辑。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Note that the rebalance listener methods are called from the Kafka polling thread and **will** block the caller thread until completion.\n"
"That’s because the rebalance protocol has synchronization barriers, and using asynchronous code in a rebalance listener may be executed after the synchronization barrier."
msgstr "请注意，再平衡(rebalance)监听器方法是在Kafka轮询线程中被调用的，并且 *会* 阻塞调用者线程直到完成。这是因为再平衡协议(rebalance protocol)有同步屏障，而在再平衡监听器中的异步代码可能会在同步屏障之后执行。"

#: _versions/3.27/guides/kafka.adoc
msgid "When topics/partitions are assigned or revoked from a consumer, it pauses the message delivery and resumes once the rebalance completes."
msgstr "当topic/分区被从消费者那里分配或撤销时，它会暂停消息传递， 然后在重平衡完成后立即恢复。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"If the rebalance listener handles offset commit on behalf of the user (using the `NONE` commit strategy),\n"
"the rebalance listener must commit the offset synchronously in the partitionsRevoked callback.\n"
"We also recommend applying the same logic when the application stops."
msgstr "如果使用再平衡监听器代替用户来处理偏移量提交(使用 `NONE` 提交策略)，再平衡监听器就必须在partitionRevoked回调中同步提交偏移量。我们也建议在应用程序停止时使用同样的逻辑。"

#: _versions/3.27/guides/kafka.adoc
msgid "Unlike the `ConsumerRebalanceListener` from Apache Kafka, the `io.smallrye.reactive.messaging.kafka.KafkaConsumerRebalanceListener` methods pass the Kafka Consumer and the set of topics/partitions."
msgstr "与Apache Kafka的 `ConsumerRebalanceListener` 不同， `io.smallrye.reactive.messaging.kafka.KafkaConsumerRebalanceListener` 的方法会传递Kafka消费者和topic/分区集合。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"In the following example we set up a consumer that always starts on messages from at most 10 minutes ago (or offset 0).\n"
"First we need to provide a bean that implements `io.smallrye.reactive.messaging.kafka.KafkaConsumerRebalanceListener` and is annotated with `io.smallrye.common.annotation.Identifier`.\n"
"We then must configure our inbound connector to use this bean."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"To configure the inbound connector to use the provided listener, we either set the consumer rebalance listener’s identifier:\n"
"`mp.messaging.incoming.rebalanced-example.consumer-rebalance-listener.name=rebalanced-example.rebalancer`"
msgstr "如要配置inbound connector使用所提供的监听器，我们可以通过消费者再平衡监听器的标识符 `mp.messaging.incoming.rebalanced-example.consumer-rebalance-listener.name=rebalanced-example.rebalancer` 来设置"

#: _versions/3.27/guides/kafka.adoc
msgid "Or have the listener’s name be the same as the group id:"
msgstr "或者令监听器的名字与消费者组的ID相同："

#: _versions/3.27/guides/kafka.adoc
msgid "`mp.messaging.incoming.rebalanced-example.group.id=rebalanced-example.rebalancer`"
msgstr "`mp.messaging.incoming.rebalanced-example.group.id=rebalanced-example.rebalancer`"

#: _versions/3.27/guides/kafka.adoc
msgid "Setting the consumer rebalance listener’s name takes precedence over using the group id."
msgstr "注意，设置消费者再平衡监听器的名称的方式要比使用组ID的方式优先被使用。"

#: _versions/3.27/guides/kafka.adoc
msgid "Using unique consumer groups"
msgstr "使用单一的消费者组"

#: _versions/3.27/guides/kafka.adoc
msgid "If you want to process all the records from a topic (from its beginning), you need:"
msgstr "如果您想处理一个topic中的所有记录(从其最开始时)，您需要："

#: _versions/3.27/guides/kafka.adoc
msgid "to set `auto.offset.reset = earliest`"
msgstr "设置 `auto.offset.reset = earliest`"

#: _versions/3.27/guides/kafka.adoc
msgid "assign your consumer to a consumer group not used by any other application."
msgstr "将您的消费者分配到一个不被任何其他程序使用的消费者组。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Quarkus generates a UUID that changes between two executions (including in dev mode).\n"
"So, you are sure no other consumer uses it, and you receive a new unique group id every time your application starts."
msgstr "Quarkus生成的UUID在两次执行之间会发生变化(包括在dev模式下)。因此，您可以确定没有其他消费者使用它，而且每次您的应用程序启动时都会收到一个新的唯一的组ID。"

#: _versions/3.27/guides/kafka.adoc
msgid "You can use that generated UUID as the consumer group as follows:"
msgstr "您可以使用生成的UUID作为消费者组id，如下所示："

#: _versions/3.27/guides/kafka.adoc
msgid "If the `group.id` attribute is not set, it defaults the `quarkus.application.name` configuration property."
msgstr "如果没有设置 `group.id` ，则其默认与 `quarkus.application.name` 相同。"

#: _versions/3.27/guides/kafka.adoc
msgid "Manual topic-partition assignment"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"The `assign-seek` channel attribute allows manually assigning topic-partitions to a Kafka incoming channel,\n"
"and optionally seek to a specified offset in the partition to start consuming records.\n"
"If `assign-seek` is used, the consumer will not be dynamically subscribed to topics,\n"
"but instead will statically assign the described partitions.\n"
"In manual topic-partition rebalancing doesn't happen and therefore rebalance listeners are never called."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "The attribute takes a list of triplets separated by commas: `<topic>:<partition>:<offset>`."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "For example, the configuration"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "assigns the consumer to:"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Partition 0 of topic 'topic1', setting the initial position at offset 10."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Partition 1 of topic 'topic2', setting the initial position at offset 20."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "The topic, partition, and offset in each triplet can have the following variations:"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "If the topic is omitted, the configured topic will be used."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "If the offset is omitted, partitions are assigned to the consumer but won't be sought to offset."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "If offset is 0, it seeks to the beginning of the topic-partition."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "If offset is -1, it seeks to the end of the topic-partition."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Receiving Kafka Records in Batches"
msgstr "批量接收Kafka记录"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"By default, incoming methods receive each Kafka record individually.\n"
"Under the hood, Kafka consumer clients poll the broker constantly and receive records in batches, presented inside the `ConsumerRecords` container."
msgstr "默认情况下，接收方法会单独接收每条Kafka记录。在后台，Kafka消费者client会不断地轮询broker，并批量接收记录然后放入 `ConsumerRecords` 容器中。"

#: _versions/3.27/guides/kafka.adoc
msgid "In *batch* mode, your application can receive all the records returned by the consumer *poll* in one go."
msgstr "在 *批量* 模式下，您的程序可以一次性接收消费者 *轮询* 返回的所有记录。"

#: _versions/3.27/guides/kafka.adoc
msgid "To achieve this you need to specify a compatible container type to receive all the data:"
msgstr "为了达到这一点，您需要指定一个兼容的容器类型来接收所有数据："

#: _versions/3.27/guides/kafka.adoc
msgid ""
"The incoming method can also receive `Message<List<Payload>>`, `Message<ConsumerRecords<Key, Payload>>`, and `ConsumerRecords<Key, Payload>` types.\n"
"They give access to record details such as offset or timestamp:"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Note that the successful processing of the incoming record batch will commit the latest offsets for each partition received inside the batch.\n"
"The configured commit strategy will be applied for these records only."
msgstr "注意，对于接收到的记录批次的成功处理会提交所收到批次内每个分区的最新偏移量。所配置的提交策略将只应用于这些记录。"

#: _versions/3.27/guides/kafka.adoc
msgid "Conversely, if the processing throws an exception, all messages are _nacked_, applying the failure strategy for all the records inside the batch."
msgstr "反之，如果处理过程抛出一个异常，所有的消息都_不会被确认(nacked)_ ，并且对批次中的所有记录应用失败策略。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Quarkus autodetects batch types for incoming channels and sets batch configuration automatically.\n"
"You can configure batch mode explicitly with `mp.messaging.incoming.$channel.batch` property."
msgstr "Quarkus自动检测incoming channels 的批处理类型并自动设置批处理配置。您可以用 `mp.messaging.incoming.$channel.batch` 配置批处理模式。"

#: _versions/3.27/guides/kafka.adoc
msgid "Stateful processing with Checkpointing"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "The `checkpoint` commit strategy is an experimental feature and can change in the future."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"SmallRye Reactive Messaging `checkpoint` commit strategy allows consumer applications to process messages in a stateful manner, while also respecting Kafka consumer scalability.\n"
"An incoming channel with `checkpoint` commit strategy persists consumer offsets on an external\n"
"<<state-stores,state store>>, such as a relational database or a key-value store.\n"
"As a result of processing consumed records, the consumer application can accumulate an internal state for each topic-partition assigned to the Kafka consumer.\n"
"This local state will be periodically persisted to the state store and will be associated with the offset of the record that produced it."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"This strategy does not commit any offsets to the Kafka broker, so when new partitions get assigned to the consumer,\n"
"i.e. consumer restarts or consumer group instances scale,\n"
"the consumer resumes the processing from the latest _checkpointed_ offset with its saved state."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"The `@Incoming` channel consumer code can manipulate the processing state through the `CheckpointMetadata` API.\n"
"For example, a consumer calculating the moving average of prices received on a Kafka topic would look the following:"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"The `transform` method applies the transformation function to the current state, producing a changed state and registering it locally for checkpointing.\n"
"By default, the local state is persisted to the state store periodically, period specified by `auto.commit.interval.ms`, (default: 5000).\n"
"If `persistOnAck` flag is given, the latest state is persisted to the state store eagerly on message acknowledgment.\n"
"The `setNext` method works similarly directly setting the latest state."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"The checkpoint commit strategy tracks when a processing state is last persisted for each topic-partition.\n"
"If an outstanding state change can not be persisted for `checkpoint.unsynced-state-max-age.ms` (default: 10000), the channel is marked unhealthy."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "State stores"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"State store implementations determine where and how the processing states are persisted.\n"
"This is configured by the `mp.messaging.incoming.[channel-name].checkpoint.state-store` property.\n"
"The serialization of state objects depends on the state store implementation.\n"
"In order to instruct state stores for serialization can require configuring the class name of state objects using `mp.messaging.incoming.[channel-name].checkpoint.state-type` property."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Quarkus provides following state store implementations:"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"`quarkus-redis`: Uses the xref:redis-reference.adoc[`quarkus-redis-client`] extension to persist processing states.\n"
"Jackson is used to serialize processing state in Json. For complex objects it is required to configure the `checkpoint.state-type` property with the class name of the object.\n"
"By default, the state store uses the default redis client, but if a xref:redis-reference.adoc#default-and-named-clients[named client] is to be used, the client name can be specified using the `mp.messaging.incoming.[channel-name].checkpoint.quarkus-redis.client-name` property.\n"
"Processing states will be stored in Redis using the key naming scheme `[consumer-group-id]:[topic]:[partition]`."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "For example the configuration of the previous code would be the following:"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"`quarkus-hibernate-reactive`: Uses the xref:hibernate-reactive.adoc[`quarkus-hibernate-reactive`] extension to persist processing states.\n"
"Processing state objects are required to be a Jakarta Persistence entity and extend the `CheckpointEntity` class,\n"
"which handles object identifiers composed of the consumer group id, topic and partition.\n"
"Therefore, the class name of the entity needs to be configured using the `checkpoint.state-type` property."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "With `AveragePriceEntity` being a Jakarta Persistence entity extending `CheckpointEntity`:"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"`quarkus-hibernate-orm`: Uses the xref:hibernate-orm.adoc[`quarkus-hibernate-orm`] extension to persist processing states.\n"
"It is similar to the previous state store, but it uses Hibernate ORM instead of Hibernate Reactive."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "When configured, it can use a named `persistence-unit` for the checkpointing state store:"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"For instructions on how to implement custom state stores,\n"
"see https://smallrye.io/smallrye-reactive-messaging/3.22.0/kafka/receiving-kafka-records/#implementing-state-stores[Implementing State Stores]."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Sending messages to Kafka"
msgstr "向Kafka发送消息"

#: _versions/3.27/guides/kafka.adoc
msgid "Configuration for the Kafka connector outgoing channels is similar to that of incoming:"
msgstr "Kafka Connector中用于发送的 channels 的配置与用于接收的 channel 的配置类似："

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Configure the broker location for the production profile. You can configure it globally or per channel using `mp.messaging.outgoing.$channel.bootstrap.servers` property.\n"
"In dev mode and when running tests, <<kafka-dev-services>> automatically starts a Kafka broker.\n"
"When not provided, this property defaults to `localhost:9092`."
msgstr "请配置生产环境的broker位置。您可以在全局配置，或使用 `mp.messaging.outgoing.$channel.bootstrap.servers` 来针对特定channel配置它。在开发模式和运行测试时， <<kafka-dev-services>>会自动启动一个Kafka broker。如果该属性未提供，它将默认为 `localhost:9092` 。"

#: _versions/3.27/guides/kafka.adoc
msgid "Configure the connector to manage the `prices-out` channel."
msgstr "配置connector来管理 `prices-out` channel。"

#: _versions/3.27/guides/kafka.adoc
msgid "By default, the topic name is same as the channel name. You can configure the topic attribute to override it."
msgstr "默认情况下，topic名称与channel名称相同。您可以配置topic属性来覆盖它。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Inside application configuration, channel names are unique.\n"
"Therefore, if you'd like to configure an incoming and outgoing channel on the same topic, you will need to name channels differently (like in the examples of this guide, `mp.messaging.incoming.prices` and `mp.messaging.outgoing.prices-out`)."
msgstr "在应用配置里面，channel名称是唯一的。因此，如果您打算在相同topic上同时配置一个接收和一个发送的channel，您需要对这两个 channels 使用不同的名称(比如本指南的例子，`mp.messaging.incoming.prices` 和 `mp.messaging.outgoing.prices-out` )。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Then, your application can generate messages and publish them to the `prices-out` channel.\n"
"It can use `double` payloads as in the following snippet:"
msgstr "然后，您的程序可以生成消息并将其发布到 `prices-out` channel。它可以使用 `double` payloads，如下所示："

#: _versions/3.27/guides/kafka.adoc
msgid "You should not call methods annotated with `@Incoming` and/or `@Outgoing` directly from your code. They are invoked by the framework. Having user code invoking them would not have the expected outcome."
msgstr "您不应该在您的代码中直接调用被标记了 `@Incoming` 和/或 `@Outgoing` 注解的方法。它们会被框架调用。如果在用户代码中调用将不会得到预期的结果。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Note that the `generate` method returns a `Multi<Double>`, which implements the Reactive Streams `Publisher` interface.\n"
"This publisher will be used by the framework to generate messages and send them to the configured Kafka topic."
msgstr "请注意， `generate` 方法返回了一个 `Multi<Double>` ，它实现了Reactive Streams `Publisher` 接口。Quarkus框架会使用这个发布者生成消息，并将其发送到您配置的Kafka topic中。"

#: _versions/3.27/guides/kafka.adoc
msgid "Instead of returning a payload, you can return a `io.smallrye.reactive.messaging.kafka.Record` to send key/value pairs:"
msgstr "为了发送键/值对， 您可以直接返回一个 `io.smallrye.reactive.messaging.kafka.Record` 来代理一个payload："

#: _versions/3.27/guides/kafka.adoc
msgid "Payload can be wrapped inside `org.eclipse.microprofile.reactive.messaging.Message` to have more control on the written records:"
msgstr "Payload可以被封装在 `org.eclipse.microprofile.reactive.messaging.Message`，以便对写入的记录有更多的控制："

#: _versions/3.27/guides/kafka.adoc
msgid ""
"`OutgoingKafkaRecordMetadata` allows to set metadata attributes of the Kafka record, such as `key`, `topic`, `partition` or `timestamp`.\n"
"One use case is to dynamically select the destination topic of a message.\n"
"In this case, instead of configuring the topic inside your application configuration file, you need to use the outgoing metadata to set the name of the topic."
msgstr "`OutgoingKafkaRecordMetadata` 允许您设置Kafka记录的元数据属性，如 `key` ， `topic` ， `partition` 或 `timestamp` 。一种场景是动态地选择消息的目标topic。在这种情况下，您需要使用出站元数据(outgoing metadata)来设置topic名称，而不是在配置文件中配置topic。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Other than method signatures returning a Reactive Stream `Publisher` (`Multi` being an implementation of `Publisher`), outgoing method can also return single message.\n"
"In this case the producer will use this method as generator to create an infinite stream."
msgstr "除了返回Reactive Stream `Publisher` ( `Multi` 实现了 `Publisher` )的方法签名外，发送方法也可以返回单个消息。在这种情况下，生产者将使用该方法作为生成器来创建一个无限的流(infinite stream)。"

#: _versions/3.27/guides/kafka.adoc
msgid "Sending messages with Emitter"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Sometimes, you need to have an imperative way of sending messages."
msgstr "有时，您需要使用命令式的方式来发送消息。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"For example, if you need to send a message to a stream when receiving a POST request inside a REST endpoint.\n"
"In this case, you cannot use `@Outgoing` because your method has parameters."
msgstr "例如，如果您需要在REST节点内收到一个POST请求时向一个流发送消息。在这种情况下，您无法使用 `@Outgoing` ，因为您的方法有参数。"

#: _versions/3.27/guides/kafka.adoc
msgid "For this, you can use an `Emitter`."
msgstr "这种情况下您可以使用 `Emitter` 。"

#: _versions/3.27/guides/kafka.adoc
msgid "Sending a payload returns a `CompletionStage`, completed when the message is acked. If the message transmission fails, the `CompletionStage` is completed exceptionally with the reason of the nack."
msgstr "发送一个payload会返回一个 `CompletionStage` ，并且它会在消息被确认时完成。如果消息传输失败， `CompletionStage` 会以异常结束，并且包含未被确认的原因。"

#: _versions/3.27/guides/kafka.adoc
msgid "The `Emitter` configuration is done the same way as the other stream configuration used by `@Incoming` and `@Outgoing`."
msgstr "`Emitter` 的配置方式与 其他被 `@Incoming` 和 `@Outgoing` 使用的流配置相同。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Using the `Emitter` you are sending messages from your imperative code to reactive messaging.\n"
"These messages are stored in a queue until they are sent.\n"
"If the Kafka producer client can't keep up with messages trying to be sent over to Kafka, this queue can become a memory hog and you may even run out of memory.\n"
"You can use `@OnOverflow` to configure back-pressure strategy.\n"
"It lets you configure the size of the queue (default is 256) and the strategy to apply when the buffer size is reached. Available strategies are `DROP`, `LATEST`, `FAIL`, `BUFFER`, `UNBOUNDED_BUFFER` and `NONE`."
msgstr "使用 `Emitter` 的话您会以命令式代码的方式发送消息到响应式消息中(reactive messaging)。这些消息被存储在一个队列中，直到它们被发送。如果Kafka生产者client无法跟上发送到Kafka的消息节奏，这个队列就会成为一个内存占用者，甚至会耗尽内存。您可以使用 `@OnOverflow` 来配置背压策略。它可以让您配置队列的大小(默认是256)和达到缓冲区上限时要应用的策略。可用的策略有 `DROP` , `LATEST` , `FAIL` , `BUFFER` , `UNBOUNDED_BUFFER` 和 `NONE` 。"

#: _versions/3.27/guides/kafka.adoc
msgid "With the `Emitter` API, you can also encapsulate the outgoing payload inside `Message<T>`. As with the previous examples, `Message` lets you handle the ack/nack cases differently."
msgstr "通过 `Emitter` API，您也可以将要发送的payload封装在 `Message<T>` 中 。与前面的例子一样， `Message` 让您以不同的方式处理确认/拒绝(ack/nack)的情况。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"If you prefer using Reactive Stream APIs, you can use `MutinyEmitter` that will return `Uni<Void>` from the `send` method.\n"
"You can therefore use Mutiny APIs for handling downstream messages and errors."
msgstr "如果您偏好于使用Reactive Stream APIs，您可以使用 `MutinyEmitter` ，它将在 `send` 方法中返回 `Uni<Void>` 。因此，您可以使用Mutiny APIs来处理下游的信息和错误。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"It is also possible to block on sending the event to the emitter with the `sendAndAwait` method.\n"
"It will only return from the method when the event is acked or nacked by the receiver."
msgstr "也可以通过 `sendAndAwait` 方法在发送事件到emitter的时候进行阻塞。只有当事件被接收者确认或拒绝时，它才会从该方法中返回。"

#: _versions/3.27/guides/kafka.adoc
msgid "The `io.smallrye.reactive.messaging.annotations.Emitter`, `io.smallrye.reactive.messaging.annotations.Channel` and `io.smallrye.reactive.messaging.annotations.OnOverflow` classes are now deprecated and replaced by:"
msgstr "`io.smallrye.reactive.messaging.annotations.Emitter` , `io.smallrye.reactive.messaging.annotations.Channel` 和 `io.smallrye.reactive.messaging.annotations.OnOverflow` 类现在已被废弃，并被替换为："

#: _versions/3.27/guides/kafka.adoc
msgid "`org.eclipse.microprofile.reactive.messaging.Emitter`"
msgstr "`org.eclipse.microprofile.reactive.messaging.Emitter`"

#: _versions/3.27/guides/kafka.adoc
msgid "`org.eclipse.microprofile.reactive.messaging.Channel`"
msgstr "`org.eclipse.microprofile.reactive.messaging.Channel`"

#: _versions/3.27/guides/kafka.adoc
msgid "`org.eclipse.microprofile.reactive.messaging.OnOverflow`"
msgstr "`org.eclipse.microprofile.reactive.messaging.OnOverflow`"

#: _versions/3.27/guides/kafka.adoc
msgid "The new `Emitter.send` method returns a `CompletionStage` completed when the produced message is acknowledged."
msgstr "新的 `Emitter.send` 方法会返回一个 `CompletionStage` ，并且它会在产生的消息被确认时完成。"

#: _versions/3.27/guides/kafka.adoc
msgid "`MutinyEmitter#send(Message msg)` method is deprecated in favor of following methods receiving `Message` for emitting:"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "`<M extends Message<? extends T>> Uni<Void> sendMessage(M msg)`"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "`<M extends Message<? extends T>> void sendMessageAndAwait(M msg)`"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "`<M extends Message<? extends T>> Cancellable sendMessageAndForget(M msg)`"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "More information on how to use `Emitter` can be found in https://smallrye.io/smallrye-reactive-messaging/latest/concepts/emitter/[SmallRye Reactive Messaging – Emitters and Channels]"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Write Acknowledgement"
msgstr "写确认"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"When Kafka broker receives a record, its acknowledgement can take time depending on the configuration.\n"
"Also, it stores in-memory the records that cannot be written."
msgstr "当Kafka broker收到一条记录时，它的确认可能需要时间，这取决于配置。此外，它还会在内存中存储不能写入的记录。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"By default, the connector does wait for Kafka to acknowledge the record to continue the processing (acknowledging the received Message).\n"
"You can disable this by setting the `waitForWriteCompletion` attribute to `false`."
msgstr "默认情况下，connector会等待Kafka确认记录以继续处理(确认收到的消息)。您可以通过将 `waitForWriteCompletion` 设置为 `false` 来禁用这个功能。"

#: _versions/3.27/guides/kafka.adoc
msgid "Note that the `acks` attribute has a huge impact on the record acknowledgement."
msgstr "请注意， `acks` 属性对记录的确认有巨大影响。"

#: _versions/3.27/guides/kafka.adoc
msgid "If a record cannot be written, the message is nacked."
msgstr "如果一条记录无法写入，消息就会被拒绝。"

#: _versions/3.27/guides/kafka.adoc
msgid "Backpressure"
msgstr "背压"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"The Kafka outbound connector handles back-pressure, monitoring the number of in-flight messages waiting to be written to the Kafka broker.\n"
"The number of in-flight messages is configured using the `max-inflight-messages` attribute and defaults to 1024."
msgstr "Kafka的出站connector负责处理背压，并且会监测等待写入Kafka broker中的in-flight的消息数量。in-flight的消息的数量是通过 `max-inflight-messages` 配置的，默认为1024。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"The connector only sends that amount of messages concurrently.\n"
"No other messages will be sent until at least one in-flight message gets acknowledged by the broker.\n"
"Then, the connector writes a new message to Kafka when one of the broker’s in-flight messages get acknowledged.\n"
"Be sure to configure Kafka’s `batch.size` and `linger.ms` accordingly."
msgstr "Connector只会并行发送指定数量的消息。在至少一个in-flight的消息被broker确认之前，其他消息都不会被发送。然后，当broker中有in-flight的消息得到确认时，connector才会向Kafka写入一个新的消息。请确保相应地配置Kafka的 `batch.size` 和 `linger.ms` 属性。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"You can also remove the limit of in-flight messages by setting `max-inflight-messages` to `0`.\n"
"However, note that the Kafka producer may block if the number of requests reaches `max.in.flight.requests.per.connection`."
msgstr "您也可以通过将 `max-inflight-messages` 设置为 `0` 来取消in-flight消息的限制。但请注意，如果请求数量达到 `max.in.flight.requests.per.connection` 指定的值，Kafka生产者可能会阻塞。"

#: _versions/3.27/guides/kafka.adoc
msgid "Retrying message dispatch"
msgstr "重试消息的发送"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"When the Kafka producer receives an error from the server, if it is a transient, recoverable error, the client will retry sending the batch of messages.\n"
"This behavior is controlled by `retries` and `retry.backoff.ms` parameters.\n"
"In addition to this, SmallRye Reactive Messaging will retry individual messages on recoverable errors, depending on the `retries` and `delivery.timeout.ms` parameters."
msgstr "当Kafka生产者收到来自服务器的错误时，如果它是一个暂时的、可恢复的错误，那么客户端将重试发送这批消息。这种行为是由 `retries` 和 `retry.backoff.ms` 参数控制的。除此之外，SmallRye Reactive Messaging还会在可恢复的错误中重试单个消息，这取决于 `retries` 和 `delivery.timeout.ms` 参数。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Note that while having retries in a reliable system is a best practice, the `max.in.flight.requests.per.connection` parameter defaults to `5`, meaning that the order of the messages is not guaranteed.\n"
"If the message order is a must for your use case, setting `max.in.flight.requests.per.connection` to `1` will make sure a single batch of messages is sent at a time, in the expense of limiting the throughput of the producer."
msgstr "请注意，虽然在对一个可靠的系统来说拥有重试机制是一种最佳实践，但 `max.in.flight.requests.per.connection` 参数默认为 `5` 将会意味着消息的顺序不会被保证。如果消息的顺序对您来说是必须保证的，将 `max.in.flight.requests.per.connection` 设置为 `1` 将确保一次只发送一批消息，但代价是限制生产者的吞吐量。"

#: _versions/3.27/guides/kafka.adoc
msgid "For applying retry mechanism on processing errors, see the section on <<retrying-processing>>."
msgstr "关于如何对错误处理应用重试机制，请参见 link:#retrying-processing[[重试-处理]] 一节。"

#: _versions/3.27/guides/kafka.adoc
msgid "Handling Serialization Failures"
msgstr "处理序列化失败"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"For Kafka producer client serialization failures are not recoverable, thus the message dispatch is not retried. In these cases you may need to apply a failure strategy for the serializer.\n"
"To achieve this, you need to create a bean implementing `SerializationFailureHandler<T>` interface:"
msgstr "对于Kafka生产者客户端来说序列化失败是不可恢复的，因此消息发送不会被重试。在这些情况下，您可能需要为序列化器设置一个失败策略。为了实现这一点，您需要一个实现了 `SerializationFailureHandler<T>` 接口的bean："

#: _versions/3.27/guides/kafka.adoc
msgid "To use this failure handler, the bean must be exposed with the `@Identifier` qualifier and the connector configuration must specify the attribute `mp.messaging.outgoing.$channel.[key|value]-serialization-failure-handler` (for key or value serializers)."
msgstr "要使用该故障处理，Bean必须用 `@Identifier` 限定符修饰，并且connector的配置必须指定属性 `mp.messaging.outgoing.$channel.[key|value]-serialization-failure-handler` (对于键或值序列化器)。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"The handler is called with details of the serialization, including the action represented as `Uni<byte[]>`.\n"
"Note that the method must await on the result and return the serialized byte array."
msgstr "处理器被调用，并被提供序列化的细节，包括以 `Uni<byte[]>` 表示的操作。注意，该方法必须对在结果处进行等待，并返回序列化后的字节数组。"

#: _versions/3.27/guides/kafka.adoc
msgid "In-memory channels"
msgstr "内存 channels"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"In some use cases, it is convenient to use the messaging patterns to transfer messages inside the same application.\n"
"When you don't connect a channel to a messaging backend like Kafka, everything happens in-memory, and the streams are created by chaining methods together.\n"
"Each chain is still a reactive stream and enforces the back-pressure protocol."
msgstr "在某些情况下，使用消息模式在同一个应用程序内传输消息是很方便的。当您没有将channel连接到像Kafka这样的消息后端时，一切都会发生在内存中，并且流会通过链式方法创建。每个链式调用仍是一个响应式流，并执行背压策略。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"The framework verifies that the producer/consumer chain is complete,\n"
"meaning that if the application writes messages into an in-memory channel (using a method with only `@Outgoing`, or an `Emitter`),\n"
"it must also consume the messages from within the application (using a method with only `@Incoming` or using an unmanaged stream)."
msgstr "Quarkus框架会验证生产者/消费者链是否完整，这意味着如果应用程序将消息写入内存channel(仅使用 `@Outgoing` 修饰符方法，或 `Emitter` )，它也必须从应用程序内部消费消息(仅 `@Incoming` 修饰符方法 ，或使用不受管理的流)。"

#: _versions/3.27/guides/kafka.adoc
msgid "Broadcasting messages on multiple consumers"
msgstr "对多个消费者广播信息"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"By default, a channel can be linked to a single consumer, using `@Incoming` method or `@Channel` reactive stream.\n"
"At application startup, channels are verified to form a chain of consumers and producers with single consumer and producer.\n"
"You can override this behavior by setting `mp.messaging.$channel.broadcast=true` on a channel."
msgstr "默认情况下，一个channel可以关联到一个单一的消费者上，通过使用 `@Incoming` 方法或 `@Channel` 响应式应式流。在程序启动时，channels 会被验证，以形成一个由单个消费者和生产者组成的消费者和生产者链。您可以通过在channel上设置 `mp.messaging.$channel.broadcast=true` 来覆盖这种行为。"

#: _versions/3.27/guides/kafka.adoc
msgid "In case of in-memory channels, `@Broadcast` annotation can be used on the `@Outgoing` method. For example,"
msgstr "在内存 channels 的情况下， `@Broadcast` 注释可以用在 `@Outgoing` 方法上。比如说,"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Reciprocally, multiple producers on the same channel can be merged by setting `mp.messaging.incoming.$channel.merge=true`.\n"
"On the `@Incoming` methods, you can control how multiple channels are merged using the `@Merge` annotation."
msgstr "相应地，同一 channel 上的多个生产者可以通过设置 `mp.messaging.incoming.$channel.merge=true` 来进行合并。在 `@Incoming` 方法上，您可以使用 `@Merge` 来控制多个 channels 的合并方式。"

#: _versions/3.27/guides/kafka.adoc
msgid "Repeating the `@Outgoing` annotation on outbound or processing methods allows another way of dispatching messages to multiple outgoing channels:"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"In the previous example generated price will be broadcast to both outbound channels.\n"
"The following example selectively sends messages to multiple outgoing channels using the `Targeted` container object,\n"
"containing key as channel name and value as message payload."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Note that <<serialization-autodetection,the auto-detection for Kafka serializers>> doesn't work for signatures using the `Targeted`."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "For more details on using multiple outgoings, please refer to the http://smallrye.io/smallrye-reactive-messaging/4.10.0/concepts/outgoings/[SmallRye Reactive Messaging documentation]."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Kafka Transactions"
msgstr "Kafka事务处理"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Kafka transactions enable atomic writes to multiple Kafka topics and partitions.\n"
"The Kafka connector provides `KafkaTransactions` custom emitter for writing Kafka records inside a transaction.\n"
"It can be injected as a regular emitter `@Channel`:"
msgstr "Kafka 事务支持对多个 Kafka 主题和分区进行原子写入。 Kafka 连接器提供了 `KafkaTransactions` 自定义emitter，用于在事务中写入 Kafka 记录。 它可以作为常规emitter `@Channel` 注入："

#: _versions/3.27/guides/kafka.adoc
msgid "The function given to the `withTransaction` method receives a `TransactionalEmitter` for producing records, and returns a `Uni` that provides the result of the transaction."
msgstr "传入 `withTransaction` 方法的函数参数会使用 `TransactionalEmitter` 来产生记录，并返回 `Uni` 做为事务结果。"

#: _versions/3.27/guides/kafka.adoc
msgid "If the processing completes successfully, the producer is flushed and the transaction is committed."
msgstr "如果处理成功完成，则刷新生产者并提交事务。"

#: _versions/3.27/guides/kafka.adoc
msgid "If the processing throws an exception, returns a failing `Uni`, or marks the `TransactionalEmitter` for abort, the transaction is aborted."
msgstr "如果处理过程抛出异常，会返回失败的 `Uni`，或将 `TransactionalEmitter` 标记为中止，然后事务会被中止。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Kafka transactional producers require configuring `acks=all` client property, and a unique id for `transactional.id`, which implies `enable.idempotence=true`.\n"
"When Quarkus detects the use of `KafkaTransactions` for an outgoing channel it configures these properties on the channel,\n"
"providing a default value of `\"${quarkus.application.name}-${channelName}\"` for `transactional.id` property."
msgstr "Kafka 事务生产者需要配置 `acks=all` 客户端属性，以及 `transactional.id` 的唯一 id，这意味着 `enable.idempotence=true` 。 当 Quarkus 检测到传出通道使用 `KafkaTransactions` 时，它会在通道上配置这些属性，为 `transactional.id` 属性提供默认值 `\"${quarkus.application.name}-${channelName}\"` 。"

#: _versions/3.27/guides/kafka.adoc
msgid "Note that for production use the `transactional.id` must be unique across all application instances."
msgstr "请注意，要在生产环境使用，必须确保 `transactional.id` 在所有应用实例中是唯一的。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"While a normal message emitter would support concurrent calls to `send` methods and consequently queues outgoing messages to be written to Kafka,\n"
"a `KafkaTransactions` emitter only supports one transaction at a time.\n"
"A transaction is considered in progress from the call to the `withTransaction` until the returned `Uni` results in success or failure.\n"
"While a transaction is in progress, subsequent calls to the `withTransaction`, including nested ones inside the given function, will throw `IllegalStateException`."
msgstr "虽然普通的消息emitter支持并发调用 `send` 方法并将要写入 Kafka 的传出消息顺序排队，但 `KafkaTransactions` emitter每次只支持一个事务。 从调用 `withTransaction` 直到返回的 `Uni` 导致成功或失败，事务被视为正在进行中。 当事务正在进行时，对 `withTransaction` 的后续调用，包括给定函数内的嵌套调用，都将抛出 `IllegalStateException` 。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Note that in Reactive Messaging, the execution of processing methods, is already serialized, unless `@Blocking(ordered = false)` is used.\n"
"If `withTransaction` can be called concurrently, for example from a REST endpoint, it is recommended to limit the concurrency of the execution.\n"
"This can be done using the `@Bulkhead` annotation from xref:smallrye-fault-tolerance.adoc[_Microprofile Fault Tolerance_]."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "An example usage can be found in <<chaining-kafka-transactions-with-hibernate-reactive-transactions>>."
msgstr "示例用法可以在 <<chaining-kafka-transactions-with-hibernate-reactive-transactions>> 中找到。"

#: _versions/3.27/guides/kafka.adoc
msgid "Transaction-aware consumers"
msgstr "事务感知型消费者"

#: _versions/3.27/guides/kafka.adoc
msgid "If you'd like to consume records only written and committed inside a Kafka transaction you need to configure the `isolation.level` property on the incoming channel as such:"
msgstr "如果您想使用仅在 Kafka 事务中写入和提交的记录，您需要在传入通道上配置 `isolation.level` 属性，如下所示："

#: _versions/3.27/guides/kafka.adoc
msgid "Kafka Request-Reply"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"The Kafka Request-Reply pattern allows to publish a request record to a Kafka topic and then await for a reply record that responds to the initial request.\n"
"The Kafka connector provides the `KafkaRequestReply` custom emitter that implements the requestor (or the client) of the request-reply pattern for Kafka outbound channels:"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "It can be injected as a regular emitter `@Channel`:"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"The request method publishes the record to the configured target topic of the outgoing channel,\n"
"and polls a reply topic (by default, the target topic with `-replies` suffix) for a reply record.\n"
"When the reply is received the returned `Uni` is completed with the record value.\n"
"The request send operation generates a **correlation id** and sets a header (by default `REPLY_CORRELATION_ID`),\n"
"which it expects to be sent back in the reply record."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "The replier can be implemented using a Reactive Messaging processor (see <<processing-messages>>)."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"For more information on Kafka Request Reply feature and advanced configuration options,\n"
"see the https://smallrye.io/smallrye-reactive-messaging/latest/kafka/request-reply/[SmallRye Reactive Messaging Documentation]."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Processing Messages"
msgstr "处理消息"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Applications streaming data often need to consume some events from a topic, process them and publish the result to a different topic.\n"
"A processor method can be simply implemented using both the `@Incoming` and `@Outgoing` annotations:"
msgstr "应用的流式数据常常需要从一个topic中消费一些事件，对其进行处理并将结果发布到不同的topic中。一个处理器方法可以简单地通过使用 `@Incoming` 和 `@Outgoing` 注解来实现："

#: _versions/3.27/guides/kafka.adoc
msgid ""
"The parameter of the `process` method is the incoming message payload, whereas the return value will be used as the outgoing message payload.\n"
"Previously mentioned signatures for parameter and return types are also supported, such as `Message<T>`, `Record<K, V>`, etc."
msgstr "`process` 方法的参数是传入的消息的payload，而返回值将被用作传出的消息的payload。之前提到的参数和返回类型的签名也被支持，如 `Message<T>` ， `Record<K, V>` 等等。"

#: _versions/3.27/guides/kafka.adoc
msgid "You can apply asynchronous stream processing by consuming and returning reactive stream `Multi<T>` type:"
msgstr "您可以通过消费和返回响应式流 `Multi<T>` 类型来应用异步流处理："

#: _versions/3.27/guides/kafka.adoc
msgid "Propagating Record Key"
msgstr "传播记录键"

#: _versions/3.27/guides/kafka.adoc
msgid "When processing messages, you can propagate incoming record key to the outgoing record."
msgstr "在处理信息时，您可以将传入的记录键发送给传出的记录。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Enabled with `mp.messaging.outgoing.$channel.propagate-record-key=true` configuration,\n"
"record key propagation produces the outgoing record with the same _key_ as the incoming record."
msgstr "通过启用 `mp.messaging.outgoing.$channel.propagate-record-key=true` ，记录键传播可以产生与传入记录的 _键_ 相同的传出记录。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"If the outgoing record already contains a _key_, it *won't be overridden* by the incoming record key.\n"
"If the incoming record does have a _null_ key, the `mp.messaging.outgoing.$channel.key` property is used."
msgstr "如果传出的记录已经包含一个 _键_，那么它 *不会* 被传入的记录键的键所覆盖。如果传入的记录键为_空_，那么 `mp.messaging.outgoing.$channel.key` 设置的值会被使用。"

#: _versions/3.27/guides/kafka.adoc
msgid "Exactly-Once Processing"
msgstr "（Exactly-Once Processing）精确一次处理"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Kafka Transactions allows managing consumer offsets inside a transaction, together with produced messages.\n"
"This enables coupling a consumer with a transactional producer in a _consume-transform-produce_ pattern, also known as *exactly-once processing*."
msgstr "Kafka 事务可以同时管理其中的消费端和生产端的消息偏移量。 这使得消费端能够以 _consume-transform-produce_ 模式与生产端耦合，也称为 *exactly-once 精确一次性处理*。"

#: _versions/3.27/guides/kafka.adoc
msgid "The `KafkaTransactions` custom emitter provides a way to apply exactly-once processing to an incoming Kafka message inside a transaction."
msgstr "`KafkaTransactions` 定制emitter可以提供一种对事务中传入的 Kafka 消息进行一次性处理的方法。"

#: _versions/3.27/guides/kafka.adoc
msgid "The following example includes a batch of Kafka records inside a transaction."
msgstr "以下示例包括在事务中进行 Kafka 记录的批处理。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"It is recommended to use exactly-once processing along with the batch consumption mode.\n"
"While it is possible to use it with a single Kafka message, it'll have a significant performance impact."
msgstr "建议在消费端批处理模式中采用exactly-once。 虽然可以将它与单个 Kafka 消息一起使用，但这样会对性能产生重大影响。"

#: _versions/3.27/guides/kafka.adoc
msgid "The consumed message is passed to the `KafkaTransactions#withTransactionAndAck` in order to handle the offset commits and message acks."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"The `send` method writes records to Kafka inside the transaction, without waiting for send receipt from the broker.\n"
"Messages pending to be written to Kafka will be buffered, and flushed before committing the transaction.\n"
"It is therefore recommended configuring the `@OnOverflow` `bufferSize` in order to fit enough messages, for example the `max.poll.records`, maximum amount of records returned in a batch."
msgstr "`send` 方法将记录通过事务写入 Kafka，而无需等待来自代理的发送回执。 等待写入 Kafka 的消息将被缓冲，并在提交事务之前刷新。 因此，建议配置 `@OnOverflow` `bufferSize` 以适应足够的消息，例如 `max.poll.records`，即批处理中返回的最大记录数。"

#: _versions/3.27/guides/kafka.adoc
msgid "If the processing completes successfully, _before committing the transaction_, the topic partition offsets of the given batch message will be committed to the transaction."
msgstr "如果处理成功完成，_在提交事务之前_ ，给定批处理消息的主题分区偏移量将提交给事务。"

#: _versions/3.27/guides/kafka.adoc
msgid "If the processing needs to abort, _after aborting the transaction_, the consumer's position is reset to the last committed offset, effectively resuming the consumption from that offset. If no consumer offset has been committed to a topic-partition, the consumer's position is reset to the beginning of the topic-partition, _even if the offset reset policy is `latest`_."
msgstr "如果处理需要中止，_在中止事务后_ ，消费者的位置将重置为最后提交的偏移量，能有效地从该偏移量恢复消费。 如果没有消费者偏移量被提交到主题分区，消费者的位置将被重置到主题分区的开头，_即使把 `latest` 做为偏移量重置策略_ 。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"When using exactly-once processing, consumed message offset commits are handled by the transaction and therefore the application should not commit offsets through other means.\n"
"The consumer should have `enable.auto.commit=false` (the default) and set explicitly `commit-strategy=ignore`:"
msgstr "当使用（exactly-once）精确一次处理时，消耗的消息偏移量提交由事务处理，因此应用程序不应通过其他方式提交偏移量。 消费者应该有 `enable.auto.commit=false` （默认）并明确设置 `commit-strategy=ignore` ："

#: _versions/3.27/guides/kafka.adoc
msgid "Error handling for the exactly-once processing"
msgstr "（exactly-once）精确一次处理的错误处理"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"The `Uni` returned from the `KafkaTransactions#withTransaction` will yield a failure if the transaction fails and is aborted.\n"
"The application can choose to handle the error case, but if a failing `Uni` is returned from the `@Incoming` method, the incoming channel will effectively fail and stop the reactive stream."
msgstr "如果事务失败并被中止，则从 KafkaTransactions#withTransaction 返回的 `Uni` 将产生失败。 应用程序可以选择处理错误情况，但如果从 `@Incoming` 方法返回失败的 `Uni` ，则传入通道将有效地失败并停止响应流。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"The `KafkaTransactions#withTransactionAndAck` method acks and nacks the message but will *not* return a failing `Uni`.\n"
"Nacked messages will be handled by the failure strategy of the incoming channel, (see <<error-handling>>).\n"
"Configuring `failure-strategy=ignore` simply resets the Kafka consumer to the last committed offsets and resumes the consumption from there."
msgstr "`KafkaTransactions#withTransactionAndAck` 方法确认和确认消息，但 *不会* 返回失败的 `Uni` 。 Nacked 消息将由传入通道的故障策略处理，（参见<<error-handling>>）。 配置 `failure-strategy=ignore` 只是将 Kafka 消费者重置为最后提交的偏移量并从那里恢复消费。"

#: _versions/3.27/guides/kafka.adoc
msgid "Accessing Kafka clients directly"
msgstr "直接访问Kafka客户端"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"In rare cases, you may need to access the underlying Kafka clients.\n"
"`KafkaClientService` provides thread-safe access to `Producer` and `Consumer`."
msgstr "在少数情况下，您可能需要访问底层的Kafka客户端。`KafkaClientService` 提供线程安全的方式来访问 `Producer` 和 `Consumer` 。"

#: _versions/3.27/guides/kafka.adoc
msgid "The `KafkaClientService` is an experimental API and can change in the future."
msgstr "`KafkaClientService` 是一个实验性的API，在未来可能会发生变化。"

#: _versions/3.27/guides/kafka.adoc
msgid "You can also get the Kafka configuration injected to your application and create Kafka producer, consumer and admin clients directly:"
msgstr "您也可以把Kafka配置注入到您的应用程序中来直接创建Kafka生产者，消费者以及管理客户端："

#: _versions/3.27/guides/kafka.adoc
msgid ""
"The `default-kafka-broker` configuration map contains all application properties prefixed with `kafka.` or `KAFKA_`.\n"
"For more configuration options check out <<kafka-configuration-resolution>>."
msgstr "`default-kafka-broker` 配置map包含所有以 `kafka.` 或 `KAFKA_` 为前缀的应用属性。更多的配置选项，请查看 link:#kafka-configuration-resolution[[kafka配置方案]] 。"

#: _versions/3.27/guides/kafka.adoc
msgid "JSON serialization"
msgstr "JSON序列化"

#: _versions/3.27/guides/kafka.adoc
msgid "Quarkus has built-in capabilities to deal with JSON Kafka messages."
msgstr "Quarkus有内置的能力来处理JSON格式的Kafka消息。"

#: _versions/3.27/guides/kafka.adoc
msgid "Imagine we have a `Fruit` data class as follows:"
msgstr "假设我们有一个 `Fruit` 数据类，如下所示："

#: _versions/3.27/guides/kafka.adoc
msgid "And we want to use it to receive messages from Kafka, make some price transformation, and send messages back to Kafka."
msgstr "而我们想用它来接收来自Kafka的消息，从而进行一些价格转换，并将消息传回Kafka。"

#: _versions/3.27/guides/kafka.adoc
msgid "To do this, we will need to set up JSON serialization with Jackson or JSON-B."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "With JSON serialization correctly configured, you can also use `Publisher<Fruit>` and `Emitter<Fruit>`."
msgstr "在正确配置了JSON序列化后，您也可以使用 `Publisher<Fruit>` 和 `Emitter<Fruit>` 。"

#: _versions/3.27/guides/kafka.adoc
msgid "Serializing via Jackson"
msgstr "通过Jackson进行序列化"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Quarkus has built-in support for JSON serialization and deserialization based on Jackson.\n"
"It will also <<serialization-generation,generate>> the serializer and deserializer for you, so you do not have to configure anything.\n"
"When generation is disabled, you can use the provided `ObjectMapperSerializer` and `ObjectMapperDeserializer` as explained below."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"There is an existing `ObjectMapperSerializer` that can be used to serialize all data objects via Jackson.\n"
"You may create an empty subclass if you want to use <<serialization-autodetection>>."
msgstr "有一个现有的 `ObjectMapperSerializer` ，可以用来通过Jackson来序列化所有的数据对象。如果您想使用 link:#serialization-autodetection[[自动侦测序列化]] ，您可以创建一个空的子类来继承该类。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"By default, the `ObjectMapperSerializer` serializes null as the `\"null\"` String, this can be customized by setting the Kafka configuration\n"
"property `json.serialize.null-as-null=true` which will serialize null as `null`.\n"
"This is handy when using a compacted topic, as `null` is used as a tombstone to know which messages delete during compaction phase."
msgstr "默认情况下， `ObjectMapperSerializer` 将null序列化为 `\"null\"` 字符串，这可以通过设置Kafka配置属性 `json.serialize.null-as-null=true` ，将null序列化为 `null` 。这在使用压缩的topic时很方便，因为 `null` 被用作标记来表示在压缩阶段哪些消息被删除了。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"The corresponding deserializer class needs to be subclassed.\n"
"So, let's create a `FruitDeserializer` that extends the `ObjectMapperDeserializer`."
msgstr "对应的反序列化器类也需要被子类化。因此，让我们创建一个 `FruitDeserializer` 来继承 `ObjectMapperDeserializer` 。"

#: _versions/3.27/guides/kafka.adoc
msgid "Finally, configure your channels to use the Jackson serializer and deserializer."
msgstr "最后，配置您的 channelss 以使用Jackson序列化器和反序列化器。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Now, your Kafka messages will contain a Jackson serialized representation of your `Fruit` data object.\n"
"In this case, the `deserializer` configuration is not necessary as the <<serialization-autodetection>> is enabled by default."
msgstr "现在，您的Kafka消息将包含 `Fruit` 数据对象的Jackson序列化格式。在这种情况下，`deserializer` 的配置不是必须的，因为 link:#serialization-autodetection[[序列化自动侦测]] 是默认启用的。"

#: _versions/3.27/guides/kafka.adoc
msgid "If you want to deserialize a list of fruits, you need to create a deserializer with a Jackson `TypeReference` denoted the generic collection used."
msgstr "如果您想反序列化一个fruit对象列表，您需要创建一个反序列化器，它会用Jackson `TypeReference` 表示所用到的通用集合。"

#: _versions/3.27/guides/kafka.adoc
msgid "Serializing via JSON-B"
msgstr "通过JSON-B进行序列化"

#: _versions/3.27/guides/kafka.adoc
msgid "First, you need to include the `quarkus-jsonb` extension."
msgstr "首先，您需要引入 `quarkus-jsonb` 扩展。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"There is an existing `JsonbSerializer` that can be used to serialize all data objects via JSON-B.\n"
"You may create an empty subclass if you want to use <<serialization-autodetection>>."
msgstr "有一个现有的 `JsonbSerializer` ，可以通过JSON-B来序列化所有的数据对象。如果您想使用 link:#serialization-autodetection[[序列化自动侦测]] ，您可以创建一个空的子类类继承它。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"By default, the `JsonbSerializer` serializes null as the `\"null\"` String, this can be customized by setting the Kafka configuration\n"
"property `json.serialize.null-as-null=true` which will serialize null as `null`.\n"
"This is handy when using a compacted topic, as `null` is used as a tombstone to know which messages delete during compaction phase."
msgstr "默认情况下， `JsonbSerializer` 将null序列化为 `\"null\"` 字符串，这可以通过设置Kafka配置属性 `json.serialize.null-as-null=true` 来将null序列化为 `null` 。这在使用压缩的topic时很方便，因为 `null` 被用作标记来表示在压缩阶段哪些消息被删除了。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"The corresponding deserializer class needs to be subclassed.\n"
"So, let's create a `FruitDeserializer` that extends the generic `JsonbDeserializer`."
msgstr "相应的反序列化器类需要被子类化。因此，让我们创建一个 `FruitDeserializer` 来继承 `JsonbDeserializer` 。"

#: _versions/3.27/guides/kafka.adoc
msgid "Finally, configure your channels to use the JSON-B serializer and deserializer."
msgstr "最后，通过配置来使您的 channels 使用JSON-B串行器和反串行器。"

#: _versions/3.27/guides/kafka.adoc
msgid "Now, your Kafka messages will contain a JSON-B serialized representation of your `Fruit` data object."
msgstr "现在，您的Kafka消息将包含 `Fruit` 数据对象的JSON-B序列化格式。"

#: _versions/3.27/guides/kafka.adoc
msgid "If you want to deserialize a list of fruits, you need to create a deserializer with a `Type` denoted the generic collection used."
msgstr "如果您想反序列化一个fruit对象列表，您需要创建一个反序列化器，它会用一个 `Type` 表示所用到的通用集合。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"If you don't want to create a deserializer for each data object, you can use the generic `io.vertx.kafka.client.serialization.JsonObjectDeserializer`\n"
"that will deserialize to a `io.vertx.core.json.JsonObject`. The corresponding serializer can also be used: `io.vertx.kafka.client.serialization.JsonObjectSerializer`."
msgstr "如果您不想为每个数据对象创建一个反序列化器，您可以使用通用的 `io.vertx.kafka.client.serialization.JsonObjectDeserializer` ，它将把消息反序列化为一个 `io.vertx.core.json.JsonObject` 。也可以使用与之相对应的序列化器： `io.vertx.kafka.client.serialization.JsonObjectSerializer` 。"

#: _versions/3.27/guides/kafka.adoc
msgid "Avro Serialization"
msgstr "Avro序列化"

#: _versions/3.27/guides/kafka.adoc
msgid "This is described in a dedicated guide: xref:kafka-schema-registry-avro.adoc[Using Apache Kafka with Schema Registry and Avro]."
msgstr "这部分在一个专门的指南中有所描述。 link:kafka-schema-registry-avro.html[使用Apache Kafka与Schema Registry和Avro协同工作] 。"

#: _versions/3.27/guides/kafka.adoc
msgid "JSON Schema Serialization"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "This is described in a dedicated guide: xref:kafka-schema-registry-json-schema.adoc[Using Apache Kafka with Schema Registry and JSON Schema]."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Serializer/deserializer autodetection"
msgstr "序列化/反串行器自动侦测"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"When using Quarkus Messaging with Kafka (`io.quarkus:quarkus-messaging-kafka`), Quarkus can often automatically detect the correct serializer and deserializer class.\n"
"This autodetection is based on declarations of `@Incoming` and `@Outgoing` methods, as well as injected ``@Channel``s."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "For example, if you declare"
msgstr "例如，如果您声明"

#: _versions/3.27/guides/kafka.adoc
msgid "and your configuration indicates that the `generated-price` channel uses the `smallrye-kafka` connector, then Quarkus will automatically set the `value.serializer` to Kafka's built-in `IntegerSerializer`."
msgstr "而您的配置表明 `generated-price`  channel 使用了 `smallrye-kafka` 连接器，那么Quarkus会自动将 `value.serializer` 设置为Kafka内置的 `IntegerSerializer` 。"

#: _versions/3.27/guides/kafka.adoc
msgid "Similarly, if you declare"
msgstr "同样地，如果您声明"

#: _versions/3.27/guides/kafka.adoc
msgid "and your configuration indicates that the `my-kafka-records` channel uses the `smallrye-kafka` connector, then Quarkus will automatically set the `key.deserializer` to Kafka's built-in `LongDeserializer`, as well as the `value.deserializer` to `ByteArrayDeserializer`."
msgstr "并且您的配置表明 `my-kafka-records`  channel 使用了 `smallrye-kafka` 连接器，那么Quarkus会自动将 `key.deserializer` 设置为Kafka内置的 `LongDeserializer` ，以及 `value.deserializer` 设置为 `ByteArrayDeserializer` 。"

#: _versions/3.27/guides/kafka.adoc
msgid "Finally, if you declare"
msgstr "最后，如果您声明"

#: _versions/3.27/guides/kafka.adoc
msgid "and your configuration indicates that the `price-create` channel uses the `smallrye-kafka` connector, then Quarkus will automatically set the `value.serializer` to Kafka's built-in `DoubleSerializer`."
msgstr "而您的配置表明 `price-create`  channel 使用 `smallrye-kafka` 连接器，那么Quarkus将自动将 `value.serializer` 设置为Kafka内置的 `DoubleSerializer` 。"

#: _versions/3.27/guides/kafka.adoc
msgid "The full set of types supported by the serializer/deserializer autodetection is:"
msgstr "序列化器/反序列化器自动侦测所支持的全部类型有："

#: _versions/3.27/guides/kafka.adoc
msgid "`short` and `java.lang.Short`"
msgstr "`short` 和 `java.lang.Short`"

#: _versions/3.27/guides/kafka.adoc
msgid "`int` and `java.lang.Integer`"
msgstr "`int` 和 `java.lang.Integer`"

#: _versions/3.27/guides/kafka.adoc
msgid "`long` and `java.lang.Long`"
msgstr "`long` 和 `java.lang.Long`"

#: _versions/3.27/guides/kafka.adoc
msgid "`float` and `java.lang.Float`"
msgstr "`float` 和 `java.lang.Float`"

#: _versions/3.27/guides/kafka.adoc
msgid "`double` and `java.lang.Double`"
msgstr "`double` 和 `java.lang.Double`"

#: _versions/3.27/guides/kafka.adoc
msgid "`byte[]`"
msgstr "`byte[]`"

#: _versions/3.27/guides/kafka.adoc
msgid "`java.lang.String`"
msgstr "`java.lang.String`"

#: _versions/3.27/guides/kafka.adoc
msgid "`java.util.UUID`"
msgstr "`java.util.UUID`"

#: _versions/3.27/guides/kafka.adoc
msgid "`java.nio.ByteBuffer`"
msgstr "`java.nio.ByteBuffer`"

#: _versions/3.27/guides/kafka.adoc
msgid "`org.apache.kafka.common.utils.Bytes`"
msgstr "`org.apache.kafka.common.utils.Bytes`"

#: _versions/3.27/guides/kafka.adoc
msgid "`io.vertx.core.buffer.Buffer`"
msgstr "`io.vertx.core.buffer.Buffer`"

#: _versions/3.27/guides/kafka.adoc
msgid "`io.vertx.core.json.JsonObject`"
msgstr "`io.vertx.core.json.JsonObject`"

#: _versions/3.27/guides/kafka.adoc
msgid "`io.vertx.core.json.JsonArray`"
msgstr "`io.vertx.core.json.JsonArray`"

#: _versions/3.27/guides/kafka.adoc
msgid "classes for which a direct implementation of `org.apache.kafka.common.serialization.Serializer<T>` / `org.apache.kafka.common.serialization.Deserializer<T>` is present."
msgstr "直接实现了 `org.apache.kafka.common.serialization.Serializer<T>` / `org.apache.kafka.common.serialization.Deserializer<T>` 的类。"

#: _versions/3.27/guides/kafka.adoc
msgid "the implementation needs to specify the type argument `T` as the (de-)serialized type."
msgstr "这些实现类需要指定类型参数 `T` 作为(反)序列化的类型。"

#: _versions/3.27/guides/kafka.adoc
msgid "classes generated from Avro schemas, as well as Avro `GenericRecord`, if Confluent or Apicurio Registry _serde_ is present"
msgstr "从Avro Schema生成的类，类似于Avro `GenericRecord`。如果Confluent或Apicurio Registry _serde_ 存在的话"

#: _versions/3.27/guides/kafka.adoc
msgid "in case multiple Avro serdes are present, serializer/deserializer must be configured manually for Avro-generated classes, because autodetection is impossible"
msgstr "如果存在多个Avro serdes，必须为Avro生成的类手动配置序列化器/反序列化器，因为这种情况下无法进行自动侦测"

#: _versions/3.27/guides/kafka.adoc
msgid "see xref:kafka-schema-registry-avro.adoc[Using Apache Kafka with Schema Registry and Avro] for more information about using Confluent or Apicurio Registry libraries"
msgstr "关于使用Confluent或Apicurio Registry的更多信息，请参见 link:kafka-schema-registry-avro.html[协同使用Apache Kafka，Schema Registry以及Avro]"

#: _versions/3.27/guides/kafka.adoc
msgid "classes for which a subclass of `ObjectMapperSerializer` / `ObjectMapperDeserializer` is present, as described in <<jackson-serialization>>"
msgstr "`ObjectMapperSerializer` / `ObjectMapperDeserializer` 的子类，如 link:#jackson-serialization[[jackson序列化]] 中所述"

#: _versions/3.27/guides/kafka.adoc
msgid "it is technically not needed to subclass `ObjectMapperSerializer`, but in such case, autodetection isn't possible"
msgstr "技术上不需要对 `ObjectMapperSerializer` 子类化，但在这种情况下无法进行自动侦测"

#: _versions/3.27/guides/kafka.adoc
msgid "classes for which a subclass of `JsonbSerializer` / `JsonbDeserializer` is present, as described in <<jsonb-serialization>>"
msgstr "`JsonbSerializer` / `JsonbDeserializer` 的子类，如 link:#jsonb-serialization[[jsonb序列化]] 中所述"

#: _versions/3.27/guides/kafka.adoc
msgid "it is technically not needed to subclass `JsonbSerializer`, but in such case, autodetection isn't possible"
msgstr "技术上不需要对 `JsonbSerializer` 子类化，但在这种情况下无法进行自动侦测"

#: _versions/3.27/guides/kafka.adoc
msgid "If a serializer/deserializer is set by configuration, it won't be replaced by the autodetection."
msgstr "如果一个序列化器/反序列化器是通过配置设置的，那么它不会被自动检测所取代。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"In case you have any issues with serializer autodetection, you can switch it off completely by setting `quarkus.messaging.kafka.serializer-autodetection.enabled=false`.\n"
"If you find you need to do this, please file a bug in the link:https://github.com/quarkusio/quarkus/issues[Quarkus issue tracker] so we can fix whatever problem you have."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "JSON Serializer/deserializer generation"
msgstr "JSON序列化器/反序列化器的生成"

#: _versions/3.27/guides/kafka.adoc
msgid "Quarkus automatically generates serializers and deserializers for channels where:"
msgstr "以下情况下，Quarkus会自动为 channels 生成序列化器和反序列化器："

#: _versions/3.27/guides/kafka.adoc
msgid "the serializer/deserializer is not configured"
msgstr "序列化器/反序列化器未配置"

#: _versions/3.27/guides/kafka.adoc
msgid "the auto-detection did not find a matching serializer/deserializer"
msgstr "自动侦测机制没有找到匹配的序列化器/反序列化器"

#: _versions/3.27/guides/kafka.adoc
msgid "It uses Jackson underneath."
msgstr "它的底层使用了Jackson。"

#: _versions/3.27/guides/kafka.adoc
msgid "This generation can be disabled using:"
msgstr "可以用以下方法禁用这种生成机制："

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Generation does not support collections such as `List<Fruit>`.\n"
"Refer to <<jackson-serialization>> to write your own serializer/deserializer for this case."
msgstr "生成机制不支持诸如 `List<Fruit>` 这样的集合。请参考 link:#jackson-serialization[[jackson序列化]] 为这种情况编写您自己的序列化器/反序列化器。"

#: _versions/3.27/guides/kafka.adoc
msgid "Using Schema Registry"
msgstr "使用Schema注册表"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"This is described in a dedicated guide for Avro: xref:kafka-schema-registry-avro.adoc[Using Apache Kafka with Schema Registry and Avro].\n"
"And a different one for JSON Schema: xref:kafka-schema-registry-json-schema.adoc[Using Apache Kafka with Schema Registry and JSON Schema]."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Health Checks"
msgstr "健康检查"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Quarkus provides several health checks for Kafka.\n"
"These checks are used in combination with the `quarkus-smallrye-health` extension."
msgstr "Quarkus为Kafka提供了几种健康检查方式。这些方式需要与 `quarkus-smallrye-health` 扩展结合使用。"

#: _versions/3.27/guides/kafka.adoc
msgid "Kafka Broker Readiness Check"
msgstr "Kafka Broker就绪检查(Readiness Check)"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"When using the `quarkus-kafka-client` extension, you can enable _readiness_ health check by setting the `quarkus.kafka.health.enabled` property to `true` in your `application.properties`.\n"
"This check reports the status of the interaction with a _default_ Kafka broker (configured using `kafka.bootstrap.servers`).\n"
"It requires an _admin connection_ with the Kafka broker, and it is disabled by default.\n"
"If enabled, when you access the `/q/health/ready` endpoint of your application, you will have information about the connection validation status."
msgstr "当使用 `quarkus-kafka-client` 扩展时，您可以通过在您的 `application.properties` 配置文件中将 `quarkus.kafka.health.enabled` 属性设置为 `true` 来启用 _就绪_ 健康检查。该检查会报告与 _默认的_ Kafka Broker(使用 `kafka.bootstrap.servers` 配置)的交互状态。它需要一个与Kafka Broker的 _管理员连接_ ，并且默认是禁用的。如果启用，当您访问您应用程序的 `/q/health/ready` 节点时，您将获得关于连接验证状态的信息。"

#: _versions/3.27/guides/kafka.adoc
msgid "Kafka Reactive Messaging Health Checks"
msgstr "Kafka响应式消息传递健康检查"

#: _versions/3.27/guides/kafka.adoc
msgid "When using Reactive Messaging and the Kafka connector, each configured channel (incoming or outgoing) provides _startup_, _liveness_ and _readiness_ checks."
msgstr "当使用响应式消息传递和Kafka 连接器时，每个配置的 channel (传入或传出)都会提供 _启动_ 、 _活跃度(liveness)_ 和 _就绪_ 检查。"

#: _versions/3.27/guides/kafka.adoc
msgid "The _startup_ check verifies that the communication with Kafka cluster is established."
msgstr "_启动_ 检查确保与Kafka集群的通信是否建立。"

#: _versions/3.27/guides/kafka.adoc
msgid "The _liveness_ check captures any unrecoverable failure happening during the communication with Kafka."
msgstr "_活跃性_ 检查可以捕获与Kafka通信过程中发生的任何不可恢复的故障。"

#: _versions/3.27/guides/kafka.adoc
msgid "The _readiness_ check verifies that the Kafka connector is ready to consume/produce messages to the configured Kafka topics."
msgstr "_就绪_ 检查确保Kafka 连接器是否准备好针对配置的Kafka topic消费或生产消息。"

#: _versions/3.27/guides/kafka.adoc
msgid "For each channel, you can disable the checks using:"
msgstr "对于每个 channel ，您都可以禁用检查，通过："

#: _versions/3.27/guides/kafka.adoc
msgid ""
"You can configure the `bootstrap.servers` for each channel using `mp.messaging.incoming|outgoing.$channel.bootstrap.servers` property.\n"
"Default is `kafka.bootstrap.servers`."
msgstr "您可以使用 `mp.messaging.incoming|outgoing.$channel.bootstrap.servers` 属性为每个 channel 配置 `bootstrap.servers` 。默认是 `kafka.bootstrap.servers` 。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Reactive Messaging _startup_ and _readiness_ checks offer two strategies.\n"
"The default strategy verifies that an active connection is established with the broker.\n"
"This approach is not intrusive as it's based on built-in Kafka client metrics."
msgstr "响应式消息传递的 _启动_ 和 _就绪_ 检查提供了两种策略。默认策略是确认是否与broker建立了活动连接。这种方法不具有侵入性，因为它基于内置的Kafka客户端指标。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Using the `health-topic-verification-enabled=true` attribute, _startup_ probe uses an _admin client_ to check for the list of topics.\n"
"Whereas the _readiness_ probe for an incoming channel checks that at least one partition is assigned for consumption,\n"
"and for an outgoing channel checks that the topic used by the producer exist in the broker."
msgstr "使用 `health-topic-verification-enabled=true` 属性， _启动_ 探针使用一个 _管理客户端_ 来检查topic列表。而传入 channel 的 _就绪_ 探针将检查是否至少有一个分区被分配用于消费，而传出 channel 则检查生产者使用的topic是否存在于broker中。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Note that to achieve this, an _admin connection_ is required.\n"
"You can adjust the timeout for topic verification calls to the broker using the `health-topic-verification-timeout` configuration."
msgstr "注意，要实现这一点， 一个_管理员连接_是必须存在的 。您可以使用 `health-topic-verification-timeout` 来调整对broker的topic验证调用的超时时间。"

#: _versions/3.27/guides/kafka.adoc
msgid "Observability"
msgstr "可观察性"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"If the xref:opentelemetry.adoc[OpenTelemetry extension] is present,\n"
"then the Kafka connector channels work out-of-the-box with the OpenTelemetry Tracing.\n"
"Messages written to Kafka topics propagate the current tracing span.\n"
"On incoming channels, if a consumed Kafka record contains tracing information the message processing inherits the message span as parent."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Tracing can be disabled explicitly per channel:"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"If the xref:telemetry-micrometer.adoc[Micrometer extension] is present,\n"
"then Kafka producer and consumer clients metrics are exposed as Micrometer meters."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Channel metrics"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Per channel metrics can also be gathered and exposed as Micrometer meters.\n"
"Following metrics can be gathered per channel, identified with the _channel_ tag:"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "`quarkus.messaging.message.count` : The number of messages produced or received"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "`quarkus.messaging.message.acks` : The number of messages processed successfully"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "`quarkus.messaging.message.failures` : The number of messages processed with failures"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "`quarkus.messaging.message.duration` : The duration of the message processing."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "For backwards compatibility reasons channel metrics are not enabled by default and can be enabled with:"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"The https://smallrye.io/smallrye-reactive-messaging/latest/concepts/observability/[message observation]\n"
"depends on intercepting messages and therefore doesn't support channels consuming messages with\n"
"a custom message type such as `IncomingKafkaRecord`, `KafkaRecord`, `IncomingKafkaRecordBatch` or `KafkaRecordBatch`."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"The message interception, and observation, still work with channels consuming the generic `Message` type,\n"
"or custom payloads enabled by https://smallrye.io/smallrye-reactive-messaging/latest/concepts/converters/[converters]."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Kafka Streams"
msgstr "Kafka流"

#: _versions/3.27/guides/kafka.adoc
msgid "This is described in a dedicated guide: xref:kafka-streams.adoc[Using Apache Kafka Streams]."
msgstr "这部分在专门的指南中有所描述：link:kafka-streams.html[使用Apache Kafka流] 。"

#: _versions/3.27/guides/kafka.adoc
msgid "Using Snappy for message compression"
msgstr "使用Snappy进行消息压缩"

#: _versions/3.27/guides/kafka.adoc
msgid "On _outgoing_ channels, you can enable Snappy compression by setting the `compression.type` attribute to `snappy`:"
msgstr "在 _出站_  channels 上，您可以通过将 `compression.type` 设置为 `snappy` 来启用Snappy压缩："

#: _versions/3.27/guides/kafka.adoc
msgid ""
"In JVM mode, it will work out of the box.\n"
"However, to compile your application to a native executable, you need to\n"
"add `quarkus.kafka.snappy.enabled=true` to your `application.properties`."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "In native mode, Snappy is disabled by default as the use of Snappy requires embedding a native library and unpacking it when the application starts."
msgstr "在原生模式下，Snappy默认是禁用的，因为使用Snappy需要嵌入一个原生库，并在应用程序启动时对其进行解包。"

#: _versions/3.27/guides/kafka.adoc
msgid "Authentication with OAuth"
msgstr "用OAuth进行认证"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"If your Kafka broker uses OAuth as authentication mechanism, you need to configure the Kafka consumer to enable this authentication process.\n"
"First, add the following dependency to your application:"
msgstr "如果您的Kafka broker使用OAuth作为认证机制，您需要配置Kafka消费者来启用这个认证过程。首先，在您的应用程序中添加以下依赖："

#: _versions/3.27/guides/kafka.adoc
msgid ""
"This dependency provides the callback handler required to handle the OAuth workflow.\n"
"Then, in the `application.properties`, add:"
msgstr "这个依赖提供了处理OAuth工作流所需的回调处理器。然后，在 `application.properties` ，添加："

#: _versions/3.27/guides/kafka.adoc
msgid "Update the `oauth.client.id`, `oauth.client.secret` and `oauth.token.endpoint.uri` values."
msgstr "更改 `oauth.client.id` ， `oauth.client.secret` 和 `oauth.token.endpoint.uri` 值。"

#: _versions/3.27/guides/kafka.adoc
msgid "OAuth authentication works for both JVM and native modes. Since SSL in not enabled by default in native mode, `quarkus.ssl.native=true` must be added to support JaasClientOauthLoginCallbackHandler, which uses SSL. (See the xref:native-and-ssl.adoc[Using SSL with Native Executables] guide for more details.)"
msgstr "OAuth认证在JVM和原生模式下都有效。由于SSL在原生模式下默认不启用，所以必须添加 `quarkus.ssl.native=true` ，以支持JaasClientOauthLoginCallbackHandler。它使用了SSL。(更多细节请参见《 link:native-and-ssl.html[在原生可执行文件中使用SSL] 》指南)。"

#: _versions/3.27/guides/kafka.adoc
msgid "TLS Configuration"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Kafka client extension integrates with the xref:./tls-registry-reference.adoc[Quarkus TLS registry] to configure clients."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "To configure the TLS for the default Kafka configuration, you need to provide a named TLS configuration in the `application.properties`:"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "This will in turn provide the Kafka client with a `ssl.engine.factory.class` implementation."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Make sure also to enable the SSL channel security protocol using the `security.protocol` property configured to `SSL` or `SASL_SSL`."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Quarkus Messaging channels can be configured individually to use a specific TLS configuration:"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Testing a Kafka application"
msgstr "测试一个Kafka应用程序"

#: _versions/3.27/guides/kafka.adoc
msgid "Testing without a broker"
msgstr "无broker的测试"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"It can be useful to test the application without having to start a Kafka broker.\n"
"To achieve this, you can _switch_ the channels managed by the Kafka connector to _in-memory_."
msgstr "在不启动Kafka broker的情况下测试应用程序会很有用。为了实现这一点，您可以把Kafka连接器管理的 channels  _切换_ 到_内存 _中 。"

#: _versions/3.27/guides/kafka.adoc
msgid "This approach only works for JVM tests. It cannot be used for native tests (because they do not support injection)."
msgstr "这种方法只适用于JVM测试。它不能用于原生测试(因为原生模式不支持注入)。"

#: _versions/3.27/guides/kafka.adoc
msgid "Let's say we want to test the following processor application:"
msgstr "假设我们想测试以下的处理器应用："

#: _versions/3.27/guides/kafka.adoc
msgid "First, add the following test dependency to your application:"
msgstr "首先，在您的应用程序中添加以下测试依赖："

#: _versions/3.27/guides/kafka.adoc
msgid "Then, create a Quarkus Test Resource as follows:"
msgstr "然后，按以下方法创建Quarkus测试资源："

#: _versions/3.27/guides/kafka.adoc
msgid "Switch the incoming channel `orders` (expecting messages from Kafka) to in-memory."
msgstr "将传入 channel  `orders` (等待来自Kafka的消息)切换到内存中。"

#: _versions/3.27/guides/kafka.adoc
msgid "Switch the outgoing channel `beverages` (writing messages to Kafka) to in-memory."
msgstr "将出站 channel `beverages`(向Kafka写消息)切换到内存中。"

#: _versions/3.27/guides/kafka.adoc
msgid "Builds and returns a `Map` containing all the properties required to configure the application to use in-memory channels."
msgstr "构建并返回一个 `Map` ，包含配置应用程序使用内存 channels 所需的所有属性。"

#: _versions/3.27/guides/kafka.adoc
msgid "When the test stops, clear the `InMemoryConnector` (discard all the received and sent messages)"
msgstr "当测试停止时，清除 `InMemoryConnector` (丢弃所有接收和发送的信息)。"

#: _versions/3.27/guides/kafka.adoc
msgid "Create a Quarkus Test using the test resource created above:"
msgstr "使用上面创建的测试资源创建一个Quarkus测试："

#: _versions/3.27/guides/kafka.adoc
msgid "Inject the in-memory connector in your test class."
msgstr "在您的测试类中注入内存内连接器。"

#: _versions/3.27/guides/kafka.adoc
msgid "Retrieve the incoming channel (`orders`) - the channel must have been switched to in-memory in the test resource."
msgstr "检索传入 channel ( `orders` ) - 该 channel 必须在测试资源中被切换到内存中。"

#: _versions/3.27/guides/kafka.adoc
msgid "Retrieve the outgoing channel (`beverages`) - the channel must have been switched to in-memory in the test resource."
msgstr "检索传出 channel  ( `beverages` ) - 该 channel 必须在测试资源中被切换到内存中。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Use the `send` method to send a message to the `orders` channel.\n"
"The application will process this message and send a message to `beverages` channel."
msgstr "使用 `send` 方法向 `orders`  channel 发送一个消息。应用程序将处理这个消息并向 `beverages`  channel 发送消息。"

#: _versions/3.27/guides/kafka.adoc
msgid "Use the `received` method on `beverages` channel to check the messages produced by the application."
msgstr "在 `beverages`  channel 上使用 `received` 方法来检查应用程序产生的消息。"

#: _versions/3.27/guides/kafka.adoc
msgid "If your Kafka consumer is batch based, you will need to send a batch of messages to the channel as by creating them manually."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "For instance:"
msgstr "比如："

#: _versions/3.27/guides/kafka.adoc
msgid "Create an `AtomicBoolean` to track if the batch has been committed."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Update `committed` when the batch is committed."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Create a `IncomingKafkaRecordBatch` with a single record."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Wait until the batch is committed."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"With in-memory channels we were able to test application code processing messages without starting a Kafka broker.\n"
"Note that different in-memory channels are independent, and switching channel connector to in-memory does not simulate message delivery between channels configured to the same Kafka topic."
msgstr "有了内存 channels ，我们就可以测试应用程序代码的消息方法，而无需启动Kafka broker。请注意，不同的内存 channel 是独立的，将 channel 连接器切换到内存中并不能模拟配置到同一Kafka topic的 channel 之间的消息传递。"

#: _versions/3.27/guides/kafka.adoc
msgid "Context propagation with InMemoryConnector"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "By default, in-memory channels dispatch messages on the caller thread, which would be the main thread in unit tests."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"The `quarkus-test-vertx` dependency provides the `@io.quarkus.test.vertx.RunOnVertxContext` annotation,\n"
"which when used on a test method, executes the test on a Vert.x context."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "However, most of the other connectors handle context propagation dispatching messages on separate duplicated Vert.x contexts."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"If your tests are dependent on context propagation,\n"
"you can configure the in-memory connector channels with the `run-on-vertx-context` attribute to dispatch events,\n"
"including messages and acknowledgements, on a Vert.x context.\n"
"Alternatively you can switch this behaviour using the `InMemorySource#runOnVertxContext` method."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Testing using a Kafka broker"
msgstr "使用Kafka broker的测试"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"If you are using <<kafka-dev-services>>, a Kafka broker will be started and available throughout the tests, unless it is disabled in `%test` profile.\n"
"While it is possible to connect to this broker using Kafka Clients API,\n"
"https://smallrye.io/smallrye-reactive-messaging/latest/kafka/test-companion/[Kafka Companion Library] proposes an easier way of interacting with a Kafka broker and, creating consumer, producer and admin actions inside tests."
msgstr "如果您使用 link:#kafka-dev-services[[kafka-dev-services]] ，Kafka broker将被启动并在整个测试中可用，除非它在 `%test` profile中被禁用。虽然可以使用Kafka客户端API连接到这个broker，但 link:https://smallrye.io/smallrye-reactive-messaging/latest/kafka/test-companion/[Kafka Companion Library] 提出了一种更简单的方式来与Kafka broker通信，并在测试中创建消费者、生产者和管理操作。"

#: _versions/3.27/guides/kafka.adoc
msgid "For using `KafkaCompanion` API in tests, start by adding the following dependency:"
msgstr "为了在测试中使用 `KafkaCompanion` API，首先要添加以下依赖："

#: _versions/3.27/guides/kafka.adoc
msgid "which provides `io.quarkus.test.kafka.KafkaCompanionResource` - an implementation of `io.quarkus.test.common.QuarkusTestResourceLifecycleManager`."
msgstr "它提供了 `io.quarkus.test.kafka.KafkaCompanionResource` -- `io.quarkus.test.common.QuarkusTestResourceLifecycleManager` 的一种实现。"

#: _versions/3.27/guides/kafka.adoc
msgid "Then use `@QuarkusTestResource` to configure the Kafka Companion in tests, for example:"
msgstr "然后使用 `@QuarkusTestResource` 在测试中配置Kafka Companion，比如："

#: _versions/3.27/guides/kafka.adoc
msgid "`@InjectKafkaCompanion` injects the `KafkaCompanion` instance, configured to access the Kafka broker created for tests."
msgstr "`@InjectKafkaCompanion` 注入了 `KafkaCompanion` 实例，并被配置为可访问为测试目的而创建的Kafka broker。"

#: _versions/3.27/guides/kafka.adoc
msgid "Use `KafkaCompanion` to create producer task which writes 10 records to 'orders' topic."
msgstr "使用 `KafkaCompanion` 来创建生产者任务，用于向 'orders' topic写入10条记录。"

#: _versions/3.27/guides/kafka.adoc
msgid "Create consumer task which subscribes to 'orders-processed' topic and consumes 10 records."
msgstr "创建消费者任务，用来订阅'orders-processed'topic并消费10条记录。"

#: _versions/3.27/guides/kafka.adoc
msgid "Await completion of the consumer task."
msgstr "等待消费者任务的完成。"

#: _versions/3.27/guides/kafka.adoc
msgid "You need to configure"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "otherwise you will get an `java.lang.AssertionError: No completion (or failure) event received in the last 10000 ms` in <4>"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "If the Kafka Dev Service is available during tests, `KafkaCompanionResource` uses the created Kafka broker, otherwise it creates a Kafka broker using https://github.com/strimzi/test-container[Strimzi Test Container]."
msgstr "如果Kafka Dev Service在测试期间是可用的， `KafkaCompanionResource` 则会使用创建的Kafka broker，否则就使用 link:https://github.com/strimzi/test-container[Strimzi测试容器]创建一个Kafka broker。"

#: _versions/3.27/guides/kafka.adoc
msgid "The configuration of the created Kafka broker can be customized using `@ResourceArg`, for example:"
msgstr "创建Kafka broker的配置可以通过使用 `@ResourceArg` 来自定义，例如："

#: _versions/3.27/guides/kafka.adoc
msgid "Custom test resource"
msgstr "自定义测试资源"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Alternatively, you can start a Kafka broker in a test resource.\n"
"The following snippet shows a test resource starting a Kafka broker using https://www.testcontainers.org/modules/kafka/[Testcontainers]:"
msgstr "另外，您也可以在测试资源中启动一个Kafka broker。下面的片段展示了如何在一个测试资源使用 link:https://www.testcontainers.org/modules/kafka/[Testcontainers] 启动一个Kafka broker："

#: _versions/3.27/guides/kafka.adoc
msgid "Configure the Kafka bootstrap location, so the application connects to this broker."
msgstr "配置Kafka bootstrap位置，这样应用程序就会连接到这个broker。"

#: _versions/3.27/guides/kafka.adoc
msgid "Kubernetes Service Bindings"
msgstr "Kubernetes服务绑定"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Quarkus Kafka extension supports\n"
"xref:deploying-to-kubernetes.adoc[Service Binding Specification for Kubernetes].\n"
"You can enable this by adding the `quarkus-kubernetes-service-binding` extension to your application."
msgstr "Quarkus Kafka扩展支持 link:deploying-to-kubernetes.html[Kubernetes服务绑定规范] 。您可以通过添加 `quarkus-kubernetes-service-binding` 扩展来启用它。"

#: _versions/3.27/guides/kafka.adoc
msgid "When running in appropriately configured Kubernetes clusters, Kafka extension will pull its Kafka broker connection configuration from the service binding available inside the cluster, without the need for user configuration."
msgstr "当在正确配置的Kubernetes集群中运行时，Kafka扩展将从集群内部可用的服务绑定中获取Kafka broker连接配置，而不需要用户来配置。"

#: _versions/3.27/guides/kafka.adoc
msgid "Execution model"
msgstr "执行模型"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Reactive Messaging invokes user's methods on an I/O thread.\n"
"Thus, by default, the methods must not block.\n"
"As described in <<blocking-processing>>, you need to add the `@Blocking` annotation on the method if this method will block the caller thread."
msgstr "响应式流会在I/O线程上调用用户的方法。因此在默认情况下，这些方法不能阻塞。正如 link:#blocking-processing[[阻塞处理]] 中所述，如果这个方法会阻塞调用者线程，那么您需要在方法上添加 `@Blocking` 注解。"

#: _versions/3.27/guides/kafka.adoc
msgid "See the xref:quarkus-reactive-architecture.adoc[Quarkus Reactive Architecture documentation] for further details on this topic."
msgstr "关于这个话题的更多细节，请看 link:quarkus-reactive-architecture.html[Quarkus响应式架构文档] 。"

#: _versions/3.27/guides/kafka.adoc
msgid "Channel Decorators"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "SmallRye Reactive Messaging supports decorating incoming and outgoing channels for implementing cross-cutting concerns such as monitoring, tracing or message interception. For more information on implementing decorators and message interceptors see the http://smallrye.io/smallrye-reactive-messaging/latest/concepts/decorators/[SmallRye Reactive Messaging documentation]."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Configuration Reference"
msgstr "配置参考"

#: _versions/3.27/guides/kafka.adoc
msgid "More details about the SmallRye Reactive Messaging configuration can be found in the https://smallrye.io/smallrye-reactive-messaging/latest/kafka/kafka/#using-the-kafka-connector[SmallRye Reactive Messaging - Kafka Connector Documentation]."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Each channel can be disabled via configuration using:"
msgstr "每个 channel 都可以通过配置来禁用："

#: _versions/3.27/guides/kafka.adoc
msgid "The most important attributes are listed in the tables below:"
msgstr "以下表格中列出了最重要的属性："

#: _versions/3.27/guides/kafka.adoc
msgid "Incoming channel configuration (polling from Kafka)"
msgstr "入站 channel 配置(从Kafka轮询)"

#: _versions/3.27/guides/kafka.adoc
msgid "The following attributes are configured using:"
msgstr "以下属性通过该方式进行配置："

#: _versions/3.27/guides/kafka.adoc
msgid "Some properties have aliases which can be configured globally:"
msgstr "有些属性拥有可以进行全局配置的别名："

#: _versions/3.27/guides/kafka.adoc
msgid "You can also pass any property supported by the underlying https://kafka.apache.org/documentation/#consumerconfigs[Kafka consumer]."
msgstr "您也可以传递底层 link:https://kafka.apache.org/documentation/#consumerconfigs[Kafka消费者] 支持的任何属性。"

#: _versions/3.27/guides/kafka.adoc
msgid "For example, to configure the `max.poll.records` property, use:"
msgstr "例如，要配置 `max.poll.records` 属性，可以使用："

#: _versions/3.27/guides/kafka.adoc
msgid "Some consumer client properties are configured to sensible default values:"
msgstr "一些消费者客户端属性被配置为相对合理的默认值："

#: _versions/3.27/guides/kafka.adoc
msgid "If not set, `reconnect.backoff.max.ms` is set to `10000` to avoid high load on disconnection."
msgstr "如果没有设置， `reconnect.backoff.max.ms` 则会被配置为 `10000` ，以避免断开时导致的高负载。"

#: _versions/3.27/guides/kafka.adoc
msgid "If not set, `key.deserializer` is set to `org.apache.kafka.common.serialization.StringDeserializer`."
msgstr "如果没有设置， `key.deserializer` 则会被设置为 `org.apache.kafka.common.serialization.StringDeserializer` 。"

#: _versions/3.27/guides/kafka.adoc
msgid "The consumer `client.id` is configured according to the number of clients to create using `mp.messaging.incoming.[channel].partitions` property."
msgstr "消费者 `client.id` 会根据使用 `mp.messaging.incoming.[channel].partitions` 属性创建的客户端数量进行配置。"

#: _versions/3.27/guides/kafka.adoc
msgid "If a `client.id` is provided, it is used as-is or suffixed with client index if `partitions` property is set."
msgstr "如果提供了一个 `client.id` ，它将直接被使用， 或者如果 `partitions` 属性被设置的话，则会被加上客户端索引的后缀。"

#: _versions/3.27/guides/kafka.adoc
msgid "If a `client.id` is not provided, it is generated as `[client-id-prefix][channel-name][-index]`."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Outgoing channel configuration (writing to Kafka)"
msgstr "出站 channel 配置(写入Kafka)。"

#: _versions/3.27/guides/kafka.adoc
msgid "You can also pass any property supported by the underlying https://kafka.apache.org/documentation/#producerconfigs[Kafka producer]."
msgstr "您也可以传递底层 link:https://kafka.apache.org/documentation/#producerconfigs[Kafka生产者] 支持的任何属性。"

#: _versions/3.27/guides/kafka.adoc
msgid "For example, to configure the `max.block.ms` property, use:"
msgstr "例如，要配置 `max.block.ms` 属性，可以使用："

#: _versions/3.27/guides/kafka.adoc
msgid "Some producer client properties are configured to sensible default values:"
msgstr "一些生产者客户端属性被配置为合理的默认值："

#: _versions/3.27/guides/kafka.adoc
msgid "If not set, `key.serializer` is set to `org.apache.kafka.common.serialization.StringSerializer`."
msgstr "如果没有设置， `key.serializer` 则会被设置为 `org.apache.kafka.common.serialization.StringSerializer` 。"

#: _versions/3.27/guides/kafka.adoc
msgid "If not set, producer `client.id` is generated as `[client-id-prefix][channel-name]`."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Kafka Configuration Resolution"
msgstr "Kafka配置方案"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Quarkus exposes all Kafka related application properties, prefixed with `kafka.` or `KAFKA_` inside a configuration map with `default-kafka-broker` name.\n"
"This configuration is used to establish the connection with the Kafka broker."
msgstr "Quarkus公开了所有与Kafka相关的应用属性，这些属性使用 `default-kafka-broker` 名称加 `kafka.` 或 `KAFKA_` 的前缀 。这个配置被用来建立与Kafka broker的连接。"

#: _versions/3.27/guides/kafka.adoc
msgid "In addition to this default configuration, you can configure the name of the `Map` producer using the `kafka-configuration` attribute:"
msgstr "除了这个默认配置外，您还可以使用 `kafka-configuration` 属性配置 `Map` 生产者的名称："

#: _versions/3.27/guides/kafka.adoc
msgid ""
"In this case, the connector looks for the `Map` associated with the `my-configuration` name.\n"
"If `kafka-configuration` is not set, an optional lookup for a `Map` exposed with the channel name (`my-channel` in the previous example) is done."
msgstr "在这种情况下，连接器会查询与 `my-configuration` 名称相关的 `Map` 。如果没有设置 `kafka-configuration` ，就会进行额外的查询来寻找与 channel 名称相关的 `Map` (在前面的例子中是 `my-channel` )。"

#: _versions/3.27/guides/kafka.adoc
msgid "If `kafka-configuration` is set and no `Map` can be found, the deployment fails."
msgstr "如果设置了 `kafka-configuration` ，但没有找到 `Map` ，则部署会失败。"

#: _versions/3.27/guides/kafka.adoc
msgid "Attribute values are resolved as follows:"
msgstr "属性值的解决方式如下："

#: _versions/3.27/guides/kafka.adoc
msgid "the attribute is set directly on the channel configuration (`mp.messaging.incoming.my-channel.attribute=value`),"
msgstr "属性是直接在 channel 配置上设置的( `mp.messaging.incoming.my-channel.attribute=value` ),"

#: _versions/3.27/guides/kafka.adoc
msgid "if not set, the connector looks for a `Map` with the channel name or the configured `kafka-configuration` (if set) and the value is retrieved from that `Map`"
msgstr "如果没有设置，连接器会使用 channel 名称或 `kafka-configuration` (如果设置了)来查找查找一个 `Map` ，并从 `Map` 中取值"

#: _versions/3.27/guides/kafka.adoc
msgid "If the resolved `Map` does not contain the value the default `Map` is used (exposed with the `default-kafka-broker` name)"
msgstr "如果 `Map` 不包含该值，则使用默认的 `Map` (通过 `default-kafka-broker` 名称暴露)"

#: _versions/3.27/guides/kafka.adoc
msgid "Conditionally configure channels"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"You can configure the channels using a specific profile.\n"
"Thus, the channels are only configured (and added to the application) when the specified profile is enabled."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "To achieve this, you need:"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Prefix the `mp.messaging.[incoming|outgoing].$channel` entries with `%my-profile` such as `%my-profile.mp.messaging.[incoming|outgoing].$channel.key=value`"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Use the `@IfBuildProfile(\"my-profile\")` on the CDI beans containing `@Incoming(channel)` and `@Outgoing(channel)` annotations that need only to be enabled when the profile is enabled."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Note that reactive messaging verifies that the graph is complete.\n"
"So, when using such a conditional configuration, ensure the application works with and without the profile enabled."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Note that this approach can also be used to change the channel configuration based on a profile."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Integrating with Kafka - Common patterns"
msgstr "与Kafka的整合--通用模式"

#: _versions/3.27/guides/kafka.adoc
msgid "Writing to Kafka from an HTTP endpoint"
msgstr "从HTTP节点写消息到Kafka"

#: _versions/3.27/guides/kafka.adoc
msgid "To send messages to Kafka from an HTTP endpoint, inject an `Emitter` (or a `MutinyEmitter`) in your endpoint:"
msgstr "要从HTTP节点向Kafka发送消息，可以在您的节点中注入一个 `Emitter` (或一个 `MutinyEmitter` )："

#: _versions/3.27/guides/kafka.adoc
msgid "Inject an `Emitter<String>`"
msgstr "注入一个 `Emitter<String>`"

#: _versions/3.27/guides/kafka.adoc
msgid "The HTTP method receives the payload and returns a `CompletionStage` completed when the message is written to Kafka"
msgstr "HTTP方法接会接收payload，并在消息被写入Kafka时返回一个 `CompletionStage`"

#: _versions/3.27/guides/kafka.adoc
msgid "Send the message to Kafka, the `send` method returns a `CompletionStage`"
msgstr "将消息发送到Kafka后， `send` 方法会返回一个 `CompletionStage`"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"The endpoint sends the passed payload (from a `POST` HTTP request) to the emitter.\n"
"The emitter's channel is mapped to a Kafka topic in the `application.properties` file:"
msgstr "节点将已传递的payload(来自 `POST` HTTP请求)发送给emitter。emitter的 channel 被映射到 `application.properties` 文件中指定的一个Kafka topic："

#: _versions/3.27/guides/kafka.adoc
msgid ""
"The endpoint returns a `CompletionStage` indicating the asynchronous nature of the method.\n"
"The `emitter.send` method returns a `CompletionStage<Void>` .\n"
"The returned future is completed when the message has been written to Kafka.\n"
"If the writing fails, the returned `CompletionStage` is completed exceptionally."
msgstr "节点会返回一个 `CompletionStage` ，表明该方法是异步的。 `emitter.send` 方法返回一个 `CompletionStage<Void>` 。当消息被写入Kafka时，返回的Future就被认为i完成了。如果写入失败，返回的 `CompletionStage` 会抛出异常。"

#: _versions/3.27/guides/kafka.adoc
msgid "If the endpoint does not return a `CompletionStage`, the HTTP response may be written before the message is sent to Kafka, and so failures won't be reported to the user."
msgstr "如果节点没有返回 `CompletionStage` ，HTTP响应可能会返回在消息被发送到Kafka之前，因此失败不会被报告给用户。"

#: _versions/3.27/guides/kafka.adoc
msgid "If you need to send a Kafka record, use:"
msgstr "如果您需要发送一条Kafka记录，请使用："

#: _versions/3.27/guides/kafka.adoc
msgid "Note the usage of an `Emitter<Record<K, V>>`"
msgstr "声明 `Emitter<Record<K, V>>` 的使用"

#: _versions/3.27/guides/kafka.adoc
msgid "Create the record using `Record.of(k, v)`"
msgstr "通过 `Record.of(k, v)` 来创建记录"

#: _versions/3.27/guides/kafka.adoc
msgid "Persisting Kafka messages with Hibernate with Panache"
msgstr "用Hibernate与Panache来持久化Kafka消息"

#: _versions/3.27/guides/kafka.adoc
msgid "To persist objects received from Kafka into a database, you can use Hibernate with Panache."
msgstr "为了将从Kafka接收到的对象持久化到数据库中，您可以结合使用Hibernate与Panache。"

#: _versions/3.27/guides/kafka.adoc
msgid "If you use Hibernate Reactive, look at <<persisting-kafka-messages-with-hibernate-reactive>>."
msgstr "如果您使用Hibernate Reactive，请参看 link:#persisting-kafka-messages-with-hibernate-reactive[[使用Hibernate Reactive持久化Kafka消息]] 。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Let's imagine you receive `Fruit` objects.\n"
"For simplicity purposes, our `Fruit` class is pretty simple:"
msgstr "让我们假设您收到了 `Fruit` 对象。为了简单起见，我们的 `Fruit` 类非常简单："

#: _versions/3.27/guides/kafka.adoc
msgid "To consume `Fruit` instances stored on a Kafka topic, and persist them into a database, you can use the following approach:"
msgstr "为了消费存储在Kafka topic上的 `Fruit` 实例，并将其持久化到数据库中，您可以使用以下方法："

#: _versions/3.27/guides/kafka.adoc
msgid "Configuring the incoming channel. This channel reads from Kafka."
msgstr "配置传入 channel 。该 channel 从Kafka读取消息。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"As we are writing in a database, we must be in a transaction. This annotation starts a new transaction and commits it when the method returns.\n"
"Quarkus automatically considers the method as _blocking_. Indeed, writing to a database using classic Hibernate is blocking. So, Quarkus calls the method on a worker thread you can block (and not an I/O thread)."
msgstr "由于我们是往数据库中写入数据，所以必须使用事务。这个注解启动了一个新的事务，并在方法返回时提交它。Quarkus会自动认为这个方法是 _阻塞的_ 。事实上，使用常规的Hibernate方法向数据库写入是阻塞操作。所以，Quarkus会在一个可阻塞的工作线程中调用这个方法(而不是在I/O线程中)。"

#: _versions/3.27/guides/kafka.adoc
msgid "The method receives each Fruit. Note that you would need a deserializer to reconstruct the Fruit instances from the Kafka records."
msgstr "该方法接收每个Fruit对象。注意，您需要一个反序列化器来从Kafka记录中重建Fruit实例。"

#: _versions/3.27/guides/kafka.adoc
msgid "Persist the received `fruit` object."
msgstr "持久化接收到的 `fruit` 对象。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"As mentioned in <4>, you need a deserializer that can create a `Fruit` from the record.\n"
"This can be done using a Jackson deserializer:"
msgstr "正如<4>中提到的，您需要一个能从记录中重建 `Fruit` 对象的反序列化器。可以使用Jackson的反序列化器来完成："

#: _versions/3.27/guides/kafka.adoc
msgid "The associated configuration would be:"
msgstr "相关的配置如下："

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Check <<jackson-serialization>> for more detail about the usage of Jackson with Kafka.\n"
"You can also use Avro."
msgstr "请参看 link:#jackson-serialization[[jackson序列化]]，了解更多关于Jackson与Kafka的使用细节。您也可以使用Avro。"

#: _versions/3.27/guides/kafka.adoc
msgid "Persisting Kafka messages with Hibernate Reactive"
msgstr "使用Hibernate Reactive持久化Kafka消息"

#: _versions/3.27/guides/kafka.adoc
msgid "To persist objects received from Kafka into a database, you can use Hibernate Reactive with Panache."
msgstr "为了将从Kafka收到的对象持久化到数据库中，您可以结合使用Hibernate Reactive与Panache。"

#: _versions/3.27/guides/kafka.adoc
msgid "Make sure to use the reactive variant"
msgstr "请确保使用响应式变量"

#: _versions/3.27/guides/kafka.adoc
msgid "Inject the Hibernate Reactive `Session`"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Hibernate Reactive `Session` and `Panache` APIs require an active CDI Request context.\n"
"`@ActivateRequestContext` annotation creates a new request context and destroys it when the `Uni` returned from the method completes.\n"
"If `Panache` is not used, `Mutiny.SessionFactory` can be injected and used similarly without the need of activating the request context or closing the session manually."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Requests a new transaction. The transaction completes when the passed action completes."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Persist the entity. It returns a `Uni<Fruit>`."
msgstr "持久化该实体对象。它会返回一个 `Uni<Fruit>` 。"

#: _versions/3.27/guides/kafka.adoc
msgid "Switch back to a `Uni<Void>`."
msgstr "切换并返回 `Uni<Void>` 。"

#: _versions/3.27/guides/kafka.adoc
msgid "Close the session - this is close the connection with the database. The connection can then be recycled."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Unlike with _classic_ Hibernate, you can't use `@Transactional`.\n"
"Instead, we use `session.withTransaction` and persist our entity.\n"
"The `map` is used to return a `Uni<Void>` and not a `Uni<Fruit>`."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"You need a deserializer that can create a `Fruit` from the record.\n"
"This can be done using a Jackson deserializer:"
msgstr "您需要一个能从记录中创建 `Fruit` 实例的反序列化器。可以使用Jackson的反序列化器来完成："

#: _versions/3.27/guides/kafka.adoc
msgid "Writing entities managed by Hibernate to Kafka"
msgstr "将Hibernate管理的实体写入Kafka中"

#: _versions/3.27/guides/kafka.adoc
msgid "Let's imagine the following process:"
msgstr "让我们假设以下过："

#: _versions/3.27/guides/kafka.adoc
msgid "You receive an HTTP request with a payload,"
msgstr "您收到一个带有payload的HTTP请求,"

#: _versions/3.27/guides/kafka.adoc
msgid "You create an Hibernate entity instance from this payload,"
msgstr "您从这个payload中创建一个Hibernate实体对象,"

#: _versions/3.27/guides/kafka.adoc
msgid "You persist that entity into a database,"
msgstr "您将该实体持久化到数据库中,"

#: _versions/3.27/guides/kafka.adoc
msgid "You send the entity to a Kafka topic"
msgstr "您把实体发送到一个Kafka topic中"

#: _versions/3.27/guides/kafka.adoc
msgid "If you use Hibernate Reactive, look at <<writing-entities-managed-by-hibernate-reactive-to-kafka>>."
msgstr "如果您使用Hibernate Reactive，请参看link:#writing-entities-managed-by-hibernate-reactive-to-kafka[[将hibernate reactive管理的实体写入kafka]] 。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Because we write to a database, we must run this method in a transaction.\n"
"Yet, sending the entity to Kafka happens asynchronously.\n"
"We can achieve this by using `.sendAndAwait()` or `.sendAndForget()` on the `MutinyEmitter`, or `.send().toCompletableFuture().join()` on the `Emitter`."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "To implement this process, you need the following approach:"
msgstr "为了实现这一过程，您需要采取以下方法："

#: _versions/3.27/guides/kafka.adoc
msgid "As we are writing to the database, make sure we run inside a transaction"
msgstr "当我们向数据库中写入数据时，请确保运行在事务中"

#: _versions/3.27/guides/kafka.adoc
msgid "The method receives the fruit instance to persist."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Wrap the managed entity inside a Data transfer object and send it to Kafka.\n"
"This makes sure that managed entity is not impacted by the Kafka serialization.\n"
"Then await the completion of the operation before returning."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "You should not return a `CompletionStage` or `Uni` when using `@Transactional`, as all transaction commits will happen on a single thread, which impacts performance."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Writing entities managed by Hibernate Reactive to Kafka"
msgstr "将Hibernate Reactive管理的实体写入Kafka中"

#: _versions/3.27/guides/kafka.adoc
msgid "To send to Kafka entities managed by Hibernate Reactive, we recommend using:"
msgstr "为了将Hibernate Reactive管理的实体发送到Kafka，建议使用："

#: _versions/3.27/guides/kafka.adoc
msgid "Quarkus REST to serve HTTP requests"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "A `MutinyEmitter` to send message to a channel, so it can be easily integrated with the Mutiny API exposed by Hibernate Reactive or Hibernate Reactive with Panache."
msgstr "`MutinyEmitter` 会向 channel 发送消息，所以它可以很容易地与Hibernate Reactive或Hibernate Reactive with Panache所暴露的Mutiny API进行集成。"

#: _versions/3.27/guides/kafka.adoc
msgid "The following example demonstrates how to receive a payload, store it in the database using Hibernate Reactive with Panache, and send the persisted entity to Kafka:"
msgstr "下面的例子演示了如何接收一个payload，使用Hibernate Reactive with Panache将其存储在数据库中，并将持久化的实体发送到Kafka："

#: _versions/3.27/guides/kafka.adoc
msgid "Inject a `MutinyEmitter` which exposes a Mutiny API. It simplifies the integration with the Mutiny API exposed by Hibernate Reactive with Panache."
msgstr "注入一个暴露了Mutiny API的 `MutinyEmitter` 。它简化了与Hibernate Reactive with Panache所暴露的Mutiny API的整合。"

#: _versions/3.27/guides/kafka.adoc
msgid "The HTTP method receiving the payload returns a `Uni<Void>`. The HTTP response is written when the operation completes (the entity is persisted and written to Kafka)."
msgstr "接收payload的HTTP方法返回一个 `Uni<Void>` 。当操作完成后，会返回HTTP响应(实体被持久化并被写入Kafka)。"

#: _versions/3.27/guides/kafka.adoc
msgid "We need to write the entity into the database in a transaction."
msgstr "我们需要在一个事务中把实体写进数据库。"

#: _versions/3.27/guides/kafka.adoc
msgid "Once the persist operation completes, we send the entity to Kafka. The `send` method returns a `Uni<Void>`."
msgstr "一旦持久化操作完成，我们就把实体发送到Kafka。 `send` 方法会返回一个 `Uni<Void>` 。"

#: _versions/3.27/guides/kafka.adoc
msgid "Streaming Kafka topics as server-sent events"
msgstr "将Kafka topic作为服务器发送的事件流化"

#: _versions/3.27/guides/kafka.adoc
msgid "Streaming a Kafka topic as server-sent events (SSE) is straightforward:"
msgstr "将Kafka topic作为服务器发送的事件(Server-sent events, SSE)进行流化非常直白:"

#: _versions/3.27/guides/kafka.adoc
msgid "You inject the channel representing the Kafka topic in your HTTP endpoint"
msgstr "您在您的HTTP节点中注入代表Kafka topic的 channel"

#: _versions/3.27/guides/kafka.adoc
msgid "You return that channel as a `Publisher` or a `Multi` from the HTTP method"
msgstr "您将该 channel 作为一个 `Publisher` 或 `Multi` 从HTTP方法中返回"

#: _versions/3.27/guides/kafka.adoc
msgid "The following code provides an example:"
msgstr "以下代码提供了一个例子："

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Some environment cuts the SSE connection when there is not enough activity.\n"
"The workaround consists of sending _ping_ messages (or empty objects) periodically."
msgstr "当没有足够的活跃度时，一些环境会切断SSE的连接。替代方法则是定期发送 _ping_ 消息(或空对象)。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"The workaround is a bit more complex as besides sending the fruits coming from Kafka, we need to send pings periodically.\n"
"To achieve this we merge the stream coming from Kafka and a periodic stream emitting `{}` every 10 seconds."
msgstr "该替代方法有点复杂，因为除了发送来自Kafka的fruit实例，我们还需要定期发送ping。为了实现这一点，我们合并了来自Kafka的数据流和一个每10秒发送一个 `{}` 的数据流。"

#: _versions/3.27/guides/kafka.adoc
msgid "Chaining Kafka Transactions with Hibernate Reactive transactions"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"By chaining a Kafka transaction with a Hibernate Reactive transaction you can send records to a Kafka transaction,\n"
"perform database updates and commit the Kafka transaction only if the database transaction is successful."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "The following example demonstrates:"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Receive a payload by serving HTTP requests using Quarkus REST,"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Limit concurrency of that HTTP endpoint using SmallRye Fault Tolerance,"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Start a Kafka transaction and send the payload to Kafka record,"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Store the payload in the database using Hibernate Reactive with Panache,"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Commit the Kafka transaction only if the entity is persisted successfully."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Inject a `KafkaTransactions` which exposes a Mutiny API. It allows the integration with the Mutiny API exposed by Hibernate Reactive with Panache."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Limit the concurrency of the HTTP endpoint to \"1\", preventing starting multiple transactions at a given time."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "The HTTP method receiving the payload returns a `Uni<Void>`. The HTTP response is written when the operation completes (the entity is persisted and Kafka transaction is committed)."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Begin a Kafka transaction."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Send the payload to Kafka inside the Kafka transaction."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Persist the entity into the database in a Hibernate Reactive transaction."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Once the persist operation completes, and there is no errors, the Kafka transaction is committed.\n"
"The result is omitted and returned as the HTTP response."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"In the previous example the database transaction (inner) will commit followed by the Kafka transaction (outer).\n"
"If you wish to commit the Kafka transaction first and the database transaction second, you need to nest them in the reverse order."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "The next example demonstrates that using the Hibernate Reactive API (without Panache):"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Inject the Hibernate Reactive `SessionFactory`."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Begin a Hibernate Reactive transaction."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Persist the payload and send the entity to Kafka."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Alternatively, you can use the `@WithTransaction` annotation to start a transaction and commit it when the method returns:"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Start a Hibernate Reactive transaction and commit it when the method returns."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Chaining Kafka Transactions with Hibernate ORM transactions"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"While `KafkaTransactions` provide a reactive API on top of Mutiny to manage Kafka transactions,\n"
"you can still chain Kafka transactions with blocking Hibernate ORM transactions."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Start a Hibernate ORM transaction. The transaction is committed when the method returns."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Persist the payload."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Send the entity to Kafka inside the Kafka transaction."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Wait on the returned `Uni` for the Kafka transaction to complete."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Logging"
msgstr "日志"

#: _versions/3.27/guides/kafka.adoc
msgid "To reduce the amount of log written by the Kafka client, Quarkus sets the level of the following log categories to `WARNING`:"
msgstr "为了减少Kafka客户端的日志量，Quarkus将以下日志类别的级别设置为 `WARNING`："

#: _versions/3.27/guides/kafka.adoc
msgid "`org.apache.kafka.clients`"
msgstr "`org.apache.kafka.clients`"

#: _versions/3.27/guides/kafka.adoc
msgid "`org.apache.kafka.common.utils`"
msgstr "`org.apache.kafka.common.utils`"

#: _versions/3.27/guides/kafka.adoc
msgid "`org.apache.kafka.common.metrics`"
msgstr "`org.apache.kafka.common.metrics`"

#: _versions/3.27/guides/kafka.adoc
msgid "You can override the configuration by adding the following lines to the `application.properties`:"
msgstr "您可以通过在 `application.properties` 中添加以下属性来覆盖配配置："

#: _versions/3.27/guides/kafka.adoc
msgid "Connecting to Managed Kafka clusters"
msgstr "连接到受管理的Kafka集群"

#: _versions/3.27/guides/kafka.adoc
msgid "This section explains how to connect to notorious Kafka Cloud Services."
msgstr "本节解释了如何连接到臭名昭著的Kafka云服务。"

#: _versions/3.27/guides/kafka.adoc
msgid "Azure Event Hub"
msgstr "Azure Event Hub"

#: _versions/3.27/guides/kafka.adoc
msgid "https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-for-kafka-ecosystem-overview[Azure Event Hub] provides an endpoint compatible with Apache Kafka."
msgstr "link:https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-for-kafka-ecosystem-overview[Azure Event Hub] 提供了一个与Apache Kafka兼容的节点。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Azure Event Hubs for Kafka is not available in the _basic_ tier.\n"
"You need at least the _standard_ tier to use Kafka.\n"
"See https://azure.microsoft.com/en-us/pricing/details/event-hubs/[Azure Event Hubs Pricing] to see the other options."
msgstr "Azure Event Hubs for Kafka在 _基础(basic)_ 层中不可用。您至少需要 _标准(standard)_ 层才能使用Kafka。请参阅 link:https://azure.microsoft.com/en-us/pricing/details/event-hubs/[Azure Event Hubs定价] 查看其他选项。"

#: _versions/3.27/guides/kafka.adoc
msgid "To connect to Azure Event Hub, using the Kafka protocol with TLS, you need the following configuration:"
msgstr "使用带有TLS的Kafka协议连接到Azure Event Hub的话，您需要以下配置："

#: _versions/3.27/guides/kafka.adoc
msgid "The port is `9093`."
msgstr "该端口为 `9093` 。"

#: _versions/3.27/guides/kafka.adoc
msgid "You need to use the JAAS `PlainLoginModule`."
msgstr "您需要使用JAAS `PlainLoginModule` 。"

#: _versions/3.27/guides/kafka.adoc
msgid "The username is the `$ConnectionString` string."
msgstr "用户名是 `$ConnectionString` 字符串。"

#: _versions/3.27/guides/kafka.adoc
msgid "The Event Hub connection string given by Azure."
msgstr "由Azure提供的Event Hub连接字符串。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Replace `<YOUR.EVENTHUBS.CONNECTION.STRING>` with the connection string for your Event Hubs namespace.\n"
"For instructions on getting the connection string, see https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-get-connection-string[Get an Event Hubs connection string].\n"
"The result would be something like:"
msgstr "用您的Event Hubs命名空间的连接字符串替换 `<YOUR.EVENTHUBS.CONNECTION.STRING>` 。有关获取连接字符串的说明，请参阅 link:https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-get-connection-string[获取Event Hubs连接字符串] 。结果会类似这样："

#: _versions/3.27/guides/kafka.adoc
msgid "This configuration can be global (as above), or set in the channel configuration:"
msgstr "这个配置可以是全局的(如上)，也可以在 channel 配置中设置："

#: _versions/3.27/guides/kafka.adoc
msgid "Red Hat OpenShift Streams for Apache Kafka"
msgstr "红帽OpenShift Streams for Apache Kafka"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"https://cloud.redhat.com/[Red Hat OpenShift Streams for Apache Kafka] provides managed Kafka brokers.\n"
"First, follow the instructions from https://access.redhat.com/documentation/en-us/red_hat_openshift_streams_for_apache_kafka/1/guide/88e1487a-2a14-4b35-85b9-a7a2d67a37f3[Getting started with the `rhoas` CLI for Red Hat OpenShift Streams for Apache Kafka] to create your Kafka broker instance.\n"
"Make sure you copied the client id and client secret associated with the _ServiceAccount_ you created."
msgstr "link:https://cloud.redhat.com/[红帽OpenShift Streams for Apache Kafka] 提供了受管理的Kafka brokers。首先，按照 link:https://access.redhat.com/documentation/en-us/red_hat_openshift_streams_for_apache_kafka/1/guide/88e1487a-2a14-4b35-85b9-a7a2d67a37f3[红帽OpenShift Streams for Apache Kafka的 `rhoas` 命令行入门] 的说明，创建您的Kafka broker实例。请确保您复制了与您创建的 _ServiceAccount_ 相关的客户ID和客户密码。"

#: _versions/3.27/guides/kafka.adoc
msgid "Then, you can configure the Quarkus application to connect to the broker as follows:"
msgstr "然后，您可以配置Quarkus应用程序以连接到broker，如下所示："

#: _versions/3.27/guides/kafka.adoc
msgid "The connection string, given on the admin console, such as `demo-c--bjsv-ldd-cvavkc-a.bf2.kafka.rhcloud.com:443`"
msgstr "在管理控制台所给出的连接字符串，例如 `demo-c—​bjsv-ldd-cvavkc-a.bf2.kafka.rhcloud.com:443`"

#: _versions/3.27/guides/kafka.adoc
msgid "The kafka username (the client id from the service account)"
msgstr "Kafka的用户名(来自service account的客户端ID)"

#: _versions/3.27/guides/kafka.adoc
msgid "the kafka password (the client secret from the service account)"
msgstr "kafka密码(来自service account的客户密码)"

#: _versions/3.27/guides/kafka.adoc
msgid "In general, these properties are prefixed using `%prod` to enable them only when running in production mode."
msgstr "一般来说，这些属性的前缀使用 `%prod` ，以便只在生产模式下运行时启用。"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"As explained in https://access.redhat.com/documentation/en-us/red_hat_openshift_streams_for_apache_kafka/1/guide/88e1487a-2a14-4b35-85b9-a7a2d67a37f3[Getting started with the rhoas CLI for Red Hat OpenShift Streams for Apache Kafka], to use Red Hat OpenShift Streams for Apache Kafka, you must create the topic beforehand, create a _Service Account_, and provide permissions to read and write to your topic from that service account.\n"
"The authentication data (client id and secret) relates to the service account, which means you can implement fine-grain permissions and restrict access to the topic."
msgstr "正如在《 link:https://access.redhat.com/documentation/en-us/red_hat_openshift_streams_for_apache_kafka/1/guide/88e1487a-2a14-4b35-85b9-a7a2d67a37f3[红帽OpenShift Streams for Apache Kafka的 `rhoas` 命令行入门] 》中所解释的那样，要使用红帽OpenShift Streams for Apache Kafka，您必须事先创建topic，创建一个 _Service Account_ ，并提供从该服务账户读取和写入topic的权限。认证数据(客户端ID和密码)与服务账户有关，这意味着您可以实现细粒度的权限，并限制对topic的访问。"

#: _versions/3.27/guides/kafka.adoc
msgid "When using Kubernetes, it is recommended to set the client id and secret in a Kubernetes secret:"
msgstr "当使用Kubernetes时，建议在Kubernetes secret中设置客户端ID和secret："

#: _versions/3.27/guides/kafka.adoc
msgid "To allow your Quarkus application to use that secret, add the following line to the `application.properties` file:"
msgstr "为了允许您的Quarkus应用程序使用该secret，请在 `application.properties` 文件中添加下面一行："

#: _versions/3.27/guides/kafka.adoc
msgid "Red Hat OpenShift Service Registry"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"https://www.redhat.com/en/technologies/cloud-computing/openshift/openshift-service-registry[Red Hat OpenShift Service Registry]\n"
"provides fully managed service registry for handling Kafka schemas."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"You can follow the instructions from\n"
"https://access.redhat.com/documentation/en-us/red_hat_openshift_service_registry/1/guide/ab1894d1-cae0-4d11-b185-81d62b4aabc7#_60472331-fa00-48ec-a621-bbd039500c7d[Getting started with Red Hat OpenShift Service Registry],\n"
"or use the `rhoas` CLI to create a new service registry instance:"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Make sure to note the _Registry URL_ of the instance created.\n"
"For authentication, you can use the same _ServiceAccount_ you created previously.\n"
"You need to make sure that it has the necessary permissions to access the service registry."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "For example, using the `rhoas` CLI, you can grant the `MANAGER` role to the service account:"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Then, you can configure the Quarkus application to connect to the schema registry as follows:"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "The service registry URL, given on the admin console, such as `https://bu98.serviceregistry.rhcloud.com/t/0e95af2c-6e11-475e-82ee-f13bd782df24/apis/registry/v2`"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "The OAuth token endpoint URL, such as `https://identity.api.openshift.com/auth/realms/rhoas/protocol/openid-connect/token`"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "The client id (from the service account)"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "The client secret (from the service account)"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Binding Red Hat OpenShift managed services to Quarkus application using the Service Binding Operator"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"If your Quarkus application is deployed on a Kubernetes or OpenShift cluster with link:https://github.com/redhat-developer/service-binding-operator[Service Binding Operator] and link:https://github.com/redhat-developer/app-services-operator/tree/main/docs[OpenShift Application Services] operators installed,\n"
"configurations necessary to access Red Hat OpenShift Streams for Apache Kafka and Service Registry can be injected to the application using xref:deploying-to-kubernetes.adoc#service_binding[Kubernetes Service Binding]."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"In order to set up the Service Binding, you need first to connect OpenShift managed services to your cluster.\n"
"For an OpenShift cluster you can follow the instructions from link:https://github.com/redhat-developer/app-services-guides/tree/main/docs/registry/service-binding-registry#connecting-a-kafka-and-service-registry-instance-to-your-openshift-cluster[Connecting a Kafka and Service Registry instance to your OpenShift cluster]."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Once you've connected your cluster with the RHOAS Kafka and Service Registry instances, make sure you've granted necessary permissions to the newly created service account."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"Then, using the xref:deploying-to-kubernetes.adoc#service_binding[Kubernetes Service Binding] extension,\n"
"you can configure the Quarkus application to generate `ServiceBinding` resources for those services:"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "For this example Quarkus build will generate the following `ServiceBinding` resources:"
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid ""
"You can follow xref:deploying-to-kubernetes.adoc#openshift[Deploying to OpenShift] to deploy your application, including generated `ServiceBinding` resources.\n"
"The configuration properties necessary to access the Kafka and Schema Registry instances will be injected to the application automatically at deployment."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "Going further"
msgstr "进一步探索"

#: _versions/3.27/guides/kafka.adoc
msgid ""
"This guide has shown how you can interact with Kafka using Quarkus.\n"
"It utilizes Quarkus Messaging to build data streaming applications."
msgstr ""

#: _versions/3.27/guides/kafka.adoc
msgid "If you want to go further, check the documentation of https://smallrye.io/smallrye-reactive-messaging[SmallRye Reactive Messaging], the implementation used in Quarkus."
msgstr "如果您想更进一步，请参看link:https://smallrye.io/smallrye-reactive-messaging[SmallRye Reactive Messaging]，在Quarkus中使用的实现。"
